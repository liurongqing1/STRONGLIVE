{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c203bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt   \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import torch.nn.functional as Fun\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import preprocessing\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1d7283b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Size'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-a6d7eb54d78e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#N= ['contrast', 'Hue3','Hue5','Hue6','Saturation','Value']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keypoint'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'brightness'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'contrast'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'edgeLength'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Hue1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Hue2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Hue3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Hue4'\u001b[0m\u001b[1;33m,\u001b[0m        \u001b[1;34m'Hue5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Hue6'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Hue7'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Saturation'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Value'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'TI'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'SI'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#X =  normalize(X, axis=0, norm='l1')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch_envs\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2804\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2805\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2806\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2808\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch_envs\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1550\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1552\u001b[1;33m         self._validate_read_indexer(\n\u001b[0m\u001b[0;32m   1553\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1554\u001b[0m         )\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch_envs\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1644\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"loc\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1645\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1646\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{not_found} not in index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1648\u001b[0m             \u001b[1;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Size'] not in index\""
     ]
    }
   ],
   "source": [
    "data = pd.read_excel(\"H:/edge_dection/feature_test.xlsx\")########## \n",
    "#X = data[['scale', 'sr',  'size','keypoint','brightness','contrast','edgeLength','Hue1','Hue2','Hue3','Hue4',\t'Hue5',\t'Hue6',\t'Hue7',\t'Saturation','Value','TI','SI']].values\n",
    "#print(data)\n",
    "#X = data[['keypoint','brightness','contrast','Value']].values###########\n",
    "#X = data[['keypoint','brightness','Hue4','TI','SI','Size']].values\n",
    "#N= ['contrast', 'Hue3','Hue5','Hue6','Saturation','Value']\n",
    "N=['keypoint','brightness','contrast','edgeLength', 'Hue1','Hue2','Hue3','Hue4',\t'Hue5',\t'Hue6',\t'Hue7','Saturation','Value','TI','SI','Size']\n",
    "X = data[N].values\n",
    "\n",
    "#X =  normalize(X, axis=0, norm='l1')\n",
    "#min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#X = min_max_scaler.fit_transform(X)\n",
    "X = preprocessing.QuantileTransformer(output_distribution=\"normal\").fit_transform(X)\n",
    "#X = preprocessing.StandardScaler().fit_transform(X)\n",
    "print(len(X))\n",
    "Y = data.label.values.reshape(-1, 1)#######3\n",
    "\n",
    "train_x,test_x,train_y,test_y=train_test_split(X,Y,train_size=0.8,shuffle=True,random_state=10)  #,random_state=1 shuffle=False 就是按照顺序划分的测试集和验证集,默认为true才行\n",
    "\n",
    "print(type(train_x))\n",
    "print(test_x.shape)\n",
    "'''\n",
    "#将数据转换成Tensor LongTensor等价于int64\n",
    "train_x = torch.from_numpy(train_x).type(torch.FloatTensor)\n",
    "train_y = torch.from_numpy(train_y).type(torch.FloatTensor)\n",
    "test_x  =torch.from_numpy(test_x).type(torch.FloatTensor)\n",
    "test_y  = torch.from_numpy(test_y).type(torch.FloatTensor)\n",
    "print(test_x.shape)\n",
    "points = int(test_x.shape[1])\n",
    "\n",
    "\n",
    "batch = 8#32\n",
    "#no_of_batches = len(data)//batch\n",
    "epochs = 100#3000\n",
    "\n",
    "#TensorDataset()可以对tensor进行打包即合并\n",
    "train_ds = TensorDataset(train_x,train_y)\n",
    "#希望模型不关注训练集数据顺序故用乱序\n",
    "train_dl = DataLoader(train_ds,batch_size=batch,shuffle=True)#\n",
    "test_ds = TensorDataset(test_x,test_y)\n",
    "#对测试集不需要用乱序避免工作量增加\n",
    "test_dl = DataLoader(test_ds,batch_size=batch)\n",
    "#print(test_x,test_y)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed96946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "import random\n",
    "from sklearn import neighbors\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, precision_score, accuracy_score,recall_score, f1_score, roc_auc_score, roc_curve\n",
    "\n",
    "def accury(model,x_test, y_test):\n",
    "    #print(pred)\n",
    "    pred = model.predict(x_test)\n",
    "    #print(pred,y)\n",
    "    #print(sum(pred == y),sum(pred != y),len(y))\n",
    "    print(sum(pred == y_test)/len(y_test)*100) \n",
    "    \n",
    "def knn_predict(k,train_x, train_y,test_x):\n",
    "    clf = neighbors.KNeighborsClassifier(k)\n",
    "    clf.fit(train_x, train_y.ravel())\n",
    "    prediction = clf.predict(test_x)\n",
    "    return clf,prediction \n",
    "\n",
    "best_score = 0.0\n",
    "best_k = 0\n",
    "for k in range(1, 2):\n",
    "    clf,y_predict = knn_predict(k,train_x, train_y,test_x)\n",
    "    score = clf.score(test_x, test_y)\n",
    "    #print(score)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_k = k\n",
    "print(\"best k is: \", best_k)\n",
    "print(\"best score is: \", best_score)\n",
    "print(y_predict,test_y.ravel())\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier(1)\n",
    "clf.fit(train_x, train_y.ravel())\n",
    "accury(clf,test_x, test_y.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f60e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import confusion_matrix, precision_score, accuracy_score,recall_score, f1_score, roc_auc_score, \\\n",
    "    roc_curve\n",
    "    \n",
    " # 测试模型\n",
    "def test(model, x_test, y_test):\n",
    "    # 预测结果\n",
    "    y_pre = model.predict(x_test)\n",
    "    print(y_pre[1000:1030],y_test[1000:1030].ravel())\n",
    "    #print(y_pre)\n",
    "    # 混淆矩阵\n",
    "    con_matrix = confusion_matrix(y_test, y_pre)\n",
    "    \n",
    "    print('confusion_matrix:\\n', con_matrix)\n",
    "    print('accuracy:{}'.format(accuracy_score(y_test, y_pre)))\n",
    "    print('precision:{}'.format(precision_score(y_test, y_pre, average='micro')))\n",
    "    print('recall:{}'.format(recall_score(y_test, y_pre, average='micro')))\n",
    "    print('f1-score:{}'.format(f1_score(y_test, y_pre, average='micro')))\n",
    "    \n",
    "def accury(model,x_test, y_test):\n",
    "    #print(pred)\n",
    "    pred = model.predict(x_test)\n",
    "    #print(pred,y_test)\n",
    "    #print(sum(pred == y),sum(pred != y),len(y))\n",
    "    print(sum(pred == y_test)/len(y_test)*100) \n",
    "    return sum(pred == y_test)/len(y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a04d078e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 0 1 0 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[618  79]\n",
      " [102 541]]\n",
      "accuracy:0.8649253731343284\n",
      "precision:0.8649253731343284\n",
      "recall:0.8649253731343284\n",
      "f1-score:0.8649253731343284\n",
      "86.49253731343283\n"
     ]
    }
   ],
   "source": [
    "#KNN分类\n",
    "def trainKNN(x_train, y_train):\n",
    "    # KNN生成和训练\n",
    "    clf = neighbors.KNeighborsClassifier(1)\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = trainKNN(train_x, train_y.ravel())\n",
    "    test(model,test_x, test_y.ravel())\n",
    "    KNN=accury(model,test_x, test_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "8dc4a037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[547 150]\n",
      " [198 445]]\n",
      "accuracy:0.7402985074626866\n",
      "precision:0.7402985074626866\n",
      "recall:0.7402985074626866\n",
      "f1-score:0.7402985074626866\n",
      "74.02985074626865\n"
     ]
    }
   ],
   "source": [
    "# Logistic分类\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def trainLS(x_train, y_train):\n",
    "    # Logistic生成和训练\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = trainLS(train_x, train_y.ravel())\n",
    "    test(model,test_x, test_y.ravel())\n",
    "    \n",
    "    Log=accury(model,test_x, test_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0e93c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#决策树\n",
    "from sklearn import tree\n",
    "# 训练决策树\n",
    "def trainDT(x_train, y_train):\n",
    "    # DT生成和训练\n",
    "    clf = tree.DecisionTreeClassifier(criterion='entropy')#ID3分类树，信息增益特征选择\n",
    "    #clf = tree.DecisionTreeClassifier(criterion='gini')#CART分类树，基尼系数特征选择\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = trainDT(train_x, train_y.ravel())\n",
    "    start = time.time()\n",
    "    test(model,test_x, test_y.ravel())\n",
    "    end = time.time()\n",
    "    print('耗时：',str(end-start))\n",
    "    DT=accury(model,test_x,test_y.ravel())\n",
    "    #绘制树图\n",
    "    plt.figure(dpi=500,figsize=(50,20))\n",
    "    feature_names = ['keypoint','brightness','contrast','edgeLength','Hue1','Hue2','Hue3','Hue4',\t'Hue5',\t'Hue6',\t'Hue7','Saturation','Value','TI','SI','Size']\n",
    "    target_names = ['0','1']\n",
    "    dot_data = tree.plot_tree(model, feature_names = feature_names, class_names = target_names, filled = True, rounded = True)\n",
    "    #print(tree.export_text(model))   \n",
    "    #t = tree.export_text(model)\n",
    "    #with open('E:/研学/绘图/me/tree.txt','w') as f:\n",
    "        #f.write(t)\n",
    "    #plt.show()\n",
    "    #plt.savefig(\"E:/研学/绘图/me/tree.png\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "240d5346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['keypoint','brightness','contrast','edgeLength','Hue1','Hue2','Hue3','Hue4','Hue5','Hue6','Hue7','Saturation','Value','TI','SI','Size']\n",
    "[0,          1,           2,          3,          4,     5,     6,     7,     8,     9,     10,    11,           12,    13,  14,   15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "196911b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 0 1 0 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[683  14]\n",
      " [ 29 614]]\n",
      "accuracy:0.9679104477611941\n",
      "precision:0.9679104477611941\n",
      "recall:0.9679104477611941\n",
      "f1-score:0.9679104477611941\n",
      "96.7910447761194\n"
     ]
    }
   ],
   "source": [
    "#随机森林\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "def trainRF(x_train, y_train):\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = trainRF(train_x, train_y.ravel())\n",
    "    test(model,test_x, test_y.ravel())\n",
    "    RF=accury(model,test_x, test_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f7c947fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[624  73]\n",
      " [116 527]]\n",
      "accuracy:0.858955223880597\n",
      "precision:0.858955223880597\n",
      "recall:0.858955223880597\n",
      "f1-score:0.858955223880597\n",
      "85.8955223880597\n"
     ]
    }
   ],
   "source": [
    "#AdaBoost分类模型\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "def trainAB(x_train, y_train):\n",
    "    clf = AdaBoostClassifier()\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = trainAB(train_x, train_y.ravel())\n",
    "    test(model,test_x, test_y.ravel())\n",
    "    AB=accury(model,test_x, test_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "a9739fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[618  79]\n",
      " [142 501]]\n",
      "accuracy:0.8350746268656717\n",
      "precision:0.8350746268656717\n",
      "recall:0.8350746268656717\n",
      "f1-score:0.8350746268656717\n",
      "83.50746268656717\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "# 训练SVM模性\n",
    "from sklearn import svm\n",
    "def trainSVM(x_train, y_train):\n",
    "    # SVM生成和训练\n",
    "    clf = svm.SVC(kernel='rbf', probability=True)\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = trainSVM(train_x, train_y.ravel())\n",
    "    test(model,test_x, test_y.ravel())\n",
    "    SVM=accury(model,test_x, test_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "28a5000b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[490 207]\n",
      " [164 479]]\n",
      "accuracy:0.7231343283582089\n",
      "precision:0.7231343283582089\n",
      "recall:0.7231343283582089\n",
      "f1-score:0.7231343283582089\n",
      "72.31343283582089\n"
     ]
    }
   ],
   "source": [
    "#朴素贝叶斯\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#拟合数据\n",
    "def trainNB(x_train, y_train):\n",
    "    # 生成和训练\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = trainNB(train_x, train_y.ravel())\n",
    "    test(model,test_x, test_y.ravel())\n",
    "    NB=accury(model,test_x, test_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3627a1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8525721c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6697\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([1340, 6])\n"
     ]
    }
   ],
   "source": [
    "#'''\n",
    "data = pd.read_excel(\"H:/edge_dection/feature_test.xlsx\")########## \n",
    "#X = data[['scale', 'sr',  'size','keypoint','brightness','contrast','edgeLength','Hue1','Hue2','Hue3','Hue4',\t'Hue5',\t'Hue6',\t'Hue7',\t'Saturation','Value','TI','SI']].values\n",
    "\n",
    "#X = data[['keypoint','brightness','contrast','Value']].values###########\n",
    "N =['contrast', 'Hue3','Hue5','Hue6','Saturation','Value']\n",
    "#['keypoint','brightness','contrast','edgeLength','Hue1','Hue2','Hue3','Hue4','Hue5',\t'Hue6',\t'Hue7','Saturation','Value','TI','SI']\n",
    "\n",
    "X = data[N].values\n",
    "#X =  normalize(X, axis=0, norm='l1')\n",
    "#min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#X = min_max_scaler.fit_transform(X)\n",
    "X = preprocessing.QuantileTransformer(output_distribution=\"normal\").fit_transform(X)\n",
    "#X = preprocessing.StandardScaler().fit_transform(X)\n",
    "print(len(X))\n",
    "Y = data.label.values.reshape(-1, 1)#######3\n",
    "\n",
    "train_x,test_x,train_y,test_y=train_test_split(X,Y,train_size=0.8,random_state=10)  # shuffle=False 就是按照顺序划分的测试集和验证集,默认为true才行\n",
    "\n",
    "print(type(train_x))\n",
    "#'''\n",
    "#将数据转换成Tensor LongTensor等价于int64\n",
    "train_x = torch.from_numpy(train_x).type(torch.FloatTensor)\n",
    "train_y = torch.from_numpy(train_y).type(torch.FloatTensor)\n",
    "test_x  =torch.from_numpy(test_x).type(torch.FloatTensor)\n",
    "test_y  = torch.from_numpy(test_y).type(torch.FloatTensor)\n",
    "print(test_x.shape)\n",
    "points = int(test_x.shape[1])\n",
    "\n",
    "\n",
    "batch = 8#32\n",
    "#no_of_batches = len(data)//batch\n",
    "epochs = 500#3000\n",
    "\n",
    "#TensorDataset()可以对tensor进行打包即合并\n",
    "train_ds = TensorDataset(train_x,train_y)\n",
    "#希望模型不关注训练集数据顺序故用乱序\n",
    "train_dl = DataLoader(train_ds,batch_size=batch,shuffle=True)#\n",
    "test_ds = TensorDataset(test_x,test_y)\n",
    "#对测试集不需要用乱序避免工作量增加\n",
    "test_dl = DataLoader(test_ds,batch_size=batch)\n",
    "#print(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2bc19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ues it!!!!!\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,n_feature):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(n_feature, int(n_feature*2), bias=True)\n",
    "        self.linear2 = torch.nn.Linear(int(n_feature*2), int(n_feature*4), bias=True)\n",
    "        self.linear3 = torch.nn.Linear(int(n_feature*4), int(n_feature*2), bias=True)\n",
    "        self.linear4 = torch.nn.Linear(int(n_feature*2), int(n_feature), bias=True)\n",
    "        self.linear5 = torch.nn.Linear(int(n_feature), 1, bias=True)\n",
    "        self.relu = nn.ReLU()  # 模块的激活函数\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.relu(self.linear3(x))\n",
    "        x = self.relu(self.linear4(x))\n",
    "        x = torch.sigmoid(self.linear5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94ffa9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立神经网络\n",
    "class Net(torch.nn.Module):     # 继承 torch 的 Module\n",
    "    def __init__(self, n_feature):\n",
    "        super(Net, self).__init__()     # 继承 __init__ 功能\n",
    "        self.hidden1 = torch.nn.Linear(n_feature, int(n_feature*2))   # 隐藏层线性输出\n",
    "        self.hidden2 = torch.nn.Linear(int(n_feature*2), int(n_feature*2))   # 隐藏层线性输出\n",
    "        self.hidden3 = torch.nn.Linear(int(n_feature*2), int(n_feature*2))   # 隐藏层线性输出\n",
    "        self.hidden4 = torch.nn.Linear(int(n_feature*2), int(n_feature))   # 隐藏层线性输出\n",
    "        self.out = torch.nn.Linear(int(n_feature),1)       # 输出层线性输出\n",
    "        self.relu = nn.ReLU()  # 模块的激活函数\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 正向传播输入值, 神经网络分析出输出值\n",
    "        x = self.relu(self.hidden1(x))   # 激励函数(隐藏层的线性值)\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.relu(self.hidden3(x))\n",
    "        x = self.relu(self.hidden4(x))\n",
    "        #x = nn.Tanh(x)\n",
    "        x = torch.sigmoid(self.out(x)) \n",
    "        #print(x)# 输出值, 但是这个不是预测值, 预测值还需要再另外计算\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "960a9c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "耗时：: 0.0249326229095459\n",
      "epoch:  0 train_loss:  0.499 train_acc:  75.079 test_loss:  0.51 test_acc:  74.254\n",
      "min loss epoch:0\n",
      "max acc epoch：0        max acc：74.254\n",
      "耗时：: 0.03191494941711426\n",
      "epoch:  1 train_loss:  0.49 train_acc:  76.311 test_loss:  0.508 test_acc:  75.224\n",
      "min loss epoch:1\n",
      "max acc epoch：1        max acc：75.224\n",
      "耗时：: 0.03960251808166504\n",
      "epoch:  2 train_loss:  0.482 train_acc:  76.143 test_loss:  0.5 test_acc:  75.821\n",
      "min loss epoch:2\n",
      "max acc epoch：2        max acc：75.821\n",
      "耗时：: 0.03690147399902344\n",
      "epoch:  3 train_loss:  0.48 train_acc:  76.386 test_loss:  0.498 test_acc:  75.597\n",
      "min loss epoch:3\n",
      "耗时：: 0.02493429183959961\n",
      "epoch:  4 train_loss:  0.482 train_acc:  76.087 test_loss:  0.505 test_acc:  74.478\n",
      "耗时：: 0.043847084045410156\n",
      "epoch:  5 train_loss:  0.474 train_acc:  76.517 test_loss:  0.494 test_acc:  75.522\n",
      "min loss epoch:5\n",
      "耗时：: 0.018989086151123047\n",
      "epoch:  6 train_loss:  0.471 train_acc:  76.554 test_loss:  0.493 test_acc:  75.299\n",
      "min loss epoch:6\n",
      "耗时：: 0.0259401798248291\n",
      "epoch:  7 train_loss:  0.47 train_acc:  76.946 test_loss:  0.492 test_acc:  75.448\n",
      "min loss epoch:7\n",
      "耗时：: 0.02194070816040039\n",
      "epoch:  8 train_loss:  0.469 train_acc:  76.722 test_loss:  0.496 test_acc:  75.373\n",
      "耗时：: 0.025930404663085938\n",
      "epoch:  9 train_loss:  0.465 train_acc:  76.442 test_loss:  0.489 test_acc:  75.522\n",
      "min loss epoch:9\n",
      "耗时：: 0.02493119239807129\n",
      "epoch:  10 train_loss:  0.462 train_acc:  77.095 test_loss:  0.485 test_acc:  75.746\n",
      "min loss epoch:10\n",
      "耗时：: 0.02290201187133789\n",
      "epoch:  11 train_loss:  0.458 train_acc:  76.983 test_loss:  0.483 test_acc:  75.597\n",
      "min loss epoch:11\n",
      "耗时：: 0.02588963508605957\n",
      "epoch:  12 train_loss:  0.465 train_acc:  76.143 test_loss:  0.49 test_acc:  75.522\n",
      "耗时：: 0.023934602737426758\n",
      "epoch:  13 train_loss:  0.455 train_acc:  76.647 test_loss:  0.483 test_acc:  75.0\n",
      "min loss epoch:13\n",
      "耗时：: 0.023933887481689453\n",
      "epoch:  14 train_loss:  0.452 train_acc:  76.573 test_loss:  0.479 test_acc:  75.597\n",
      "min loss epoch:14\n",
      "耗时：: 0.03191399574279785\n",
      "epoch:  15 train_loss:  0.449 train_acc:  76.647 test_loss:  0.475 test_acc:  75.896\n",
      "min loss epoch:15\n",
      "max acc epoch：15        max acc：75.896\n",
      "耗时：: 0.02593088150024414\n",
      "epoch:  16 train_loss:  0.449 train_acc:  76.778 test_loss:  0.477 test_acc:  76.119\n",
      "max acc epoch：16        max acc：76.119\n",
      "耗时：: 0.020943403244018555\n",
      "epoch:  17 train_loss:  0.445 train_acc:  77.077 test_loss:  0.468 test_acc:  76.119\n",
      "min loss epoch:17\n",
      "耗时：: 0.029917240142822266\n",
      "epoch:  18 train_loss:  0.442 train_acc:  77.282 test_loss:  0.466 test_acc:  75.746\n",
      "min loss epoch:18\n",
      "耗时：: 0.03091740608215332\n",
      "epoch:  19 train_loss:  0.44 train_acc:  77.189 test_loss:  0.465 test_acc:  75.821\n",
      "min loss epoch:19\n",
      "耗时：: 0.030920743942260742\n",
      "epoch:  20 train_loss:  0.444 train_acc:  76.815 test_loss:  0.469 test_acc:  75.224\n",
      "耗时：: 0.017954587936401367\n",
      "epoch:  21 train_loss:  0.436 train_acc:  77.058 test_loss:  0.465 test_acc:  76.791\n",
      "min loss epoch:21\n",
      "max acc epoch：21        max acc：76.791\n",
      "耗时：: 0.02796196937561035\n",
      "epoch:  22 train_loss:  0.432 train_acc:  77.319 test_loss:  0.463 test_acc:  76.045\n",
      "min loss epoch:22\n",
      "耗时：: 0.03246665000915527\n",
      "epoch:  23 train_loss:  0.434 train_acc:  77.357 test_loss:  0.465 test_acc:  75.97\n",
      "耗时：: 0.02489638328552246\n",
      "epoch:  24 train_loss:  0.433 train_acc:  77.73 test_loss:  0.466 test_acc:  76.194\n",
      "耗时：: 0.023937225341796875\n",
      "epoch:  25 train_loss:  0.433 train_acc:  77.319 test_loss:  0.463 test_acc:  75.075\n",
      "min loss epoch:25\n",
      "耗时：: 0.020943641662597656\n",
      "epoch:  26 train_loss:  0.429 train_acc:  77.375 test_loss:  0.462 test_acc:  75.896\n",
      "min loss epoch:26\n",
      "耗时：: 0.019989728927612305\n",
      "epoch:  27 train_loss:  0.429 train_acc:  77.655 test_loss:  0.463 test_acc:  75.672\n",
      "耗时：: 0.03087592124938965\n",
      "epoch:  28 train_loss:  0.426 train_acc:  77.823 test_loss:  0.462 test_acc:  76.269\n",
      "耗时：: 0.024898529052734375\n",
      "epoch:  29 train_loss:  0.424 train_acc:  77.842 test_loss:  0.46 test_acc:  75.672\n",
      "min loss epoch:29\n",
      "耗时：: 0.018948078155517578\n",
      "epoch:  30 train_loss:  0.422 train_acc:  77.954 test_loss:  0.457 test_acc:  75.448\n",
      "min loss epoch:30\n",
      "耗时：: 0.024936914443969727\n",
      "epoch:  31 train_loss:  0.424 train_acc:  78.01 test_loss:  0.467 test_acc:  75.97\n",
      "耗时：: 0.06781768798828125\n",
      "epoch:  32 train_loss:  0.429 train_acc:  77.226 test_loss:  0.47 test_acc:  75.373\n",
      "耗时：: 0.04845023155212402\n",
      "epoch:  33 train_loss:  0.424 train_acc:  77.693 test_loss:  0.463 test_acc:  75.896\n",
      "耗时：: 0.023497343063354492\n",
      "epoch:  34 train_loss:  0.421 train_acc:  77.917 test_loss:  0.462 test_acc:  75.821\n",
      "耗时：: 0.030951976776123047\n",
      "epoch:  35 train_loss:  0.422 train_acc:  78.122 test_loss:  0.462 test_acc:  75.522\n",
      "耗时：: 0.02293872833251953\n",
      "epoch:  36 train_loss:  0.418 train_acc:  78.066 test_loss:  0.459 test_acc:  75.672\n",
      "耗时：: 0.02988147735595703\n",
      "epoch:  37 train_loss:  0.417 train_acc:  78.066 test_loss:  0.457 test_acc:  75.075\n",
      "耗时：: 0.01898336410522461\n",
      "epoch:  38 train_loss:  0.416 train_acc:  78.383 test_loss:  0.459 test_acc:  75.97\n",
      "耗时：: 0.025928497314453125\n",
      "epoch:  39 train_loss:  0.416 train_acc:  78.29 test_loss:  0.454 test_acc:  75.448\n",
      "min loss epoch:39\n",
      "耗时：: 0.03549313545227051\n",
      "epoch:  40 train_loss:  0.416 train_acc:  77.693 test_loss:  0.456 test_acc:  75.224\n",
      "耗时：: 0.026926040649414062\n",
      "epoch:  41 train_loss:  0.417 train_acc:  78.383 test_loss:  0.454 test_acc:  75.224\n",
      "耗时：: 0.029920101165771484\n",
      "epoch:  42 train_loss:  0.419 train_acc:  78.215 test_loss:  0.462 test_acc:  75.821\n",
      "耗时：: 0.025963306427001953\n",
      "epoch:  43 train_loss:  0.417 train_acc:  77.898 test_loss:  0.463 test_acc:  76.269\n",
      "耗时：: 0.026967287063598633\n",
      "epoch:  44 train_loss:  0.415 train_acc:  78.103 test_loss:  0.454 test_acc:  75.597\n",
      "耗时：: 0.022978544235229492\n",
      "epoch:  45 train_loss:  0.417 train_acc:  77.954 test_loss:  0.459 test_acc:  75.448\n",
      "耗时：: 0.03683280944824219\n",
      "epoch:  46 train_loss:  0.414 train_acc:  78.197 test_loss:  0.456 test_acc:  75.075\n",
      "耗时：: 0.03586721420288086\n",
      "epoch:  47 train_loss:  0.413 train_acc:  78.066 test_loss:  0.453 test_acc:  75.373\n",
      "min loss epoch:47\n",
      "耗时：: 0.03494715690612793\n",
      "epoch:  48 train_loss:  0.412 train_acc:  78.047 test_loss:  0.461 test_acc:  74.627\n",
      "耗时：: 0.02692723274230957\n",
      "epoch:  49 train_loss:  0.408 train_acc:  78.197 test_loss:  0.448 test_acc:  75.224\n",
      "min loss epoch:49\n",
      "耗时：: 0.030917644500732422\n",
      "epoch:  50 train_loss:  0.41 train_acc:  78.197 test_loss:  0.449 test_acc:  75.0\n",
      "耗时：: 0.02361273765563965\n",
      "epoch:  51 train_loss:  0.414 train_acc:  78.178 test_loss:  0.454 test_acc:  75.821\n",
      "耗时：: 0.03151893615722656\n",
      "epoch:  52 train_loss:  0.414 train_acc:  78.253 test_loss:  0.457 test_acc:  75.075\n",
      "耗时：: 0.02892279624938965\n",
      "epoch:  53 train_loss:  0.416 train_acc:  78.029 test_loss:  0.464 test_acc:  76.045\n",
      "耗时：: 0.024973392486572266\n",
      "epoch:  54 train_loss:  0.41 train_acc:  78.495 test_loss:  0.455 test_acc:  75.373\n",
      "耗时：: 0.03195953369140625\n",
      "epoch:  55 train_loss:  0.407 train_acc:  78.645 test_loss:  0.452 test_acc:  74.925\n",
      "耗时：: 0.028905391693115234\n",
      "epoch:  56 train_loss:  0.411 train_acc:  78.439 test_loss:  0.45 test_acc:  75.597\n",
      "耗时：: 0.02689337730407715\n",
      "epoch:  57 train_loss:  0.411 train_acc:  78.514 test_loss:  0.455 test_acc:  76.791\n",
      "耗时：: 0.03391075134277344\n",
      "epoch:  58 train_loss:  0.41 train_acc:  78.271 test_loss:  0.451 test_acc:  76.045\n",
      "耗时：: 0.025931119918823242\n",
      "epoch:  59 train_loss:  0.414 train_acc:  78.309 test_loss:  0.455 test_acc:  75.075\n",
      "耗时：: 0.037900447845458984\n",
      "epoch:  60 train_loss:  0.414 train_acc:  78.365 test_loss:  0.453 test_acc:  74.925\n",
      "耗时：: 0.02589273452758789\n",
      "epoch:  61 train_loss:  0.41 train_acc:  78.383 test_loss:  0.456 test_acc:  75.746\n",
      "耗时：: 0.02861785888671875\n",
      "epoch:  62 train_loss:  0.405 train_acc:  78.365 test_loss:  0.45 test_acc:  74.627\n",
      "耗时：: 0.025968551635742188\n",
      "epoch:  63 train_loss:  0.409 train_acc:  78.327 test_loss:  0.452 test_acc:  75.896\n",
      "耗时：: 0.04349493980407715\n",
      "epoch:  64 train_loss:  0.408 train_acc:  77.973 test_loss:  0.458 test_acc:  76.045\n",
      "耗时：: 0.020982027053833008\n",
      "epoch:  65 train_loss:  0.412 train_acc:  77.954 test_loss:  0.463 test_acc:  75.448\n",
      "耗时：: 0.03390789031982422\n",
      "epoch:  66 train_loss:  0.412 train_acc:  78.645 test_loss:  0.449 test_acc:  76.194\n",
      "耗时：: 0.027885913848876953\n",
      "epoch:  67 train_loss:  0.405 train_acc:  78.57 test_loss:  0.454 test_acc:  74.776\n",
      "耗时：: 0.025930404663085938\n",
      "epoch:  68 train_loss:  0.404 train_acc:  78.775 test_loss:  0.45 test_acc:  74.776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "耗时：: 0.020939350128173828\n",
      "epoch:  69 train_loss:  0.407 train_acc:  78.57 test_loss:  0.447 test_acc:  75.522\n",
      "min loss epoch:69\n",
      "耗时：: 0.020944595336914062\n",
      "epoch:  70 train_loss:  0.411 train_acc:  78.626 test_loss:  0.455 test_acc:  75.821\n",
      "耗时：: 0.027924537658691406\n",
      "epoch:  71 train_loss:  0.411 train_acc:  78.458 test_loss:  0.448 test_acc:  75.149\n",
      "耗时：: 0.022937774658203125\n",
      "epoch:  72 train_loss:  0.405 train_acc:  78.551 test_loss:  0.452 test_acc:  76.119\n",
      "耗时：: 0.022940874099731445\n",
      "epoch:  73 train_loss:  0.406 train_acc:  78.458 test_loss:  0.449 test_acc:  76.418\n",
      "耗时：: 0.02098536491394043\n",
      "epoch:  74 train_loss:  0.404 train_acc:  78.645 test_loss:  0.449 test_acc:  76.418\n",
      "耗时：: 0.01994609832763672\n",
      "epoch:  75 train_loss:  0.409 train_acc:  78.439 test_loss:  0.454 test_acc:  76.716\n",
      "耗时：: 0.018990516662597656\n",
      "epoch:  76 train_loss:  0.407 train_acc:  78.869 test_loss:  0.455 test_acc:  76.119\n",
      "耗时：: 0.027881383895874023\n",
      "epoch:  77 train_loss:  0.405 train_acc:  78.719 test_loss:  0.455 test_acc:  76.045\n",
      "耗时：: 0.026927709579467773\n",
      "epoch:  78 train_loss:  0.415 train_acc:  78.365 test_loss:  0.46 test_acc:  75.299\n",
      "耗时：: 0.025931358337402344\n",
      "epoch:  79 train_loss:  0.402 train_acc:  78.962 test_loss:  0.449 test_acc:  75.746\n",
      "耗时：: 0.02692723274230957\n",
      "epoch:  80 train_loss:  0.407 train_acc:  78.346 test_loss:  0.455 test_acc:  75.522\n",
      "耗时：: 0.03187823295593262\n",
      "epoch:  81 train_loss:  0.401 train_acc:  78.57 test_loss:  0.447 test_acc:  75.0\n",
      "耗时：: 0.024931669235229492\n",
      "epoch:  82 train_loss:  0.408 train_acc:  78.234 test_loss:  0.459 test_acc:  75.075\n",
      "耗时：: 0.023935794830322266\n",
      "epoch:  83 train_loss:  0.403 train_acc:  78.57 test_loss:  0.446 test_acc:  76.119\n",
      "min loss epoch:83\n",
      "耗时：: 0.022942066192626953\n",
      "epoch:  84 train_loss:  0.408 train_acc:  78.421 test_loss:  0.458 test_acc:  75.373\n",
      "耗时：: 0.023935794830322266\n",
      "epoch:  85 train_loss:  0.411 train_acc:  78.346 test_loss:  0.457 test_acc:  76.343\n",
      "耗时：: 0.018947601318359375\n",
      "epoch:  86 train_loss:  0.405 train_acc:  78.719 test_loss:  0.457 test_acc:  75.224\n",
      "耗时：: 0.028920650482177734\n",
      "epoch:  87 train_loss:  0.401 train_acc:  78.757 test_loss:  0.444 test_acc:  75.597\n",
      "min loss epoch:87\n",
      "耗时：: 0.023976802825927734\n",
      "epoch:  88 train_loss:  0.402 train_acc:  79.074 test_loss:  0.45 test_acc:  75.896\n",
      "耗时：: 0.01890850067138672\n",
      "epoch:  89 train_loss:  0.399 train_acc:  78.831 test_loss:  0.45 test_acc:  75.97\n",
      "耗时：: 0.02892279624938965\n",
      "epoch:  90 train_loss:  0.4 train_acc:  78.887 test_loss:  0.45 test_acc:  74.776\n",
      "耗时：: 0.02098369598388672\n",
      "epoch:  91 train_loss:  0.402 train_acc:  78.775 test_loss:  0.452 test_acc:  75.672\n",
      "耗时：: 0.023897171020507812\n",
      "epoch:  92 train_loss:  0.406 train_acc:  78.738 test_loss:  0.445 test_acc:  75.746\n",
      "耗时：: 0.03191423416137695\n",
      "epoch:  93 train_loss:  0.404 train_acc:  78.253 test_loss:  0.451 test_acc:  76.119\n",
      "耗时：: 0.025892972946166992\n",
      "epoch:  94 train_loss:  0.403 train_acc:  78.421 test_loss:  0.453 test_acc:  75.224\n",
      "耗时：: 0.030879974365234375\n",
      "epoch:  95 train_loss:  0.4 train_acc:  78.943 test_loss:  0.444 test_acc:  75.448\n",
      "耗时：: 0.019947052001953125\n",
      "epoch:  96 train_loss:  0.401 train_acc:  79.111 test_loss:  0.447 test_acc:  75.522\n",
      "耗时：: 0.04088997840881348\n",
      "epoch:  97 train_loss:  0.4 train_acc:  79.149 test_loss:  0.444 test_acc:  75.373\n",
      "耗时：: 0.022937536239624023\n",
      "epoch:  98 train_loss:  0.401 train_acc:  79.018 test_loss:  0.449 test_acc:  74.627\n",
      "耗时：: 0.018950939178466797\n",
      "epoch:  99 train_loss:  0.4 train_acc:  78.943 test_loss:  0.446 test_acc:  75.672\n",
      "耗时：: 0.020945072174072266\n",
      "epoch:  100 train_loss:  0.404 train_acc:  78.383 test_loss:  0.459 test_acc:  75.821\n",
      "耗时：: 0.02396988868713379\n",
      "epoch:  101 train_loss:  0.397 train_acc:  79.335 test_loss:  0.443 test_acc:  75.522\n",
      "min loss epoch:101\n",
      "耗时：: 0.019681453704833984\n",
      "epoch:  102 train_loss:  0.4 train_acc:  79.037 test_loss:  0.449 test_acc:  75.299\n",
      "耗时：: 0.01990985870361328\n",
      "epoch:  103 train_loss:  0.397 train_acc:  79.317 test_loss:  0.447 test_acc:  75.522\n",
      "耗时：: 0.0279238224029541\n",
      "epoch:  104 train_loss:  0.401 train_acc:  78.458 test_loss:  0.442 test_acc:  75.448\n",
      "min loss epoch:104\n",
      "耗时：: 0.0238950252532959\n",
      "epoch:  105 train_loss:  0.398 train_acc:  78.962 test_loss:  0.443 test_acc:  75.597\n",
      "耗时：: 0.02688884735107422\n",
      "epoch:  106 train_loss:  0.409 train_acc:  78.701 test_loss:  0.447 test_acc:  75.597\n",
      "耗时：: 0.02493143081665039\n",
      "epoch:  107 train_loss:  0.401 train_acc:  79.261 test_loss:  0.445 test_acc:  76.269\n",
      "耗时：: 0.024892807006835938\n",
      "epoch:  108 train_loss:  0.4 train_acc:  79.074 test_loss:  0.445 test_acc:  76.642\n",
      "耗时：: 0.031952857971191406\n",
      "epoch:  109 train_loss:  0.397 train_acc:  78.925 test_loss:  0.448 test_acc:  75.522\n",
      "耗时：: 0.031949758529663086\n",
      "epoch:  110 train_loss:  0.405 train_acc:  78.887 test_loss:  0.465 test_acc:  75.821\n",
      "耗时：: 0.03095269203186035\n",
      "epoch:  111 train_loss:  0.397 train_acc:  79.391 test_loss:  0.441 test_acc:  76.194\n",
      "min loss epoch:111\n",
      "耗时：: 0.022979736328125\n",
      "epoch:  112 train_loss:  0.396 train_acc:  79.186 test_loss:  0.44 test_acc:  76.94\n",
      "min loss epoch:112\n",
      "max acc epoch：112        max acc：76.94\n",
      "耗时：: 0.018949270248413086\n",
      "epoch:  113 train_loss:  0.396 train_acc:  79.354 test_loss:  0.444 test_acc:  75.373\n",
      "耗时：: 0.024932384490966797\n",
      "epoch:  114 train_loss:  0.392 train_acc:  78.962 test_loss:  0.443 test_acc:  75.448\n",
      "耗时：: 0.021938800811767578\n",
      "epoch:  115 train_loss:  0.397 train_acc:  79.055 test_loss:  0.445 test_acc:  76.194\n",
      "耗时：: 0.025560855865478516\n",
      "epoch:  116 train_loss:  0.402 train_acc:  79.149 test_loss:  0.451 test_acc:  75.821\n",
      "耗时：: 0.023615360260009766\n",
      "epoch:  117 train_loss:  0.396 train_acc:  78.925 test_loss:  0.451 test_acc:  75.597\n",
      "耗时：: 0.019945144653320312\n",
      "epoch:  118 train_loss:  0.394 train_acc:  79.242 test_loss:  0.449 test_acc:  75.821\n",
      "耗时：: 0.03291058540344238\n",
      "epoch:  119 train_loss:  0.393 train_acc:  78.925 test_loss:  0.443 test_acc:  75.821\n",
      "耗时：: 0.023936748504638672\n",
      "epoch:  120 train_loss:  0.396 train_acc:  79.13 test_loss:  0.449 test_acc:  76.493\n",
      "耗时：: 0.03394675254821777\n",
      "epoch:  121 train_loss:  0.398 train_acc:  79.149 test_loss:  0.445 test_acc:  75.597\n",
      "耗时：: 0.023971080780029297\n",
      "epoch:  122 train_loss:  0.392 train_acc:  79.223 test_loss:  0.448 test_acc:  75.746\n",
      "耗时：: 0.0195467472076416\n",
      "epoch:  123 train_loss:  0.393 train_acc:  79.466 test_loss:  0.44 test_acc:  77.015\n",
      "min loss epoch:123\n",
      "max acc epoch：123        max acc：77.015\n",
      "耗时：: 0.024935007095336914\n",
      "epoch:  124 train_loss:  0.393 train_acc:  79.13 test_loss:  0.441 test_acc:  75.149\n",
      "耗时：: 0.0318758487701416\n",
      "epoch:  125 train_loss:  0.402 train_acc:  78.85 test_loss:  0.458 test_acc:  75.97\n",
      "耗时：: 0.019948720932006836\n",
      "epoch:  126 train_loss:  0.4 train_acc:  78.962 test_loss:  0.459 test_acc:  75.97\n",
      "耗时：: 0.018950700759887695\n",
      "epoch:  127 train_loss:  0.393 train_acc:  79.298 test_loss:  0.44 test_acc:  75.896\n",
      "耗时：: 0.027925491333007812\n",
      "epoch:  128 train_loss:  0.399 train_acc:  78.775 test_loss:  0.453 test_acc:  76.119\n",
      "耗时：: 0.01894688606262207\n",
      "epoch:  129 train_loss:  0.392 train_acc:  79.205 test_loss:  0.445 test_acc:  76.045\n",
      "耗时：: 0.029920101165771484\n",
      "epoch:  130 train_loss:  0.398 train_acc:  79.466 test_loss:  0.447 test_acc:  76.269\n",
      "耗时：: 0.0318751335144043\n",
      "epoch:  131 train_loss:  0.395 train_acc:  79.186 test_loss:  0.446 test_acc:  75.97\n",
      "耗时：: 0.018950939178466797\n",
      "epoch:  132 train_loss:  0.405 train_acc:  78.421 test_loss:  0.455 test_acc:  75.224\n",
      "耗时：: 0.024543046951293945\n",
      "epoch:  133 train_loss:  0.394 train_acc:  79.503 test_loss:  0.444 test_acc:  76.045\n",
      "耗时：: 0.02451491355895996\n",
      "epoch:  134 train_loss:  0.394 train_acc:  79.242 test_loss:  0.45 test_acc:  76.045\n",
      "耗时：: 0.020943403244018555\n",
      "epoch:  135 train_loss:  0.392 train_acc:  79.522 test_loss:  0.442 test_acc:  75.299\n",
      "耗时：: 0.02592945098876953\n",
      "epoch:  136 train_loss:  0.404 train_acc:  78.831 test_loss:  0.453 test_acc:  74.552\n",
      "耗时：: 0.01994633674621582\n",
      "epoch:  137 train_loss:  0.39 train_acc:  79.541 test_loss:  0.443 test_acc:  76.194\n",
      "耗时：: 0.03091573715209961\n",
      "epoch:  138 train_loss:  0.396 train_acc:  78.981 test_loss:  0.453 test_acc:  75.448\n",
      "耗时：: 0.03080153465270996\n",
      "epoch:  139 train_loss:  0.397 train_acc:  78.981 test_loss:  0.444 test_acc:  75.522\n",
      "耗时：: 0.019947290420532227\n",
      "epoch:  140 train_loss:  0.396 train_acc:  79.615 test_loss:  0.459 test_acc:  75.299\n",
      "耗时：: 0.03790092468261719\n",
      "epoch:  141 train_loss:  0.393 train_acc:  79.149 test_loss:  0.449 test_acc:  75.821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "耗时：: 0.02792668342590332\n",
      "epoch:  142 train_loss:  0.388 train_acc:  79.559 test_loss:  0.445 test_acc:  75.0\n",
      "耗时：: 0.028948307037353516\n",
      "epoch:  143 train_loss:  0.407 train_acc:  78.887 test_loss:  0.454 test_acc:  75.597\n",
      "耗时：: 0.021941184997558594\n",
      "epoch:  144 train_loss:  0.392 train_acc:  79.466 test_loss:  0.451 test_acc:  75.821\n",
      "耗时：: 0.02992081642150879\n",
      "epoch:  145 train_loss:  0.393 train_acc:  79.111 test_loss:  0.452 test_acc:  75.373\n",
      "耗时：: 0.031914710998535156\n",
      "epoch:  146 train_loss:  0.389 train_acc:  79.391 test_loss:  0.445 test_acc:  76.567\n",
      "耗时：: 0.024934768676757812\n",
      "epoch:  147 train_loss:  0.391 train_acc:  79.466 test_loss:  0.449 test_acc:  75.597\n",
      "耗时：: 0.03191637992858887\n",
      "epoch:  148 train_loss:  0.391 train_acc:  79.466 test_loss:  0.448 test_acc:  75.896\n",
      "耗时：: 0.028922080993652344\n",
      "epoch:  149 train_loss:  0.392 train_acc:  79.093 test_loss:  0.45 test_acc:  75.299\n",
      "耗时：: 0.024935007095336914\n",
      "epoch:  150 train_loss:  0.388 train_acc:  79.914 test_loss:  0.449 test_acc:  75.373\n",
      "耗时：: 0.02991962432861328\n",
      "epoch:  151 train_loss:  0.386 train_acc:  79.802 test_loss:  0.447 test_acc:  76.343\n",
      "耗时：: 0.024933338165283203\n",
      "epoch:  152 train_loss:  0.391 train_acc:  79.559 test_loss:  0.442 test_acc:  76.119\n",
      "耗时：: 0.020534515380859375\n",
      "epoch:  153 train_loss:  0.392 train_acc:  79.653 test_loss:  0.448 test_acc:  75.896\n",
      "耗时：: 0.020941734313964844\n",
      "epoch:  154 train_loss:  0.388 train_acc:  79.951 test_loss:  0.449 test_acc:  76.194\n",
      "耗时：: 0.03490114212036133\n",
      "epoch:  155 train_loss:  0.386 train_acc:  80.101 test_loss:  0.444 test_acc:  76.194\n",
      "耗时：: 0.02094292640686035\n",
      "epoch:  156 train_loss:  0.386 train_acc:  80.082 test_loss:  0.444 test_acc:  76.343\n",
      "耗时：: 0.025931358337402344\n",
      "epoch:  157 train_loss:  0.391 train_acc:  79.597 test_loss:  0.449 test_acc:  75.522\n",
      "耗时：: 0.03291153907775879\n",
      "epoch:  158 train_loss:  0.384 train_acc:  80.175 test_loss:  0.442 test_acc:  75.597\n",
      "耗时：: 0.022937774658203125\n",
      "epoch:  159 train_loss:  0.388 train_acc:  79.746 test_loss:  0.45 test_acc:  76.119\n",
      "耗时：: 0.03291130065917969\n",
      "epoch:  160 train_loss:  0.416 train_acc:  78.103 test_loss:  0.482 test_acc:  75.448\n",
      "耗时：: 0.02193927764892578\n",
      "epoch:  161 train_loss:  0.392 train_acc:  79.447 test_loss:  0.448 test_acc:  75.373\n",
      "耗时：: 0.024930953979492188\n",
      "epoch:  162 train_loss:  0.385 train_acc:  79.914 test_loss:  0.452 test_acc:  75.373\n",
      "耗时：: 0.03789520263671875\n",
      "epoch:  163 train_loss:  0.388 train_acc:  80.026 test_loss:  0.444 test_acc:  75.97\n",
      "耗时：: 0.03191494941711426\n",
      "epoch:  164 train_loss:  0.387 train_acc:  80.045 test_loss:  0.447 test_acc:  76.045\n",
      "耗时：: 0.017950057983398438\n",
      "epoch:  165 train_loss:  0.394 train_acc:  79.485 test_loss:  0.457 test_acc:  75.448\n",
      "耗时：: 0.034912824630737305\n",
      "epoch:  166 train_loss:  0.393 train_acc:  79.541 test_loss:  0.454 test_acc:  75.821\n",
      "耗时：: 0.04886817932128906\n",
      "epoch:  167 train_loss:  0.388 train_acc:  79.541 test_loss:  0.452 test_acc:  75.597\n",
      "耗时：: 0.03191351890563965\n",
      "epoch:  168 train_loss:  0.394 train_acc:  79.093 test_loss:  0.46 test_acc:  74.851\n",
      "耗时：: 0.018909692764282227\n",
      "epoch:  169 train_loss:  0.383 train_acc:  79.933 test_loss:  0.446 test_acc:  76.716\n",
      "耗时：: 0.022937297821044922\n",
      "epoch:  170 train_loss:  0.388 train_acc:  79.447 test_loss:  0.443 test_acc:  76.269\n",
      "耗时：: 0.019945621490478516\n",
      "epoch:  171 train_loss:  0.391 train_acc:  79.335 test_loss:  0.444 test_acc:  75.448\n",
      "耗时：: 0.019943952560424805\n",
      "epoch:  172 train_loss:  0.384 train_acc:  79.802 test_loss:  0.448 test_acc:  75.896\n",
      "耗时：: 0.03490781784057617\n",
      "epoch:  173 train_loss:  0.389 train_acc:  79.746 test_loss:  0.451 test_acc:  75.821\n",
      "耗时：: 0.030916929244995117\n",
      "epoch:  174 train_loss:  0.384 train_acc:  79.709 test_loss:  0.438 test_acc:  75.448\n",
      "min loss epoch:174\n",
      "耗时：: 0.04288434982299805\n",
      "epoch:  175 train_loss:  0.382 train_acc:  80.269 test_loss:  0.443 test_acc:  75.97\n",
      "耗时：: 0.03291177749633789\n",
      "epoch:  176 train_loss:  0.394 train_acc:  79.354 test_loss:  0.455 test_acc:  76.642\n",
      "耗时：: 0.024933338165283203\n",
      "epoch:  177 train_loss:  0.386 train_acc:  79.522 test_loss:  0.447 test_acc:  75.522\n",
      "耗时：: 0.01994466781616211\n",
      "epoch:  178 train_loss:  0.394 train_acc:  79.279 test_loss:  0.46 test_acc:  75.448\n",
      "耗时：: 0.03291058540344238\n",
      "epoch:  179 train_loss:  0.396 train_acc:  78.514 test_loss:  0.461 test_acc:  75.224\n",
      "耗时：: 0.021941661834716797\n",
      "epoch:  180 train_loss:  0.385 train_acc:  79.783 test_loss:  0.449 test_acc:  75.672\n",
      "耗时：: 0.019945859909057617\n",
      "epoch:  181 train_loss:  0.386 train_acc:  79.933 test_loss:  0.45 test_acc:  75.97\n",
      "耗时：: 0.02991795539855957\n",
      "epoch:  182 train_loss:  0.383 train_acc:  80.213 test_loss:  0.449 test_acc:  76.194\n",
      "耗时：: 0.028922557830810547\n",
      "epoch:  183 train_loss:  0.385 train_acc:  80.605 test_loss:  0.446 test_acc:  75.821\n",
      "耗时：: 0.021942615509033203\n",
      "epoch:  184 train_loss:  0.387 train_acc:  79.746 test_loss:  0.451 test_acc:  75.746\n",
      "耗时：: 0.02596569061279297\n",
      "epoch:  185 train_loss:  0.396 train_acc:  79.13 test_loss:  0.461 test_acc:  75.97\n",
      "耗时：: 0.024934053421020508\n",
      "epoch:  186 train_loss:  0.385 train_acc:  79.653 test_loss:  0.448 test_acc:  75.821\n",
      "耗时：: 0.019946575164794922\n",
      "epoch:  187 train_loss:  0.388 train_acc:  79.578 test_loss:  0.454 test_acc:  76.269\n",
      "耗时：: 0.021941184997558594\n",
      "epoch:  188 train_loss:  0.38 train_acc:  80.381 test_loss:  0.449 test_acc:  75.373\n",
      "耗时：: 0.026928424835205078\n",
      "epoch:  189 train_loss:  0.379 train_acc:  80.082 test_loss:  0.448 test_acc:  75.821\n",
      "耗时：: 0.01990675926208496\n",
      "epoch:  190 train_loss:  0.385 train_acc:  79.802 test_loss:  0.449 test_acc:  75.896\n",
      "耗时：: 0.024931669235229492\n",
      "epoch:  191 train_loss:  0.39 train_acc:  79.671 test_loss:  0.462 test_acc:  75.075\n",
      "耗时：: 0.030916452407836914\n",
      "epoch:  192 train_loss:  0.383 train_acc:  79.877 test_loss:  0.446 test_acc:  75.746\n",
      "耗时：: 0.021981477737426758\n",
      "epoch:  193 train_loss:  0.384 train_acc:  79.858 test_loss:  0.456 test_acc:  74.925\n",
      "耗时：: 0.029921531677246094\n",
      "epoch:  194 train_loss:  0.383 train_acc:  79.914 test_loss:  0.447 test_acc:  76.866\n",
      "耗时：: 0.01894664764404297\n",
      "epoch:  195 train_loss:  0.388 train_acc:  79.167 test_loss:  0.456 test_acc:  75.448\n",
      "耗时：: 0.03249335289001465\n",
      "epoch:  196 train_loss:  0.389 train_acc:  79.933 test_loss:  0.465 test_acc:  76.418\n",
      "耗时：: 0.01894664764404297\n",
      "epoch:  197 train_loss:  0.388 train_acc:  79.821 test_loss:  0.451 test_acc:  75.97\n",
      "耗时：: 0.018950462341308594\n",
      "epoch:  198 train_loss:  0.388 train_acc:  79.802 test_loss:  0.45 test_acc:  75.373\n",
      "耗时：: 0.029183149337768555\n",
      "epoch:  199 train_loss:  0.38 train_acc:  80.138 test_loss:  0.451 test_acc:  76.567\n",
      "耗时：: 0.020943880081176758\n",
      "epoch:  200 train_loss:  0.382 train_acc:  79.671 test_loss:  0.45 test_acc:  76.045\n",
      "耗时：: 0.017987728118896484\n",
      "epoch:  201 train_loss:  0.384 train_acc:  79.634 test_loss:  0.454 test_acc:  75.373\n",
      "耗时：: 0.02491021156311035\n",
      "epoch:  202 train_loss:  0.384 train_acc:  79.242 test_loss:  0.451 test_acc:  76.418\n",
      "耗时：: 0.0249330997467041\n",
      "epoch:  203 train_loss:  0.383 train_acc:  80.194 test_loss:  0.462 test_acc:  75.149\n",
      "耗时：: 0.023935556411743164\n",
      "epoch:  204 train_loss:  0.383 train_acc:  79.466 test_loss:  0.448 test_acc:  75.746\n",
      "耗时：: 0.020982980728149414\n",
      "epoch:  205 train_loss:  0.381 train_acc:  80.25 test_loss:  0.45 test_acc:  76.194\n",
      "耗时：: 0.02194046974182129\n",
      "epoch:  206 train_loss:  0.378 train_acc:  80.362 test_loss:  0.445 test_acc:  76.119\n",
      "耗时：: 0.02493429183959961\n",
      "epoch:  207 train_loss:  0.394 train_acc:  79.503 test_loss:  0.458 test_acc:  76.045\n",
      "耗时：: 0.01795172691345215\n",
      "epoch:  208 train_loss:  0.394 train_acc:  78.981 test_loss:  0.461 test_acc:  75.821\n",
      "耗时：: 0.024970054626464844\n",
      "epoch:  209 train_loss:  0.381 train_acc:  80.399 test_loss:  0.455 test_acc:  75.896\n",
      "耗时：: 0.029919147491455078\n",
      "epoch:  210 train_loss:  0.382 train_acc:  79.783 test_loss:  0.457 test_acc:  75.597\n",
      "耗时：: 0.018949031829833984\n",
      "epoch:  211 train_loss:  0.377 train_acc:  79.765 test_loss:  0.445 test_acc:  75.821\n",
      "耗时：: 0.02489781379699707\n",
      "epoch:  212 train_loss:  0.393 train_acc:  79.559 test_loss:  0.475 test_acc:  76.045\n",
      "耗时：: 0.01890873908996582\n",
      "epoch:  213 train_loss:  0.38 train_acc:  80.063 test_loss:  0.455 test_acc:  76.045\n",
      "耗时：: 0.031958818435668945\n",
      "epoch:  214 train_loss:  0.382 train_acc:  80.474 test_loss:  0.461 test_acc:  77.239\n",
      "max acc epoch：214        max acc：77.239\n",
      "耗时：: 0.027887582778930664\n",
      "epoch:  215 train_loss:  0.38 train_acc:  80.474 test_loss:  0.448 test_acc:  76.642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "耗时：: 0.018947362899780273\n",
      "epoch:  216 train_loss:  0.378 train_acc:  80.138 test_loss:  0.445 test_acc:  75.97\n",
      "耗时：: 0.021938562393188477\n",
      "epoch:  217 train_loss:  0.384 train_acc:  79.97 test_loss:  0.462 test_acc:  76.493\n",
      "耗时：: 0.027925729751586914\n",
      "epoch:  218 train_loss:  0.381 train_acc:  80.623 test_loss:  0.459 test_acc:  76.493\n",
      "耗时：: 0.02592945098876953\n",
      "epoch:  219 train_loss:  0.393 train_acc:  79.261 test_loss:  0.455 test_acc:  75.97\n",
      "耗时：: 0.035903215408325195\n",
      "epoch:  220 train_loss:  0.381 train_acc:  80.791 test_loss:  0.446 test_acc:  76.045\n",
      "耗时：: 0.027924776077270508\n",
      "epoch:  221 train_loss:  0.379 train_acc:  80.269 test_loss:  0.458 test_acc:  76.045\n",
      "耗时：: 0.022938966751098633\n",
      "epoch:  222 train_loss:  0.379 train_acc:  80.754 test_loss:  0.451 test_acc:  76.269\n",
      "耗时：: 0.018948793411254883\n",
      "epoch:  223 train_loss:  0.375 train_acc:  80.586 test_loss:  0.447 test_acc:  76.567\n",
      "耗时：: 0.017960309982299805\n",
      "epoch:  224 train_loss:  0.376 train_acc:  80.717 test_loss:  0.447 test_acc:  76.119\n",
      "耗时：: 0.017951488494873047\n",
      "epoch:  225 train_loss:  0.375 train_acc:  80.698 test_loss:  0.444 test_acc:  76.269\n",
      "耗时：: 0.01891016960144043\n",
      "epoch:  226 train_loss:  0.377 train_acc:  80.231 test_loss:  0.452 test_acc:  76.642\n",
      "耗时：: 0.024895429611206055\n",
      "epoch:  227 train_loss:  0.378 train_acc:  80.045 test_loss:  0.455 test_acc:  75.821\n",
      "耗时：: 0.0209047794342041\n",
      "epoch:  228 train_loss:  0.374 train_acc:  80.81 test_loss:  0.452 test_acc:  75.896\n",
      "耗时：: 0.021515369415283203\n",
      "epoch:  229 train_loss:  0.376 train_acc:  80.698 test_loss:  0.444 test_acc:  76.493\n",
      "耗时：: 0.021976709365844727\n",
      "epoch:  230 train_loss:  0.374 train_acc:  80.922 test_loss:  0.451 test_acc:  75.597\n",
      "耗时：: 0.018949508666992188\n",
      "epoch:  231 train_loss:  0.391 train_acc:  79.597 test_loss:  0.463 test_acc:  76.493\n",
      "耗时：: 0.027927637100219727\n",
      "epoch:  232 train_loss:  0.377 train_acc:  80.661 test_loss:  0.447 test_acc:  76.269\n",
      "耗时：: 0.03191423416137695\n",
      "epoch:  233 train_loss:  0.38 train_acc:  79.839 test_loss:  0.454 test_acc:  76.269\n",
      "耗时：: 0.024931907653808594\n",
      "epoch:  234 train_loss:  0.376 train_acc:  80.698 test_loss:  0.451 test_acc:  76.567\n",
      "耗时：: 0.03087902069091797\n",
      "epoch:  235 train_loss:  0.375 train_acc:  80.437 test_loss:  0.457 test_acc:  75.373\n",
      "耗时：: 0.01890277862548828\n",
      "epoch:  236 train_loss:  0.376 train_acc:  80.549 test_loss:  0.447 test_acc:  76.343\n",
      "耗时：: 0.018949270248413086\n",
      "epoch:  237 train_loss:  0.379 train_acc:  80.474 test_loss:  0.453 test_acc:  76.567\n",
      "耗时：: 0.02194070816040039\n",
      "epoch:  238 train_loss:  0.376 train_acc:  80.623 test_loss:  0.457 test_acc:  76.716\n",
      "耗时：: 0.029920101165771484\n",
      "epoch:  239 train_loss:  0.379 train_acc:  80.511 test_loss:  0.444 test_acc:  76.716\n",
      "耗时：: 0.02796316146850586\n",
      "epoch:  240 train_loss:  0.391 train_acc:  79.653 test_loss:  0.457 test_acc:  75.97\n",
      "耗时：: 0.031916141510009766\n",
      "epoch:  241 train_loss:  0.373 train_acc:  80.623 test_loss:  0.461 test_acc:  75.896\n",
      "耗时：: 0.02094268798828125\n",
      "epoch:  242 train_loss:  0.373 train_acc:  80.437 test_loss:  0.448 test_acc:  76.716\n",
      "耗时：: 0.020936965942382812\n",
      "epoch:  243 train_loss:  0.372 train_acc:  80.754 test_loss:  0.451 test_acc:  76.119\n",
      "耗时：: 0.027923107147216797\n",
      "epoch:  244 train_loss:  0.377 train_acc:  80.231 test_loss:  0.454 test_acc:  76.791\n",
      "耗时：: 0.01795220375061035\n",
      "epoch:  245 train_loss:  0.374 train_acc:  80.661 test_loss:  0.447 test_acc:  76.716\n",
      "耗时：: 0.027923583984375\n",
      "epoch:  246 train_loss:  0.38 train_acc:  80.231 test_loss:  0.452 test_acc:  75.522\n",
      "耗时：: 0.01894855499267578\n",
      "epoch:  247 train_loss:  0.375 train_acc:  80.231 test_loss:  0.453 test_acc:  76.567\n",
      "耗时：: 0.01894855499267578\n",
      "epoch:  248 train_loss:  0.394 train_acc:  79.335 test_loss:  0.463 test_acc:  75.448\n",
      "耗时：: 0.024899005889892578\n",
      "epoch:  249 train_loss:  0.371 train_acc:  80.586 test_loss:  0.456 test_acc:  75.299\n",
      "耗时：: 0.018949031829833984\n",
      "epoch:  250 train_loss:  0.38 train_acc:  80.81 test_loss:  0.475 test_acc:  76.194\n",
      "耗时：: 0.030952930450439453\n",
      "epoch:  251 train_loss:  0.372 train_acc:  80.455 test_loss:  0.453 test_acc:  75.821\n",
      "耗时：: 0.031914472579956055\n",
      "epoch:  252 train_loss:  0.378 train_acc:  80.53 test_loss:  0.461 test_acc:  74.851\n",
      "耗时：: 0.023935794830322266\n",
      "epoch:  253 train_loss:  0.378 train_acc:  80.605 test_loss:  0.454 test_acc:  76.418\n",
      "耗时：: 0.01894998550415039\n",
      "epoch:  254 train_loss:  0.38 train_acc:  80.045 test_loss:  0.456 test_acc:  76.716\n",
      "耗时：: 0.018950700759887695\n",
      "epoch:  255 train_loss:  0.372 train_acc:  80.493 test_loss:  0.458 test_acc:  76.642\n",
      "耗时：: 0.018988370895385742\n",
      "epoch:  256 train_loss:  0.374 train_acc:  81.09 test_loss:  0.448 test_acc:  75.746\n",
      "耗时：: 0.02297520637512207\n",
      "epoch:  257 train_loss:  0.368 train_acc:  80.661 test_loss:  0.444 test_acc:  76.194\n",
      "耗时：: 0.021938085556030273\n",
      "epoch:  258 train_loss:  0.373 train_acc:  80.754 test_loss:  0.453 test_acc:  77.313\n",
      "max acc epoch：258        max acc：77.313\n",
      "耗时：: 0.019909143447875977\n",
      "epoch:  259 train_loss:  0.376 train_acc:  80.642 test_loss:  0.452 test_acc:  76.716\n",
      "耗时：: 0.023938655853271484\n",
      "epoch:  260 train_loss:  0.385 train_acc:  80.026 test_loss:  0.459 test_acc:  76.269\n",
      "耗时：: 0.020941972732543945\n",
      "epoch:  261 train_loss:  0.371 train_acc:  80.53 test_loss:  0.452 test_acc:  75.672\n",
      "耗时：: 0.026887893676757812\n",
      "epoch:  262 train_loss:  0.378 train_acc:  80.567 test_loss:  0.458 test_acc:  75.597\n",
      "耗时：: 0.02692556381225586\n",
      "epoch:  263 train_loss:  0.374 train_acc:  80.362 test_loss:  0.445 test_acc:  77.239\n",
      "耗时：: 0.01994800567626953\n",
      "epoch:  264 train_loss:  0.375 train_acc:  80.343 test_loss:  0.463 test_acc:  75.821\n",
      "耗时：: 0.031915903091430664\n",
      "epoch:  265 train_loss:  0.372 train_acc:  81.202 test_loss:  0.448 test_acc:  76.493\n",
      "耗时：: 0.02393198013305664\n",
      "epoch:  266 train_loss:  0.376 train_acc:  80.959 test_loss:  0.454 test_acc:  76.418\n",
      "耗时：: 0.030878067016601562\n",
      "epoch:  267 train_loss:  0.371 train_acc:  80.903 test_loss:  0.451 test_acc:  75.896\n",
      "耗时：: 0.020944833755493164\n",
      "epoch:  268 train_loss:  0.372 train_acc:  80.567 test_loss:  0.459 test_acc:  76.045\n",
      "耗时：: 0.018908977508544922\n",
      "epoch:  269 train_loss:  0.375 train_acc:  80.661 test_loss:  0.446 test_acc:  76.418\n",
      "耗时：: 0.017951488494873047\n",
      "epoch:  270 train_loss:  0.367 train_acc:  80.81 test_loss:  0.451 test_acc:  76.642\n",
      "耗时：: 0.018950462341308594\n",
      "epoch:  271 train_loss:  0.369 train_acc:  80.623 test_loss:  0.446 test_acc:  77.164\n",
      "耗时：: 0.017956256866455078\n",
      "epoch:  272 train_loss:  0.375 train_acc:  80.362 test_loss:  0.451 test_acc:  76.791\n",
      "耗时：: 0.025931596755981445\n",
      "epoch:  273 train_loss:  0.375 train_acc:  80.922 test_loss:  0.466 test_acc:  76.343\n",
      "耗时：: 0.03191423416137695\n",
      "epoch:  274 train_loss:  0.374 train_acc:  79.895 test_loss:  0.451 test_acc:  75.821\n",
      "耗时：: 0.0249326229095459\n",
      "epoch:  275 train_loss:  0.37 train_acc:  80.866 test_loss:  0.446 test_acc:  76.418\n",
      "耗时：: 0.025930166244506836\n",
      "epoch:  276 train_loss:  0.375 train_acc:  80.605 test_loss:  0.451 test_acc:  75.746\n",
      "耗时：: 0.026927947998046875\n",
      "epoch:  277 train_loss:  0.37 train_acc:  80.455 test_loss:  0.451 test_acc:  75.821\n",
      "耗时：: 0.021673917770385742\n",
      "epoch:  278 train_loss:  0.374 train_acc:  80.53 test_loss:  0.451 test_acc:  76.269\n",
      "耗时：: 0.029959440231323242\n",
      "epoch:  279 train_loss:  0.373 train_acc:  80.829 test_loss:  0.47 test_acc:  76.343\n",
      "耗时：: 0.02393507957458496\n",
      "epoch:  280 train_loss:  0.382 train_acc:  79.914 test_loss:  0.479 test_acc:  76.343\n",
      "耗时：: 0.022938251495361328\n",
      "epoch:  281 train_loss:  0.368 train_acc:  80.735 test_loss:  0.451 test_acc:  76.94\n",
      "耗时：: 0.020943641662597656\n",
      "epoch:  282 train_loss:  0.374 train_acc:  80.362 test_loss:  0.459 test_acc:  76.418\n",
      "耗时：: 0.024931669235229492\n",
      "epoch:  283 train_loss:  0.375 train_acc:  80.157 test_loss:  0.451 test_acc:  76.791\n",
      "耗时：: 0.030957698822021484\n",
      "epoch:  284 train_loss:  0.372 train_acc:  80.213 test_loss:  0.452 test_acc:  76.343\n",
      "耗时：: 0.01994633674621582\n",
      "epoch:  285 train_loss:  0.369 train_acc:  80.661 test_loss:  0.449 test_acc:  76.418\n",
      "耗时：: 0.018949031829833984\n",
      "epoch:  286 train_loss:  0.371 train_acc:  80.922 test_loss:  0.456 test_acc:  76.791\n",
      "耗时：: 0.01898646354675293\n",
      "epoch:  287 train_loss:  0.369 train_acc:  80.213 test_loss:  0.455 test_acc:  75.97\n",
      "耗时：: 0.026928186416625977\n",
      "epoch:  288 train_loss:  0.367 train_acc:  80.717 test_loss:  0.455 test_acc:  76.716\n",
      "耗时：: 0.01795816421508789\n",
      "epoch:  289 train_loss:  0.367 train_acc:  81.239 test_loss:  0.449 test_acc:  76.866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "耗时：: 0.018913984298706055\n",
      "epoch:  290 train_loss:  0.364 train_acc:  80.773 test_loss:  0.447 test_acc:  76.045\n",
      "耗时：: 0.02393817901611328\n",
      "epoch:  291 train_loss:  0.373 train_acc:  80.829 test_loss:  0.448 test_acc:  76.642\n",
      "耗时：: 0.017950773239135742\n",
      "epoch:  292 train_loss:  0.38 train_acc:  80.455 test_loss:  0.453 test_acc:  76.119\n",
      "耗时：: 0.025974273681640625\n",
      "epoch:  293 train_loss:  0.373 train_acc:  80.437 test_loss:  0.462 test_acc:  76.418\n",
      "耗时：: 0.024932146072387695\n",
      "epoch:  294 train_loss:  0.368 train_acc:  80.959 test_loss:  0.448 test_acc:  76.642\n",
      "耗时：: 0.018909215927124023\n",
      "epoch:  295 train_loss:  0.37 train_acc:  80.586 test_loss:  0.454 test_acc:  76.269\n",
      "耗时：: 0.018949508666992188\n",
      "epoch:  296 train_loss:  0.371 train_acc:  80.717 test_loss:  0.462 test_acc:  76.194\n",
      "耗时：: 0.020935535430908203\n",
      "epoch:  297 train_loss:  0.372 train_acc:  81.071 test_loss:  0.461 test_acc:  75.896\n",
      "耗时：: 0.017991065979003906\n",
      "epoch:  298 train_loss:  0.366 train_acc:  80.81 test_loss:  0.456 test_acc:  76.567\n",
      "耗时：: 0.026887893676757812\n",
      "epoch:  299 train_loss:  0.37 train_acc:  80.81 test_loss:  0.451 test_acc:  76.194\n",
      "耗时：: 0.01891326904296875\n",
      "epoch:  300 train_loss:  0.367 train_acc:  80.885 test_loss:  0.445 test_acc:  76.567\n",
      "耗时：: 0.02988266944885254\n",
      "epoch:  301 train_loss:  0.372 train_acc:  80.437 test_loss:  0.456 test_acc:  75.597\n",
      "耗时：: 0.01856207847595215\n",
      "epoch:  302 train_loss:  0.368 train_acc:  80.959 test_loss:  0.452 test_acc:  76.269\n",
      "耗时：: 0.03490614891052246\n",
      "epoch:  303 train_loss:  0.37 train_acc:  81.071 test_loss:  0.452 test_acc:  76.493\n",
      "耗时：: 0.01894974708557129\n",
      "epoch:  304 train_loss:  0.383 train_acc:  80.418 test_loss:  0.462 test_acc:  76.791\n",
      "耗时：: 0.028922319412231445\n",
      "epoch:  305 train_loss:  0.369 train_acc:  80.941 test_loss:  0.45 test_acc:  77.015\n",
      "耗时：: 0.020905733108520508\n",
      "epoch:  306 train_loss:  0.372 train_acc:  80.493 test_loss:  0.447 test_acc:  76.343\n",
      "耗时：: 0.01795220375061035\n",
      "epoch:  307 train_loss:  0.366 train_acc:  80.642 test_loss:  0.449 test_acc:  76.716\n",
      "耗时：: 0.02393364906311035\n",
      "epoch:  308 train_loss:  0.366 train_acc:  80.698 test_loss:  0.456 test_acc:  77.015\n",
      "耗时：: 0.018949031829833984\n",
      "epoch:  309 train_loss:  0.365 train_acc:  81.034 test_loss:  0.445 test_acc:  77.388\n",
      "max acc epoch：309        max acc：77.388\n",
      "耗时：: 0.018949508666992188\n",
      "epoch:  310 train_loss:  0.37 train_acc:  80.773 test_loss:  0.459 test_acc:  76.866\n",
      "耗时：: 0.019507408142089844\n",
      "epoch:  311 train_loss:  0.366 train_acc:  80.866 test_loss:  0.448 test_acc:  76.493\n",
      "耗时：: 0.027963638305664062\n",
      "epoch:  312 train_loss:  0.372 train_acc:  80.885 test_loss:  0.454 test_acc:  76.94\n",
      "耗时：: 0.018907546997070312\n",
      "epoch:  313 train_loss:  0.37 train_acc:  80.455 test_loss:  0.451 test_acc:  76.119\n",
      "耗时：: 0.018949270248413086\n",
      "epoch:  314 train_loss:  0.363 train_acc:  80.997 test_loss:  0.452 test_acc:  75.896\n",
      "耗时：: 0.02497267723083496\n",
      "epoch:  315 train_loss:  0.369 train_acc:  80.847 test_loss:  0.442 test_acc:  76.343\n",
      "耗时：: 0.02393317222595215\n",
      "epoch:  316 train_loss:  0.364 train_acc:  81.333 test_loss:  0.454 test_acc:  76.716\n",
      "耗时：: 0.02393484115600586\n",
      "epoch:  317 train_loss:  0.377 train_acc:  80.343 test_loss:  0.454 test_acc:  75.522\n",
      "耗时：: 0.030950307846069336\n",
      "epoch:  318 train_loss:  0.362 train_acc:  81.221 test_loss:  0.449 test_acc:  76.493\n",
      "耗时：: 0.02692556381225586\n",
      "epoch:  319 train_loss:  0.367 train_acc:  81.127 test_loss:  0.448 test_acc:  76.642\n",
      "耗时：: 0.02760171890258789\n",
      "epoch:  320 train_loss:  0.372 train_acc:  80.493 test_loss:  0.455 test_acc:  76.567\n",
      "耗时：: 0.01795482635498047\n",
      "epoch:  321 train_loss:  0.371 train_acc:  80.829 test_loss:  0.439 test_acc:  75.97\n",
      "耗时：: 0.030915260314941406\n",
      "epoch:  322 train_loss:  0.373 train_acc:  80.642 test_loss:  0.447 test_acc:  76.343\n",
      "耗时：: 0.024932384490966797\n",
      "epoch:  323 train_loss:  0.378 train_acc:  80.81 test_loss:  0.452 test_acc:  76.418\n",
      "耗时：: 0.018993139266967773\n",
      "epoch:  324 train_loss:  0.362 train_acc:  81.426 test_loss:  0.457 test_acc:  77.015\n",
      "耗时：: 0.023897886276245117\n",
      "epoch:  325 train_loss:  0.365 train_acc:  81.165 test_loss:  0.446 test_acc:  77.239\n",
      "耗时：: 0.02197718620300293\n",
      "epoch:  326 train_loss:  0.371 train_acc:  80.661 test_loss:  0.465 test_acc:  76.791\n",
      "耗时：: 0.027926206588745117\n",
      "epoch:  327 train_loss:  0.369 train_acc:  80.885 test_loss:  0.456 test_acc:  76.119\n",
      "耗时：: 0.020943164825439453\n",
      "epoch:  328 train_loss:  0.365 train_acc:  80.773 test_loss:  0.443 test_acc:  77.09\n",
      "耗时：: 0.018948793411254883\n",
      "epoch:  329 train_loss:  0.364 train_acc:  80.847 test_loss:  0.445 test_acc:  77.313\n",
      "耗时：: 0.01894664764404297\n",
      "epoch:  330 train_loss:  0.359 train_acc:  81.426 test_loss:  0.447 test_acc:  77.463\n",
      "max acc epoch：330        max acc：77.463\n",
      "耗时：: 0.020941972732543945\n",
      "epoch:  331 train_loss:  0.368 train_acc:  81.034 test_loss:  0.458 test_acc:  75.896\n",
      "耗时：: 0.019946813583374023\n",
      "epoch:  332 train_loss:  0.364 train_acc:  80.81 test_loss:  0.447 test_acc:  76.716\n",
      "耗时：: 0.02991962432861328\n",
      "epoch:  333 train_loss:  0.361 train_acc:  81.146 test_loss:  0.449 test_acc:  77.015\n",
      "耗时：: 0.02393651008605957\n",
      "epoch:  334 train_loss:  0.375 train_acc:  80.437 test_loss:  0.461 test_acc:  76.866\n",
      "耗时：: 0.01795172691345215\n",
      "epoch:  335 train_loss:  0.364 train_acc:  81.09 test_loss:  0.447 test_acc:  76.642\n",
      "耗时：: 0.029920339584350586\n",
      "epoch:  336 train_loss:  0.371 train_acc:  80.661 test_loss:  0.453 test_acc:  76.493\n",
      "耗时：: 0.02094244956970215\n",
      "epoch:  337 train_loss:  0.365 train_acc:  81.184 test_loss:  0.447 test_acc:  76.94\n",
      "耗时：: 0.03195834159851074\n",
      "epoch:  338 train_loss:  0.372 train_acc:  80.847 test_loss:  0.465 test_acc:  76.716\n",
      "耗时：: 0.017952680587768555\n",
      "epoch:  339 train_loss:  0.361 train_acc:  80.997 test_loss:  0.447 test_acc:  76.567\n",
      "耗时：: 0.021942615509033203\n",
      "epoch:  340 train_loss:  0.364 train_acc:  81.127 test_loss:  0.451 test_acc:  76.642\n",
      "耗时：: 0.03195524215698242\n",
      "epoch:  341 train_loss:  0.363 train_acc:  81.034 test_loss:  0.451 test_acc:  76.343\n",
      "耗时：: 0.02297186851501465\n",
      "epoch:  342 train_loss:  0.371 train_acc:  80.754 test_loss:  0.451 test_acc:  77.015\n",
      "耗时：: 0.0279233455657959\n",
      "epoch:  343 train_loss:  0.359 train_acc:  81.352 test_loss:  0.445 test_acc:  77.463\n",
      "耗时：: 0.0249326229095459\n",
      "epoch:  344 train_loss:  0.37 train_acc:  80.549 test_loss:  0.463 test_acc:  76.418\n",
      "耗时：: 0.022978544235229492\n",
      "epoch:  345 train_loss:  0.365 train_acc:  80.959 test_loss:  0.442 test_acc:  76.045\n",
      "耗时：: 0.026616811752319336\n",
      "epoch:  346 train_loss:  0.368 train_acc:  80.903 test_loss:  0.447 test_acc:  76.493\n",
      "耗时：: 0.027919292449951172\n",
      "epoch:  347 train_loss:  0.364 train_acc:  81.109 test_loss:  0.453 test_acc:  76.94\n",
      "耗时：: 0.02991199493408203\n",
      "epoch:  348 train_loss:  0.36 train_acc:  81.426 test_loss:  0.449 test_acc:  77.09\n",
      "耗时：: 0.024931907653808594\n",
      "epoch:  349 train_loss:  0.366 train_acc:  80.81 test_loss:  0.445 test_acc:  76.791\n",
      "耗时：: 0.020945072174072266\n",
      "epoch:  350 train_loss:  0.367 train_acc:  81.109 test_loss:  0.452 test_acc:  76.567\n",
      "耗时：: 0.01898670196533203\n",
      "epoch:  351 train_loss:  0.361 train_acc:  81.202 test_loss:  0.447 test_acc:  76.269\n",
      "耗时：: 0.0249326229095459\n",
      "epoch:  352 train_loss:  0.367 train_acc:  81.239 test_loss:  0.466 test_acc:  76.716\n",
      "耗时：: 0.03390860557556152\n",
      "epoch:  353 train_loss:  0.363 train_acc:  81.464 test_loss:  0.447 test_acc:  77.015\n",
      "耗时：: 0.020946025848388672\n",
      "epoch:  354 train_loss:  0.367 train_acc:  81.127 test_loss:  0.457 test_acc:  75.97\n",
      "耗时：: 0.018949508666992188\n",
      "epoch:  355 train_loss:  0.358 train_acc:  81.277 test_loss:  0.445 test_acc:  78.134\n",
      "max acc epoch：355        max acc：78.134\n",
      "耗时：: 0.026885032653808594\n",
      "epoch:  356 train_loss:  0.374 train_acc:  80.997 test_loss:  0.449 test_acc:  76.119\n",
      "耗时：: 0.023936033248901367\n",
      "epoch:  357 train_loss:  0.364 train_acc:  81.277 test_loss:  0.454 test_acc:  77.09\n",
      "耗时：: 0.025892019271850586\n",
      "epoch:  358 train_loss:  0.366 train_acc:  81.053 test_loss:  0.447 test_acc:  76.418\n",
      "耗时：: 0.01894235610961914\n",
      "epoch:  359 train_loss:  0.362 train_acc:  81.557 test_loss:  0.449 test_acc:  76.269\n",
      "耗时：: 0.025929689407348633\n",
      "epoch:  360 train_loss:  0.37 train_acc:  81.034 test_loss:  0.451 test_acc:  76.119\n",
      "耗时：: 0.01894974708557129\n",
      "epoch:  361 train_loss:  0.362 train_acc:  81.071 test_loss:  0.459 test_acc:  76.716\n",
      "耗时：: 0.01994633674621582\n",
      "epoch:  362 train_loss:  0.362 train_acc:  81.034 test_loss:  0.449 test_acc:  76.866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "耗时：: 0.0275418758392334\n",
      "epoch:  363 train_loss:  0.361 train_acc:  81.426 test_loss:  0.452 test_acc:  76.866\n",
      "耗时：: 0.024890661239624023\n",
      "epoch:  364 train_loss:  0.364 train_acc:  81.165 test_loss:  0.449 test_acc:  76.567\n",
      "耗时：: 0.023900508880615234\n",
      "epoch:  365 train_loss:  0.36 train_acc:  81.015 test_loss:  0.453 test_acc:  76.567\n",
      "耗时：: 0.018908023834228516\n",
      "epoch:  366 train_loss:  0.367 train_acc:  80.903 test_loss:  0.463 test_acc:  76.716\n",
      "耗时：: 0.02493000030517578\n",
      "epoch:  367 train_loss:  0.364 train_acc:  80.791 test_loss:  0.451 test_acc:  76.642\n",
      "耗时：: 0.02493429183959961\n",
      "epoch:  368 train_loss:  0.364 train_acc:  81.053 test_loss:  0.461 test_acc:  77.537\n",
      "耗时：: 0.019946575164794922\n",
      "epoch:  369 train_loss:  0.366 train_acc:  81.277 test_loss:  0.448 test_acc:  76.791\n",
      "耗时：: 0.0209043025970459\n",
      "epoch:  370 train_loss:  0.372 train_acc:  80.362 test_loss:  0.458 test_acc:  76.343\n",
      "耗时：: 0.020943880081176758\n",
      "epoch:  371 train_loss:  0.36 train_acc:  81.464 test_loss:  0.459 test_acc:  76.567\n",
      "耗时：: 0.025523900985717773\n",
      "epoch:  372 train_loss:  0.364 train_acc:  81.426 test_loss:  0.458 test_acc:  76.269\n",
      "耗时：: 0.018985271453857422\n",
      "epoch:  373 train_loss:  0.359 train_acc:  81.127 test_loss:  0.45 test_acc:  76.418\n",
      "耗时：: 0.018950700759887695\n",
      "epoch:  374 train_loss:  0.367 train_acc:  80.81 test_loss:  0.455 test_acc:  76.493\n",
      "耗时：: 0.018985748291015625\n",
      "epoch:  375 train_loss:  0.363 train_acc:  80.903 test_loss:  0.457 test_acc:  76.045\n",
      "耗时：: 0.028871774673461914\n",
      "epoch:  376 train_loss:  0.363 train_acc:  81.109 test_loss:  0.455 test_acc:  76.866\n",
      "耗时：: 0.018950462341308594\n",
      "epoch:  377 train_loss:  0.373 train_acc:  80.922 test_loss:  0.454 test_acc:  76.194\n",
      "耗时：: 0.018947601318359375\n",
      "epoch:  378 train_loss:  0.356 train_acc:  81.818 test_loss:  0.451 test_acc:  77.09\n",
      "耗时：: 0.02098393440246582\n",
      "epoch:  379 train_loss:  0.363 train_acc:  80.642 test_loss:  0.46 test_acc:  77.239\n",
      "耗时：: 0.024892330169677734\n",
      "epoch:  380 train_loss:  0.358 train_acc:  81.594 test_loss:  0.462 test_acc:  77.015\n",
      "耗时：: 0.023975133895874023\n",
      "epoch:  381 train_loss:  0.367 train_acc:  80.698 test_loss:  0.466 test_acc:  76.567\n",
      "耗时：: 0.018950223922729492\n",
      "epoch:  382 train_loss:  0.361 train_acc:  81.071 test_loss:  0.445 test_acc:  77.015\n",
      "耗时：: 0.024933338165283203\n",
      "epoch:  383 train_loss:  0.358 train_acc:  81.426 test_loss:  0.445 test_acc:  77.313\n",
      "耗时：: 0.031879425048828125\n",
      "epoch:  384 train_loss:  0.364 train_acc:  81.202 test_loss:  0.471 test_acc:  76.418\n",
      "耗时：: 0.021981239318847656\n",
      "epoch:  385 train_loss:  0.368 train_acc:  80.959 test_loss:  0.463 test_acc:  76.791\n",
      "耗时：: 0.018948078155517578\n",
      "epoch:  386 train_loss:  0.361 train_acc:  81.501 test_loss:  0.456 test_acc:  76.866\n",
      "耗时：: 0.028879404067993164\n",
      "epoch:  387 train_loss:  0.371 train_acc:  80.829 test_loss:  0.473 test_acc:  76.567\n",
      "耗时：: 0.01998138427734375\n",
      "epoch:  388 train_loss:  0.36 train_acc:  81.165 test_loss:  0.465 test_acc:  76.269\n",
      "耗时：: 0.018949270248413086\n",
      "epoch:  389 train_loss:  0.354 train_acc:  82.117 test_loss:  0.45 test_acc:  76.791\n",
      "耗时：: 0.017951011657714844\n",
      "epoch:  390 train_loss:  0.369 train_acc:  81.333 test_loss:  0.454 test_acc:  76.119\n",
      "耗时：: 0.022939682006835938\n",
      "epoch:  391 train_loss:  0.367 train_acc:  80.885 test_loss:  0.462 test_acc:  76.269\n",
      "耗时：: 0.0329127311706543\n",
      "epoch:  392 train_loss:  0.363 train_acc:  80.455 test_loss:  0.449 test_acc:  77.09\n",
      "耗时：: 0.025929927825927734\n",
      "epoch:  393 train_loss:  0.357 train_acc:  81.538 test_loss:  0.448 test_acc:  76.791\n",
      "耗时：: 0.02792525291442871\n",
      "epoch:  394 train_loss:  0.36 train_acc:  81.389 test_loss:  0.455 test_acc:  76.269\n",
      "耗时：: 0.01998758316040039\n",
      "epoch:  395 train_loss:  0.358 train_acc:  81.669 test_loss:  0.449 test_acc:  77.463\n",
      "耗时：: 0.018950462341308594\n",
      "epoch:  396 train_loss:  0.359 train_acc:  81.464 test_loss:  0.456 test_acc:  76.045\n",
      "耗时：: 0.02090597152709961\n",
      "epoch:  397 train_loss:  0.368 train_acc:  80.642 test_loss:  0.476 test_acc:  77.612\n",
      "耗时：: 0.023935794830322266\n",
      "epoch:  398 train_loss:  0.369 train_acc:  81.333 test_loss:  0.45 test_acc:  76.866\n",
      "耗时：: 0.019946575164794922\n",
      "epoch:  399 train_loss:  0.359 train_acc:  81.258 test_loss:  0.456 test_acc:  76.716\n",
      "耗时：: 0.03091740608215332\n",
      "epoch:  400 train_loss:  0.356 train_acc:  81.65 test_loss:  0.458 test_acc:  77.09\n",
      "耗时：: 0.026489973068237305\n",
      "epoch:  401 train_loss:  0.358 train_acc:  81.352 test_loss:  0.454 test_acc:  76.567\n",
      "耗时：: 0.017952442169189453\n",
      "epoch:  402 train_loss:  0.363 train_acc:  80.978 test_loss:  0.447 test_acc:  77.239\n",
      "耗时：: 0.030919313430786133\n",
      "epoch:  403 train_loss:  0.353 train_acc:  81.632 test_loss:  0.462 test_acc:  76.493\n",
      "耗时：: 0.02193927764892578\n",
      "epoch:  404 train_loss:  0.368 train_acc:  80.978 test_loss:  0.454 test_acc:  76.194\n",
      "耗时：: 0.023937463760375977\n",
      "epoch:  405 train_loss:  0.366 train_acc:  81.146 test_loss:  0.451 test_acc:  76.269\n",
      "耗时：: 0.017984628677368164\n",
      "epoch:  406 train_loss:  0.363 train_acc:  81.445 test_loss:  0.453 test_acc:  77.164\n",
      "耗时：: 0.024933576583862305\n",
      "epoch:  407 train_loss:  0.364 train_acc:  81.239 test_loss:  0.456 test_acc:  76.493\n",
      "耗时：: 0.020980119705200195\n",
      "epoch:  408 train_loss:  0.362 train_acc:  81.445 test_loss:  0.446 test_acc:  76.194\n",
      "耗时：: 0.01898813247680664\n",
      "epoch:  409 train_loss:  0.361 train_acc:  81.632 test_loss:  0.454 test_acc:  76.791\n",
      "耗时：: 0.017952680587768555\n",
      "epoch:  410 train_loss:  0.374 train_acc:  81.09 test_loss:  0.493 test_acc:  76.716\n",
      "耗时：: 0.02696681022644043\n",
      "epoch:  411 train_loss:  0.361 train_acc:  81.202 test_loss:  0.465 test_acc:  76.866\n",
      "耗时：: 0.022939205169677734\n",
      "epoch:  412 train_loss:  0.371 train_acc:  80.978 test_loss:  0.482 test_acc:  77.015\n",
      "耗时：: 0.02493453025817871\n",
      "epoch:  413 train_loss:  0.382 train_acc:  80.399 test_loss:  0.455 test_acc:  75.522\n",
      "耗时：: 0.019940614700317383\n",
      "epoch:  414 train_loss:  0.355 train_acc:  81.968 test_loss:  0.456 test_acc:  77.313\n",
      "耗时：: 0.01990199089050293\n",
      "epoch:  415 train_loss:  0.357 train_acc:  81.482 test_loss:  0.463 test_acc:  77.463\n",
      "耗时：: 0.017915010452270508\n",
      "epoch:  416 train_loss:  0.353 train_acc:  81.669 test_loss:  0.448 test_acc:  76.716\n",
      "耗时：: 0.02293848991394043\n",
      "epoch:  417 train_loss:  0.361 train_acc:  80.997 test_loss:  0.474 test_acc:  76.716\n",
      "耗时：: 0.027967214584350586\n",
      "epoch:  418 train_loss:  0.363 train_acc:  81.389 test_loss:  0.457 test_acc:  76.194\n",
      "耗时：: 0.03509855270385742\n",
      "epoch:  419 train_loss:  0.36 train_acc:  81.202 test_loss:  0.454 test_acc:  77.09\n",
      "耗时：: 0.03391122817993164\n",
      "epoch:  420 train_loss:  0.359 train_acc:  80.866 test_loss:  0.46 test_acc:  77.612\n",
      "耗时：: 0.03690075874328613\n",
      "epoch:  421 train_loss:  0.355 train_acc:  81.781 test_loss:  0.454 test_acc:  77.164\n",
      "耗时：: 0.04587841033935547\n",
      "epoch:  422 train_loss:  0.364 train_acc:  81.202 test_loss:  0.466 test_acc:  76.493\n",
      "耗时：: 0.03291177749633789\n",
      "epoch:  423 train_loss:  0.354 train_acc:  81.762 test_loss:  0.451 test_acc:  76.567\n",
      "耗时：: 0.03291177749633789\n",
      "epoch:  424 train_loss:  0.355 train_acc:  81.744 test_loss:  0.456 test_acc:  77.313\n",
      "耗时：: 0.02792501449584961\n",
      "epoch:  425 train_loss:  0.351 train_acc:  81.65 test_loss:  0.458 test_acc:  77.313\n",
      "耗时：: 0.02393651008605957\n",
      "epoch:  426 train_loss:  0.357 train_acc:  81.557 test_loss:  0.461 test_acc:  76.493\n",
      "耗时：: 0.018949508666992188\n",
      "epoch:  427 train_loss:  0.366 train_acc:  80.754 test_loss:  0.472 test_acc:  76.119\n",
      "耗时：: 0.02597188949584961\n",
      "epoch:  428 train_loss:  0.356 train_acc:  81.352 test_loss:  0.456 test_acc:  76.866\n",
      "耗时：: 0.02293705940246582\n",
      "epoch:  429 train_loss:  0.36 train_acc:  81.426 test_loss:  0.441 test_acc:  77.015\n",
      "耗时：: 0.02592921257019043\n",
      "epoch:  430 train_loss:  0.367 train_acc:  81.071 test_loss:  0.469 test_acc:  76.343\n",
      "耗时：: 0.025931835174560547\n",
      "epoch:  431 train_loss:  0.364 train_acc:  81.333 test_loss:  0.464 test_acc:  75.746\n",
      "耗时：: 0.02548527717590332\n",
      "epoch:  432 train_loss:  0.353 train_acc:  81.912 test_loss:  0.451 test_acc:  76.194\n",
      "耗时：: 0.024933338165283203\n",
      "epoch:  433 train_loss:  0.37 train_acc:  80.847 test_loss:  0.461 test_acc:  76.194\n",
      "耗时：: 0.018951416015625\n",
      "epoch:  434 train_loss:  0.353 train_acc:  82.154 test_loss:  0.45 test_acc:  77.239\n",
      "耗时：: 0.021983623504638672\n",
      "epoch:  435 train_loss:  0.357 train_acc:  81.557 test_loss:  0.453 test_acc:  77.015\n",
      "耗时：: 0.02294015884399414\n",
      "epoch:  436 train_loss:  0.354 train_acc:  82.08 test_loss:  0.463 test_acc:  76.716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "耗时：: 0.0258939266204834\n",
      "epoch:  437 train_loss:  0.361 train_acc:  81.034 test_loss:  0.453 test_acc:  76.567\n",
      "耗时：: 0.03287220001220703\n",
      "epoch:  438 train_loss:  0.351 train_acc:  81.594 test_loss:  0.461 test_acc:  76.716\n",
      "耗时：: 0.02995777130126953\n",
      "epoch:  439 train_loss:  0.351 train_acc:  81.8 test_loss:  0.452 test_acc:  76.94\n",
      "耗时：: 0.021939754486083984\n",
      "epoch:  440 train_loss:  0.381 train_acc:  80.455 test_loss:  0.489 test_acc:  75.97\n",
      "耗时：: 0.02788853645324707\n",
      "epoch:  441 train_loss:  0.351 train_acc:  82.192 test_loss:  0.451 test_acc:  77.015\n",
      "耗时：: 0.0359036922454834\n",
      "epoch:  442 train_loss:  0.358 train_acc:  81.688 test_loss:  0.454 test_acc:  76.493\n",
      "耗时：: 0.021939754486083984\n",
      "epoch:  443 train_loss:  0.348 train_acc:  81.856 test_loss:  0.455 test_acc:  76.642\n",
      "耗时：: 0.02297377586364746\n",
      "epoch:  444 train_loss:  0.365 train_acc:  80.791 test_loss:  0.473 test_acc:  76.94\n",
      "耗时：: 0.027924299240112305\n",
      "epoch:  445 train_loss:  0.365 train_acc:  81.109 test_loss:  0.479 test_acc:  76.418\n",
      "耗时：: 0.018950223922729492\n",
      "epoch:  446 train_loss:  0.354 train_acc:  81.837 test_loss:  0.462 test_acc:  76.716\n",
      "耗时：: 0.023070335388183594\n",
      "epoch:  447 train_loss:  0.356 train_acc:  81.594 test_loss:  0.461 test_acc:  76.791\n",
      "耗时：: 0.029921293258666992\n",
      "epoch:  448 train_loss:  0.366 train_acc:  81.146 test_loss:  0.456 test_acc:  76.194\n",
      "耗时：: 0.032911062240600586\n",
      "epoch:  449 train_loss:  0.357 train_acc:  81.314 test_loss:  0.452 test_acc:  77.015\n",
      "耗时：: 0.030498504638671875\n",
      "epoch:  450 train_loss:  0.354 train_acc:  81.221 test_loss:  0.453 test_acc:  77.463\n",
      "耗时：: 0.01990795135498047\n",
      "epoch:  451 train_loss:  0.364 train_acc:  81.165 test_loss:  0.45 test_acc:  76.343\n",
      "耗时：: 0.028713703155517578\n",
      "epoch:  452 train_loss:  0.357 train_acc:  81.239 test_loss:  0.465 test_acc:  76.866\n",
      "耗时：: 0.017949342727661133\n",
      "epoch:  453 train_loss:  0.355 train_acc:  81.37 test_loss:  0.461 test_acc:  77.09\n",
      "耗时：: 0.018949508666992188\n",
      "epoch:  454 train_loss:  0.352 train_acc:  81.576 test_loss:  0.461 test_acc:  76.94\n",
      "耗时：: 0.022977352142333984\n",
      "epoch:  455 train_loss:  0.353 train_acc:  81.688 test_loss:  0.461 test_acc:  77.537\n",
      "耗时：: 0.022974014282226562\n",
      "epoch:  456 train_loss:  0.359 train_acc:  81.408 test_loss:  0.467 test_acc:  77.612\n",
      "耗时：: 0.021942615509033203\n",
      "epoch:  457 train_loss:  0.347 train_acc:  82.173 test_loss:  0.449 test_acc:  77.537\n",
      "耗时：: 0.01795029640197754\n",
      "epoch:  458 train_loss:  0.35 train_acc:  81.706 test_loss:  0.462 test_acc:  77.015\n",
      "耗时：: 0.03091597557067871\n",
      "epoch:  459 train_loss:  0.351 train_acc:  81.501 test_loss:  0.472 test_acc:  77.09\n",
      "耗时：: 0.030917882919311523\n",
      "epoch:  460 train_loss:  0.363 train_acc:  81.576 test_loss:  0.46 test_acc:  77.463\n",
      "耗时：: 0.019908666610717773\n",
      "epoch:  461 train_loss:  0.352 train_acc:  81.968 test_loss:  0.456 test_acc:  77.239\n",
      "耗时：: 0.02294158935546875\n",
      "epoch:  462 train_loss:  0.351 train_acc:  81.968 test_loss:  0.454 test_acc:  77.164\n",
      "耗时：: 0.024931907653808594\n",
      "epoch:  463 train_loss:  0.346 train_acc:  82.042 test_loss:  0.455 test_acc:  76.716\n",
      "耗时：: 0.02589583396911621\n",
      "epoch:  464 train_loss:  0.363 train_acc:  81.501 test_loss:  0.465 test_acc:  76.343\n",
      "耗时：: 0.024969100952148438\n",
      "epoch:  465 train_loss:  0.358 train_acc:  81.464 test_loss:  0.473 test_acc:  77.313\n",
      "耗时：: 0.017915725708007812\n",
      "epoch:  466 train_loss:  0.354 train_acc:  81.837 test_loss:  0.455 test_acc:  77.313\n",
      "耗时：: 0.017915964126586914\n",
      "epoch:  467 train_loss:  0.346 train_acc:  81.837 test_loss:  0.458 test_acc:  77.313\n",
      "耗时：: 0.018952369689941406\n",
      "epoch:  468 train_loss:  0.357 train_acc:  81.37 test_loss:  0.455 test_acc:  76.418\n",
      "耗时：: 0.028920650482177734\n",
      "epoch:  469 train_loss:  0.35 train_acc:  81.762 test_loss:  0.455 test_acc:  77.164\n",
      "耗时：: 0.03095221519470215\n",
      "epoch:  470 train_loss:  0.366 train_acc:  80.623 test_loss:  0.467 test_acc:  76.119\n",
      "耗时：: 0.02389359474182129\n",
      "epoch:  471 train_loss:  0.353 train_acc:  81.837 test_loss:  0.457 test_acc:  77.164\n",
      "耗时：: 0.03191328048706055\n",
      "epoch:  472 train_loss:  0.35 train_acc:  81.986 test_loss:  0.464 test_acc:  76.94\n",
      "耗时：: 0.02198028564453125\n",
      "epoch:  473 train_loss:  0.353 train_acc:  81.744 test_loss:  0.468 test_acc:  76.418\n",
      "耗时：: 0.02992081642150879\n",
      "epoch:  474 train_loss:  0.353 train_acc:  80.903 test_loss:  0.449 test_acc:  76.567\n",
      "耗时：: 0.023937463760375977\n",
      "epoch:  475 train_loss:  0.348 train_acc:  82.024 test_loss:  0.452 test_acc:  77.239\n",
      "耗时：: 0.019906282424926758\n",
      "epoch:  476 train_loss:  0.36 train_acc:  81.258 test_loss:  0.469 test_acc:  76.418\n",
      "耗时：: 0.019456863403320312\n",
      "epoch:  477 train_loss:  0.361 train_acc:  81.37 test_loss:  0.465 test_acc:  77.015\n",
      "耗时：: 0.01990365982055664\n",
      "epoch:  478 train_loss:  0.35 train_acc:  81.856 test_loss:  0.453 test_acc:  76.567\n",
      "耗时：: 0.01994609832763672\n",
      "epoch:  479 train_loss:  0.348 train_acc:  81.93 test_loss:  0.459 test_acc:  77.463\n",
      "耗时：: 0.025930404663085938\n",
      "epoch:  480 train_loss:  0.357 train_acc:  81.501 test_loss:  0.468 test_acc:  76.716\n",
      "耗时：: 0.029882192611694336\n",
      "epoch:  481 train_loss:  0.348 train_acc:  81.8 test_loss:  0.461 test_acc:  77.313\n",
      "耗时：: 0.018949031829833984\n",
      "epoch:  482 train_loss:  0.348 train_acc:  81.949 test_loss:  0.471 test_acc:  77.239\n",
      "耗时：: 0.031949758529663086\n",
      "epoch:  483 train_loss:  0.355 train_acc:  81.669 test_loss:  0.451 test_acc:  77.09\n",
      "耗时：: 0.019946575164794922\n",
      "epoch:  484 train_loss:  0.362 train_acc:  81.221 test_loss:  0.466 test_acc:  77.164\n",
      "耗时：: 0.021942615509033203\n",
      "epoch:  485 train_loss:  0.349 train_acc:  81.856 test_loss:  0.452 test_acc:  77.388\n",
      "耗时：: 0.01894855499267578\n",
      "epoch:  486 train_loss:  0.35 train_acc:  81.538 test_loss:  0.448 test_acc:  77.313\n",
      "耗时：: 0.022936105728149414\n",
      "epoch:  487 train_loss:  0.355 train_acc:  81.445 test_loss:  0.468 test_acc:  76.269\n",
      "耗时：: 0.02588963508605957\n",
      "epoch:  488 train_loss:  0.366 train_acc:  81.071 test_loss:  0.466 test_acc:  75.896\n",
      "耗时：: 0.01998758316040039\n",
      "epoch:  489 train_loss:  0.356 train_acc:  81.538 test_loss:  0.46 test_acc:  76.642\n",
      "耗时：: 0.0329127311706543\n",
      "epoch:  490 train_loss:  0.35 train_acc:  81.856 test_loss:  0.471 test_acc:  76.567\n",
      "耗时：: 0.018946409225463867\n",
      "epoch:  491 train_loss:  0.358 train_acc:  81.538 test_loss:  0.46 test_acc:  77.687\n",
      "耗时：: 0.018955469131469727\n",
      "epoch:  492 train_loss:  0.356 train_acc:  81.221 test_loss:  0.457 test_acc:  76.642\n",
      "耗时：: 0.01894831657409668\n",
      "epoch:  493 train_loss:  0.349 train_acc:  81.389 test_loss:  0.461 test_acc:  76.269\n",
      "耗时：: 0.01891350746154785\n",
      "epoch:  494 train_loss:  0.357 train_acc:  81.464 test_loss:  0.457 test_acc:  77.09\n",
      "耗时：: 0.018950462341308594\n",
      "epoch:  495 train_loss:  0.352 train_acc:  81.594 test_loss:  0.459 test_acc:  76.716\n",
      "耗时：: 0.019908905029296875\n",
      "epoch:  496 train_loss:  0.357 train_acc:  81.912 test_loss:  0.468 test_acc:  76.866\n",
      "耗时：: 0.018947839736938477\n",
      "epoch:  497 train_loss:  0.353 train_acc:  81.277 test_loss:  0.46 test_acc:  77.463\n",
      "耗时：: 0.026926517486572266\n",
      "epoch:  498 train_loss:  0.346 train_acc:  81.856 test_loss:  0.454 test_acc:  76.791\n",
      "耗时：: 0.02393198013305664\n",
      "epoch:  499 train_loss:  0.357 train_acc:  81.053 test_loss:  0.47 test_acc:  77.015\n",
      "End max acc epoch：499        max acc：78.134\n",
      "            Actual     Predicted\n",
      "0     [tensor(0.)]  [tensor(0.)]\n",
      "1     [tensor(1.)]  [tensor(0.)]\n",
      "2     [tensor(1.)]  [tensor(0.)]\n",
      "3     [tensor(0.)]  [tensor(0.)]\n",
      "4     [tensor(0.)]  [tensor(0.)]\n",
      "...            ...           ...\n",
      "1335  [tensor(0.)]  [tensor(0.)]\n",
      "1336  [tensor(1.)]  [tensor(1.)]\n",
      "1337  [tensor(0.)]  [tensor(0.)]\n",
      "1338  [tensor(1.)]  [tensor(1.)]\n",
      "1339  [tensor(0.)]  [tensor(0.)]\n",
      "\n",
      "[1340 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor t in range(10):\\n    out = net(train_x)     # 喂给 net 训练数据 x, 输出分析值\\n    #out = torch.max(F.softmax(out, dim=1), 1)[1].type(torch.FloatTensor)\\n    #print(out,train_y)#.squeeze()\\n    loss = loss_func(out, train_y)#.squeeze()     # 计算两者的误差\\n    optimizer.zero_grad()   \\n    loss.backward()         \\n    optimizer.step()        \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#net = Net(n_feature=points, n_hidden=10, n_output=1) # 几个类别就几个 output\n",
    "\n",
    "#便于随着训练的进行观察数值的变化\n",
    "train_loss=[]\n",
    "test_loss=[]\n",
    "min_loss=1\n",
    "max_acc=0.1\n",
    "\n",
    "def trans(pred):\n",
    "    P=[]\n",
    "    for i  in pred:\n",
    "        if i>=0.5:\n",
    "            i=[1]\n",
    "        else:\n",
    "            i=[0]\n",
    "        P.append(i)\n",
    "    pred = torch.Tensor(P)\n",
    "    return pred\n",
    "\n",
    "def accury(pred,y):\n",
    "    #print(pred)\n",
    "    pred = trans(pred)\n",
    "    #print(pred,y)\n",
    "    #print(sum(pred == y),sum(pred != y),len(y))\n",
    "    return sum(pred == y)/len(y)*100\n",
    "\n",
    "#获得这个模型\n",
    "model = Net(n_feature=points)\n",
    "#优化函数 优化的是模型所有变量即model.parameters()\n",
    "optim = torch.optim.Adam(model.parameters(),lr=0.001)  #lr=0.0001  #选择使用哪种优化器 761个数据集用这个优化器\n",
    "#optim = torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.3)   # lr=0.01,momentum=0.3\n",
    "\n",
    "# 训练网络\n",
    "# 算误差的时候, 注意真实值!不是! one-hot 形式的, 而是1D Tensor, (batch,)\n",
    "loss_fn =nn.BCELoss()#nn.CrossEntropyLoss()##nn.BCEWithLogitsLoss()# #\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for x,y in train_dl:\n",
    "        y_pred = model(x)\n",
    "        #print(y_pred,y)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        #print(loss)\n",
    "        # 梯度置为0\n",
    "        optim.zero_grad()\n",
    "        # 反向传播求解梯度\n",
    "        loss.backward()\n",
    "        # 优化\n",
    "        optim.step()\n",
    "        \n",
    "    # 不需要进行梯度计算\n",
    "    with torch.no_grad():\n",
    "        epoch_loss = loss_fn(model(train_x), train_y).data\n",
    "        epoch_acc = accury(model(train_x), train_y).numpy()\n",
    "        #print(epoch_acc)\n",
    "        epoch_test_loss = loss_fn(model(test_x), test_y).data\n",
    "        start = time.time()\n",
    "        epoch_test_acc = accury(model(test_x), test_y).numpy()\n",
    "        end = time.time()\n",
    "        print('耗时：:',str(end-start))\n",
    "        #print(epoch_acc,sum(epoch_acc),len(epoch_acc),sum(epoch_acc)/len(epoch_acc))\n",
    "        print('epoch: ',epoch,'train_loss: ',round(epoch_loss.item(),3),'train_acc: ', round(float(sum(epoch_acc)/len(epoch_acc)),3),\n",
    "              'test_loss: ',round(epoch_test_loss.item(),3),'test_acc: ',round(float(sum(epoch_test_acc)/len(epoch_test_acc)),3))\n",
    "        train_loss.append(epoch_loss)\n",
    "        test_loss.append(epoch_test_loss)\n",
    "        if epoch_test_loss < min_loss:\n",
    "            min_loss = epoch_test_loss\n",
    "            print(\"min loss epoch:\"+str(epoch))\n",
    "            torch.save(model, 'H:/vamf_model/model_patch_6.pth')\n",
    "            df = pd.DataFrame({'Actual':list(test_y), 'Predicted':list(trans(model(test_x)))})  \n",
    "            #print(df)\n",
    "        if round(float(sum(epoch_test_acc)/len(epoch_test_acc)),3) > max_acc:\n",
    "            max_acc = round(float(sum(epoch_test_acc)/len(epoch_test_acc)),3)\n",
    "            print(\"max acc epoch：\"+str(epoch)+\"        max acc：\"+str(max_acc))\n",
    "            \n",
    "print(\"End max acc epoch：\"+str(epoch)+\"        max acc：\"+str(max_acc))        \n",
    "#print(model(test_x), test_y)\n",
    "ANN=max_acc\n",
    "\n",
    "df = pd.DataFrame({'Actual':list(test_y), 'Predicted':list(trans(model(test_x)))})\n",
    "print(df)\n",
    "\n",
    "plt.plot(range(1,epochs+1),train_loss,label='train_loss')\n",
    "plt.plot(range(1,epochs+1),test_loss,label='test_loss')\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "for t in range(10):\n",
    "    out = net(train_x)     # 喂给 net 训练数据 x, 输出分析值\n",
    "    #out = torch.max(F.softmax(out, dim=1), 1)[1].type(torch.FloatTensor)\n",
    "    #print(out,train_y)#.squeeze()\n",
    "    loss = loss_func(out, train_y)#.squeeze()     # 计算两者的误差\n",
    "    optimizer.zero_grad()   \n",
    "    loss.backward()         \n",
    "    optimizer.step()        \n",
    "'''\n",
    "#print(target_y,pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "04f36964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[86.49253731343283,\n",
       " 74.02985074626865,\n",
       " 98.2089552238806,\n",
       " 96.7910447761194,\n",
       " 85.8955223880597,\n",
       " 83.50746268656717,\n",
       " 72.31343283582089,\n",
       " 94.179]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc=[KNN,Log,DT,RF,AB,SVM,NB,ANN]\n",
    "model = ['KNN','Logistic','决策树','RF','Adaboost','SVM','朴素贝叶斯','ANN']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "706b92ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brightness', 'contrast', 'edgeLength', 'Hue1', 'Hue2', 'Hue3', 'Hue4', 'Hue5', 'Hue6', 'Hue7', 'Saturation', 'Value']\n",
      "6697\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([1340, 12])\n",
      "epoch:  0 train_loss:  0.467 train_acc:  0.762 test_loss:  0.488 test_acc:  0.752\n",
      "min loss epoch:0\n",
      "max acc epoch：0        max acc：0.752\n",
      "epoch:  1 train_loss:  0.451 train_acc:  0.765 test_loss:  0.476 test_acc:  0.751\n",
      "min loss epoch:1\n",
      "epoch:  2 train_loss:  0.444 train_acc:  0.769 test_loss:  0.471 test_acc:  0.751\n",
      "min loss epoch:2\n",
      "epoch:  3 train_loss:  0.438 train_acc:  0.769 test_loss:  0.468 test_acc:  0.746\n",
      "min loss epoch:3\n",
      "epoch:  4 train_loss:  0.434 train_acc:  0.772 test_loss:  0.464 test_acc:  0.751\n",
      "min loss epoch:4\n",
      "epoch:  5 train_loss:  0.43 train_acc:  0.774 test_loss:  0.462 test_acc:  0.751\n",
      "min loss epoch:5\n",
      "epoch:  6 train_loss:  0.426 train_acc:  0.775 test_loss:  0.46 test_acc:  0.746\n",
      "min loss epoch:6\n",
      "epoch:  7 train_loss:  0.423 train_acc:  0.778 test_loss:  0.458 test_acc:  0.746\n",
      "min loss epoch:7\n",
      "epoch:  8 train_loss:  0.419 train_acc:  0.779 test_loss:  0.455 test_acc:  0.754\n",
      "min loss epoch:8\n",
      "max acc epoch：8        max acc：0.754\n",
      "epoch:  9 train_loss:  0.416 train_acc:  0.781 test_loss:  0.455 test_acc:  0.754\n",
      "min loss epoch:9\n",
      "epoch:  10 train_loss:  0.412 train_acc:  0.783 test_loss:  0.453 test_acc:  0.754\n",
      "min loss epoch:10\n",
      "epoch:  11 train_loss:  0.409 train_acc:  0.785 test_loss:  0.453 test_acc:  0.753\n",
      "min loss epoch:11\n",
      "epoch:  12 train_loss:  0.406 train_acc:  0.787 test_loss:  0.451 test_acc:  0.754\n",
      "min loss epoch:12\n",
      "epoch:  13 train_loss:  0.403 train_acc:  0.792 test_loss:  0.45 test_acc:  0.751\n",
      "min loss epoch:13\n",
      "epoch:  14 train_loss:  0.4 train_acc:  0.793 test_loss:  0.448 test_acc:  0.757\n",
      "min loss epoch:14\n",
      "max acc epoch：14        max acc：0.757\n",
      "epoch:  15 train_loss:  0.398 train_acc:  0.795 test_loss:  0.448 test_acc:  0.76\n",
      "min loss epoch:15\n",
      "max acc epoch：15        max acc：0.76\n",
      "epoch:  16 train_loss:  0.395 train_acc:  0.796 test_loss:  0.446 test_acc:  0.763\n",
      "min loss epoch:16\n",
      "max acc epoch：16        max acc：0.763\n",
      "epoch:  17 train_loss:  0.392 train_acc:  0.797 test_loss:  0.445 test_acc:  0.763\n",
      "min loss epoch:17\n",
      "epoch:  18 train_loss:  0.389 train_acc:  0.797 test_loss:  0.441 test_acc:  0.772\n",
      "min loss epoch:18\n",
      "max acc epoch：18        max acc：0.772\n",
      "epoch:  19 train_loss:  0.385 train_acc:  0.802 test_loss:  0.438 test_acc:  0.781\n",
      "min loss epoch:19\n",
      "max acc epoch：19        max acc：0.781\n",
      "epoch:  20 train_loss:  0.382 train_acc:  0.805 test_loss:  0.435 test_acc:  0.783\n",
      "min loss epoch:20\n",
      "max acc epoch：20        max acc：0.783\n",
      "epoch:  21 train_loss:  0.379 train_acc:  0.803 test_loss:  0.434 test_acc:  0.787\n",
      "min loss epoch:21\n",
      "max acc epoch：21        max acc：0.787\n",
      "epoch:  22 train_loss:  0.376 train_acc:  0.806 test_loss:  0.431 test_acc:  0.781\n",
      "min loss epoch:22\n",
      "epoch:  23 train_loss:  0.372 train_acc:  0.809 test_loss:  0.429 test_acc:  0.782\n",
      "min loss epoch:23\n",
      "epoch:  24 train_loss:  0.37 train_acc:  0.808 test_loss:  0.428 test_acc:  0.781\n",
      "min loss epoch:24\n",
      "epoch:  25 train_loss:  0.367 train_acc:  0.808 test_loss:  0.426 test_acc:  0.784\n",
      "min loss epoch:25\n",
      "epoch:  26 train_loss:  0.364 train_acc:  0.81 test_loss:  0.425 test_acc:  0.784\n",
      "min loss epoch:26\n",
      "epoch:  27 train_loss:  0.362 train_acc:  0.811 test_loss:  0.424 test_acc:  0.789\n",
      "min loss epoch:27\n",
      "max acc epoch：27        max acc：0.789\n",
      "epoch:  28 train_loss:  0.36 train_acc:  0.812 test_loss:  0.423 test_acc:  0.789\n",
      "min loss epoch:28\n",
      "epoch:  29 train_loss:  0.357 train_acc:  0.812 test_loss:  0.421 test_acc:  0.791\n",
      "min loss epoch:29\n",
      "max acc epoch：29        max acc：0.791\n",
      "epoch:  30 train_loss:  0.356 train_acc:  0.814 test_loss:  0.42 test_acc:  0.79\n",
      "min loss epoch:30\n",
      "epoch:  31 train_loss:  0.354 train_acc:  0.818 test_loss:  0.419 test_acc:  0.79\n",
      "min loss epoch:31\n",
      "epoch:  32 train_loss:  0.351 train_acc:  0.819 test_loss:  0.417 test_acc:  0.791\n",
      "min loss epoch:32\n",
      "epoch:  33 train_loss:  0.349 train_acc:  0.82 test_loss:  0.414 test_acc:  0.794\n",
      "min loss epoch:33\n",
      "max acc epoch：33        max acc：0.794\n",
      "epoch:  34 train_loss:  0.346 train_acc:  0.822 test_loss:  0.412 test_acc:  0.797\n",
      "min loss epoch:34\n",
      "max acc epoch：34        max acc：0.797\n",
      "epoch:  35 train_loss:  0.345 train_acc:  0.822 test_loss:  0.411 test_acc:  0.797\n",
      "min loss epoch:35\n",
      "epoch:  36 train_loss:  0.344 train_acc:  0.822 test_loss:  0.41 test_acc:  0.796\n",
      "min loss epoch:36\n",
      "epoch:  37 train_loss:  0.342 train_acc:  0.824 test_loss:  0.409 test_acc:  0.796\n",
      "min loss epoch:37\n",
      "epoch:  38 train_loss:  0.341 train_acc:  0.825 test_loss:  0.407 test_acc:  0.801\n",
      "min loss epoch:38\n",
      "max acc epoch：38        max acc：0.801\n",
      "epoch:  39 train_loss:  0.339 train_acc:  0.826 test_loss:  0.407 test_acc:  0.796\n",
      "min loss epoch:39\n",
      "epoch:  40 train_loss:  0.338 train_acc:  0.828 test_loss:  0.407 test_acc:  0.798\n",
      "epoch:  41 train_loss:  0.338 train_acc:  0.826 test_loss:  0.406 test_acc:  0.799\n",
      "min loss epoch:41\n",
      "epoch:  42 train_loss:  0.336 train_acc:  0.829 test_loss:  0.406 test_acc:  0.801\n",
      "epoch:  43 train_loss:  0.334 train_acc:  0.832 test_loss:  0.404 test_acc:  0.801\n",
      "min loss epoch:43\n",
      "epoch:  44 train_loss:  0.333 train_acc:  0.83 test_loss:  0.404 test_acc:  0.802\n",
      "min loss epoch:44\n",
      "max acc epoch：44        max acc：0.802\n",
      "epoch:  45 train_loss:  0.332 train_acc:  0.831 test_loss:  0.403 test_acc:  0.799\n",
      "min loss epoch:45\n",
      "epoch:  46 train_loss:  0.331 train_acc:  0.831 test_loss:  0.403 test_acc:  0.803\n",
      "min loss epoch:46\n",
      "max acc epoch：46        max acc：0.803\n",
      "epoch:  47 train_loss:  0.33 train_acc:  0.832 test_loss:  0.401 test_acc:  0.804\n",
      "min loss epoch:47\n",
      "max acc epoch：47        max acc：0.804\n",
      "epoch:  48 train_loss:  0.329 train_acc:  0.831 test_loss:  0.4 test_acc:  0.804\n",
      "min loss epoch:48\n",
      "epoch:  49 train_loss:  0.327 train_acc:  0.833 test_loss:  0.399 test_acc:  0.801\n",
      "min loss epoch:49\n",
      "epoch:  50 train_loss:  0.327 train_acc:  0.834 test_loss:  0.399 test_acc:  0.804\n",
      "min loss epoch:50\n",
      "epoch:  51 train_loss:  0.326 train_acc:  0.834 test_loss:  0.399 test_acc:  0.802\n",
      "min loss epoch:51\n",
      "epoch:  52 train_loss:  0.326 train_acc:  0.836 test_loss:  0.397 test_acc:  0.803\n",
      "min loss epoch:52\n",
      "epoch:  53 train_loss:  0.323 train_acc:  0.838 test_loss:  0.395 test_acc:  0.801\n",
      "min loss epoch:53\n",
      "epoch:  54 train_loss:  0.322 train_acc:  0.84 test_loss:  0.396 test_acc:  0.801\n",
      "epoch:  55 train_loss:  0.321 train_acc:  0.84 test_loss:  0.394 test_acc:  0.797\n",
      "min loss epoch:55\n",
      "epoch:  56 train_loss:  0.32 train_acc:  0.839 test_loss:  0.393 test_acc:  0.799\n",
      "min loss epoch:56\n",
      "epoch:  57 train_loss:  0.319 train_acc:  0.84 test_loss:  0.394 test_acc:  0.799\n",
      "epoch:  58 train_loss:  0.318 train_acc:  0.84 test_loss:  0.392 test_acc:  0.802\n",
      "min loss epoch:58\n",
      "epoch:  59 train_loss:  0.317 train_acc:  0.841 test_loss:  0.392 test_acc:  0.802\n",
      "min loss epoch:59\n",
      "epoch:  60 train_loss:  0.317 train_acc:  0.841 test_loss:  0.392 test_acc:  0.801\n",
      "epoch:  61 train_loss:  0.316 train_acc:  0.84 test_loss:  0.392 test_acc:  0.801\n",
      "min loss epoch:61\n",
      "epoch:  62 train_loss:  0.314 train_acc:  0.841 test_loss:  0.391 test_acc:  0.8\n",
      "min loss epoch:62\n",
      "epoch:  63 train_loss:  0.314 train_acc:  0.844 test_loss:  0.391 test_acc:  0.804\n",
      "epoch:  64 train_loss:  0.313 train_acc:  0.843 test_loss:  0.39 test_acc:  0.8\n",
      "min loss epoch:64\n",
      "epoch:  65 train_loss:  0.311 train_acc:  0.843 test_loss:  0.389 test_acc:  0.8\n",
      "min loss epoch:65\n",
      "epoch:  66 train_loss:  0.311 train_acc:  0.843 test_loss:  0.388 test_acc:  0.8\n",
      "min loss epoch:66\n",
      "epoch:  67 train_loss:  0.31 train_acc:  0.843 test_loss:  0.388 test_acc:  0.8\n",
      "min loss epoch:67\n",
      "epoch:  68 train_loss:  0.31 train_acc:  0.843 test_loss:  0.387 test_acc:  0.804\n",
      "min loss epoch:68\n",
      "epoch:  69 train_loss:  0.309 train_acc:  0.843 test_loss:  0.385 test_acc:  0.799\n",
      "min loss epoch:69\n",
      "epoch:  70 train_loss:  0.307 train_acc:  0.846 test_loss:  0.384 test_acc:  0.804\n",
      "min loss epoch:70\n",
      "epoch:  71 train_loss:  0.307 train_acc:  0.845 test_loss:  0.384 test_acc:  0.804\n",
      "min loss epoch:71\n",
      "epoch:  72 train_loss:  0.306 train_acc:  0.842 test_loss:  0.384 test_acc:  0.802\n",
      "min loss epoch:72\n",
      "epoch:  73 train_loss:  0.306 train_acc:  0.845 test_loss:  0.383 test_acc:  0.801\n",
      "min loss epoch:73\n",
      "epoch:  74 train_loss:  0.304 train_acc:  0.846 test_loss:  0.382 test_acc:  0.806\n",
      "min loss epoch:74\n",
      "max acc epoch：74        max acc：0.806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  75 train_loss:  0.304 train_acc:  0.848 test_loss:  0.381 test_acc:  0.804\n",
      "min loss epoch:75\n",
      "epoch:  76 train_loss:  0.303 train_acc:  0.847 test_loss:  0.382 test_acc:  0.804\n",
      "epoch:  77 train_loss:  0.302 train_acc:  0.847 test_loss:  0.38 test_acc:  0.804\n",
      "min loss epoch:77\n",
      "epoch:  78 train_loss:  0.301 train_acc:  0.85 test_loss:  0.379 test_acc:  0.806\n",
      "min loss epoch:78\n",
      "epoch:  79 train_loss:  0.3 train_acc:  0.848 test_loss:  0.378 test_acc:  0.807\n",
      "min loss epoch:79\n",
      "max acc epoch：79        max acc：0.807\n",
      "epoch:  80 train_loss:  0.299 train_acc:  0.848 test_loss:  0.378 test_acc:  0.809\n",
      "min loss epoch:80\n",
      "max acc epoch：80        max acc：0.809\n",
      "epoch:  81 train_loss:  0.298 train_acc:  0.849 test_loss:  0.378 test_acc:  0.81\n",
      "max acc epoch：81        max acc：0.81\n",
      "epoch:  82 train_loss:  0.299 train_acc:  0.85 test_loss:  0.379 test_acc:  0.81\n",
      "epoch:  83 train_loss:  0.297 train_acc:  0.85 test_loss:  0.378 test_acc:  0.81\n",
      "epoch:  84 train_loss:  0.296 train_acc:  0.85 test_loss:  0.377 test_acc:  0.815\n",
      "min loss epoch:84\n",
      "max acc epoch：84        max acc：0.815\n",
      "epoch:  85 train_loss:  0.295 train_acc:  0.851 test_loss:  0.376 test_acc:  0.813\n",
      "min loss epoch:85\n",
      "epoch:  86 train_loss:  0.295 train_acc:  0.852 test_loss:  0.376 test_acc:  0.814\n",
      "epoch:  87 train_loss:  0.295 train_acc:  0.85 test_loss:  0.375 test_acc:  0.816\n",
      "min loss epoch:87\n",
      "max acc epoch：87        max acc：0.816\n",
      "epoch:  88 train_loss:  0.294 train_acc:  0.852 test_loss:  0.378 test_acc:  0.813\n",
      "epoch:  89 train_loss:  0.294 train_acc:  0.851 test_loss:  0.377 test_acc:  0.819\n",
      "max acc epoch：89        max acc：0.819\n",
      "epoch:  90 train_loss:  0.293 train_acc:  0.853 test_loss:  0.377 test_acc:  0.819\n",
      "epoch:  91 train_loss:  0.292 train_acc:  0.853 test_loss:  0.376 test_acc:  0.819\n",
      "epoch:  92 train_loss:  0.291 train_acc:  0.853 test_loss:  0.376 test_acc:  0.818\n",
      "epoch:  93 train_loss:  0.291 train_acc:  0.854 test_loss:  0.376 test_acc:  0.819\n",
      "epoch:  94 train_loss:  0.29 train_acc:  0.854 test_loss:  0.377 test_acc:  0.816\n",
      "epoch:  95 train_loss:  0.29 train_acc:  0.855 test_loss:  0.375 test_acc:  0.818\n",
      "min loss epoch:95\n",
      "epoch:  96 train_loss:  0.289 train_acc:  0.853 test_loss:  0.375 test_acc:  0.818\n",
      "min loss epoch:96\n",
      "epoch:  97 train_loss:  0.289 train_acc:  0.856 test_loss:  0.375 test_acc:  0.819\n",
      "epoch:  98 train_loss:  0.289 train_acc:  0.853 test_loss:  0.376 test_acc:  0.813\n",
      "epoch:  99 train_loss:  0.288 train_acc:  0.856 test_loss:  0.374 test_acc:  0.818\n",
      "min loss epoch:99\n",
      "End max acc epoch：99        max acc：0.819\n",
      "['keypoint', 'contrast', 'edgeLength', 'Hue1', 'Hue2', 'Hue3', 'Hue4', 'Hue5', 'Hue6', 'Hue7', 'Saturation', 'Value']\n",
      "6697\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([1340, 12])\n",
      "epoch:  0 train_loss:  0.479 train_acc:  0.753 test_loss:  0.502 test_acc:  0.747\n",
      "min loss epoch:0\n",
      "max acc epoch：0        max acc：0.747\n",
      "epoch:  1 train_loss:  0.461 train_acc:  0.757 test_loss:  0.489 test_acc:  0.752\n",
      "min loss epoch:1\n",
      "max acc epoch：1        max acc：0.752\n",
      "epoch:  2 train_loss:  0.451 train_acc:  0.763 test_loss:  0.484 test_acc:  0.752\n",
      "min loss epoch:2\n",
      "epoch:  3 train_loss:  0.444 train_acc:  0.769 test_loss:  0.479 test_acc:  0.752\n",
      "min loss epoch:3\n",
      "epoch:  4 train_loss:  0.438 train_acc:  0.771 test_loss:  0.475 test_acc:  0.762\n",
      "min loss epoch:4\n",
      "max acc epoch：4        max acc：0.762\n",
      "epoch:  5 train_loss:  0.433 train_acc:  0.774 test_loss:  0.47 test_acc:  0.757\n",
      "min loss epoch:5\n",
      "epoch:  6 train_loss:  0.428 train_acc:  0.777 test_loss:  0.467 test_acc:  0.755\n",
      "min loss epoch:6\n",
      "epoch:  7 train_loss:  0.424 train_acc:  0.779 test_loss:  0.464 test_acc:  0.757\n",
      "min loss epoch:7\n",
      "epoch:  8 train_loss:  0.42 train_acc:  0.78 test_loss:  0.461 test_acc:  0.756\n",
      "min loss epoch:8\n",
      "epoch:  9 train_loss:  0.416 train_acc:  0.783 test_loss:  0.459 test_acc:  0.757\n",
      "min loss epoch:9\n",
      "epoch:  10 train_loss:  0.413 train_acc:  0.786 test_loss:  0.457 test_acc:  0.758\n",
      "min loss epoch:10\n",
      "epoch:  11 train_loss:  0.409 train_acc:  0.788 test_loss:  0.455 test_acc:  0.758\n",
      "min loss epoch:11\n",
      "epoch:  12 train_loss:  0.407 train_acc:  0.789 test_loss:  0.454 test_acc:  0.757\n",
      "min loss epoch:12\n",
      "epoch:  13 train_loss:  0.404 train_acc:  0.792 test_loss:  0.452 test_acc:  0.761\n",
      "min loss epoch:13\n",
      "epoch:  14 train_loss:  0.401 train_acc:  0.794 test_loss:  0.45 test_acc:  0.767\n",
      "min loss epoch:14\n",
      "max acc epoch：14        max acc：0.767\n",
      "epoch:  15 train_loss:  0.398 train_acc:  0.795 test_loss:  0.448 test_acc:  0.766\n",
      "min loss epoch:15\n",
      "epoch:  16 train_loss:  0.396 train_acc:  0.795 test_loss:  0.447 test_acc:  0.767\n",
      "min loss epoch:16\n",
      "epoch:  17 train_loss:  0.394 train_acc:  0.797 test_loss:  0.446 test_acc:  0.769\n",
      "min loss epoch:17\n",
      "max acc epoch：17        max acc：0.769\n",
      "epoch:  18 train_loss:  0.391 train_acc:  0.8 test_loss:  0.445 test_acc:  0.775\n",
      "min loss epoch:18\n",
      "max acc epoch：18        max acc：0.775\n",
      "epoch:  19 train_loss:  0.389 train_acc:  0.803 test_loss:  0.443 test_acc:  0.772\n",
      "min loss epoch:19\n",
      "epoch:  20 train_loss:  0.386 train_acc:  0.804 test_loss:  0.443 test_acc:  0.773\n",
      "min loss epoch:20\n",
      "epoch:  21 train_loss:  0.384 train_acc:  0.803 test_loss:  0.44 test_acc:  0.773\n",
      "min loss epoch:21\n",
      "epoch:  22 train_loss:  0.382 train_acc:  0.803 test_loss:  0.439 test_acc:  0.775\n",
      "min loss epoch:22\n",
      "epoch:  23 train_loss:  0.379 train_acc:  0.803 test_loss:  0.437 test_acc:  0.775\n",
      "min loss epoch:23\n",
      "epoch:  24 train_loss:  0.378 train_acc:  0.805 test_loss:  0.436 test_acc:  0.775\n",
      "min loss epoch:24\n",
      "epoch:  25 train_loss:  0.376 train_acc:  0.806 test_loss:  0.436 test_acc:  0.781\n",
      "min loss epoch:25\n",
      "max acc epoch：25        max acc：0.781\n",
      "epoch:  26 train_loss:  0.374 train_acc:  0.808 test_loss:  0.434 test_acc:  0.782\n",
      "min loss epoch:26\n",
      "max acc epoch：26        max acc：0.782\n",
      "epoch:  27 train_loss:  0.373 train_acc:  0.811 test_loss:  0.433 test_acc:  0.784\n",
      "min loss epoch:27\n",
      "max acc epoch：27        max acc：0.784\n",
      "epoch:  28 train_loss:  0.371 train_acc:  0.812 test_loss:  0.432 test_acc:  0.784\n",
      "min loss epoch:28\n",
      "epoch:  29 train_loss:  0.369 train_acc:  0.812 test_loss:  0.432 test_acc:  0.784\n",
      "min loss epoch:29\n",
      "epoch:  30 train_loss:  0.368 train_acc:  0.813 test_loss:  0.431 test_acc:  0.786\n",
      "min loss epoch:30\n",
      "max acc epoch：30        max acc：0.786\n",
      "epoch:  31 train_loss:  0.367 train_acc:  0.814 test_loss:  0.43 test_acc:  0.785\n",
      "min loss epoch:31\n",
      "epoch:  32 train_loss:  0.365 train_acc:  0.815 test_loss:  0.429 test_acc:  0.783\n",
      "min loss epoch:32\n",
      "epoch:  33 train_loss:  0.364 train_acc:  0.815 test_loss:  0.429 test_acc:  0.782\n",
      "min loss epoch:33\n",
      "epoch:  34 train_loss:  0.363 train_acc:  0.815 test_loss:  0.429 test_acc:  0.784\n",
      "min loss epoch:34\n",
      "epoch:  35 train_loss:  0.362 train_acc:  0.815 test_loss:  0.428 test_acc:  0.781\n",
      "min loss epoch:35\n",
      "epoch:  36 train_loss:  0.361 train_acc:  0.817 test_loss:  0.427 test_acc:  0.784\n",
      "min loss epoch:36\n",
      "epoch:  37 train_loss:  0.36 train_acc:  0.818 test_loss:  0.427 test_acc:  0.785\n",
      "min loss epoch:37\n",
      "epoch:  38 train_loss:  0.359 train_acc:  0.816 test_loss:  0.427 test_acc:  0.787\n",
      "max acc epoch：38        max acc：0.787\n",
      "epoch:  39 train_loss:  0.358 train_acc:  0.817 test_loss:  0.427 test_acc:  0.787\n",
      "epoch:  40 train_loss:  0.356 train_acc:  0.819 test_loss:  0.426 test_acc:  0.787\n",
      "min loss epoch:40\n",
      "epoch:  41 train_loss:  0.355 train_acc:  0.82 test_loss:  0.426 test_acc:  0.784\n",
      "min loss epoch:41\n",
      "epoch:  42 train_loss:  0.354 train_acc:  0.821 test_loss:  0.425 test_acc:  0.784\n",
      "min loss epoch:42\n",
      "epoch:  43 train_loss:  0.353 train_acc:  0.822 test_loss:  0.424 test_acc:  0.786\n",
      "min loss epoch:43\n",
      "epoch:  44 train_loss:  0.352 train_acc:  0.822 test_loss:  0.423 test_acc:  0.786\n",
      "min loss epoch:44\n",
      "epoch:  45 train_loss:  0.351 train_acc:  0.822 test_loss:  0.422 test_acc:  0.788\n",
      "min loss epoch:45\n",
      "max acc epoch：45        max acc：0.788\n",
      "epoch:  46 train_loss:  0.35 train_acc:  0.825 test_loss:  0.421 test_acc:  0.787\n",
      "min loss epoch:46\n",
      "epoch:  47 train_loss:  0.349 train_acc:  0.823 test_loss:  0.421 test_acc:  0.787\n",
      "min loss epoch:47\n",
      "epoch:  48 train_loss:  0.348 train_acc:  0.825 test_loss:  0.42 test_acc:  0.788\n",
      "min loss epoch:48\n",
      "epoch:  49 train_loss:  0.347 train_acc:  0.827 test_loss:  0.42 test_acc:  0.788\n",
      "epoch:  50 train_loss:  0.346 train_acc:  0.824 test_loss:  0.42 test_acc:  0.787\n",
      "min loss epoch:50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  51 train_loss:  0.345 train_acc:  0.827 test_loss:  0.419 test_acc:  0.789\n",
      "min loss epoch:51\n",
      "max acc epoch：51        max acc：0.789\n",
      "epoch:  52 train_loss:  0.344 train_acc:  0.828 test_loss:  0.419 test_acc:  0.789\n",
      "min loss epoch:52\n",
      "epoch:  53 train_loss:  0.344 train_acc:  0.827 test_loss:  0.419 test_acc:  0.79\n",
      "max acc epoch：53        max acc：0.79\n",
      "epoch:  54 train_loss:  0.343 train_acc:  0.828 test_loss:  0.418 test_acc:  0.79\n",
      "min loss epoch:54\n",
      "epoch:  55 train_loss:  0.342 train_acc:  0.829 test_loss:  0.418 test_acc:  0.79\n",
      "min loss epoch:55\n",
      "epoch:  56 train_loss:  0.341 train_acc:  0.831 test_loss:  0.416 test_acc:  0.793\n",
      "min loss epoch:56\n",
      "max acc epoch：56        max acc：0.793\n",
      "epoch:  57 train_loss:  0.34 train_acc:  0.831 test_loss:  0.416 test_acc:  0.795\n",
      "min loss epoch:57\n",
      "max acc epoch：57        max acc：0.795\n",
      "epoch:  58 train_loss:  0.339 train_acc:  0.831 test_loss:  0.415 test_acc:  0.793\n",
      "min loss epoch:58\n",
      "epoch:  59 train_loss:  0.338 train_acc:  0.831 test_loss:  0.414 test_acc:  0.794\n",
      "min loss epoch:59\n",
      "epoch:  60 train_loss:  0.337 train_acc:  0.833 test_loss:  0.412 test_acc:  0.795\n",
      "min loss epoch:60\n",
      "epoch:  61 train_loss:  0.336 train_acc:  0.833 test_loss:  0.412 test_acc:  0.797\n",
      "min loss epoch:61\n",
      "max acc epoch：61        max acc：0.797\n",
      "epoch:  62 train_loss:  0.336 train_acc:  0.834 test_loss:  0.411 test_acc:  0.797\n",
      "min loss epoch:62\n",
      "epoch:  63 train_loss:  0.335 train_acc:  0.836 test_loss:  0.41 test_acc:  0.798\n",
      "min loss epoch:63\n",
      "max acc epoch：63        max acc：0.798\n",
      "epoch:  64 train_loss:  0.335 train_acc:  0.836 test_loss:  0.41 test_acc:  0.797\n",
      "epoch:  65 train_loss:  0.333 train_acc:  0.835 test_loss:  0.409 test_acc:  0.799\n",
      "min loss epoch:65\n",
      "max acc epoch：65        max acc：0.799\n",
      "epoch:  66 train_loss:  0.333 train_acc:  0.835 test_loss:  0.409 test_acc:  0.799\n",
      "min loss epoch:66\n",
      "epoch:  67 train_loss:  0.332 train_acc:  0.837 test_loss:  0.408 test_acc:  0.8\n",
      "min loss epoch:67\n",
      "max acc epoch：67        max acc：0.8\n",
      "epoch:  68 train_loss:  0.332 train_acc:  0.837 test_loss:  0.407 test_acc:  0.804\n",
      "min loss epoch:68\n",
      "max acc epoch：68        max acc：0.804\n",
      "epoch:  69 train_loss:  0.33 train_acc:  0.838 test_loss:  0.406 test_acc:  0.804\n",
      "min loss epoch:69\n",
      "epoch:  70 train_loss:  0.33 train_acc:  0.837 test_loss:  0.407 test_acc:  0.8\n",
      "epoch:  71 train_loss:  0.329 train_acc:  0.836 test_loss:  0.406 test_acc:  0.802\n",
      "min loss epoch:71\n",
      "epoch:  72 train_loss:  0.328 train_acc:  0.838 test_loss:  0.405 test_acc:  0.804\n",
      "min loss epoch:72\n",
      "epoch:  73 train_loss:  0.328 train_acc:  0.839 test_loss:  0.404 test_acc:  0.804\n",
      "min loss epoch:73\n",
      "epoch:  74 train_loss:  0.326 train_acc:  0.839 test_loss:  0.403 test_acc:  0.804\n",
      "min loss epoch:74\n",
      "epoch:  75 train_loss:  0.326 train_acc:  0.84 test_loss:  0.403 test_acc:  0.802\n",
      "min loss epoch:75\n",
      "epoch:  76 train_loss:  0.325 train_acc:  0.84 test_loss:  0.402 test_acc:  0.804\n",
      "min loss epoch:76\n",
      "epoch:  77 train_loss:  0.325 train_acc:  0.839 test_loss:  0.402 test_acc:  0.799\n",
      "min loss epoch:77\n",
      "epoch:  78 train_loss:  0.324 train_acc:  0.84 test_loss:  0.401 test_acc:  0.801\n",
      "min loss epoch:78\n",
      "epoch:  79 train_loss:  0.324 train_acc:  0.841 test_loss:  0.401 test_acc:  0.799\n",
      "min loss epoch:79\n",
      "epoch:  80 train_loss:  0.323 train_acc:  0.841 test_loss:  0.4 test_acc:  0.801\n",
      "min loss epoch:80\n",
      "epoch:  81 train_loss:  0.323 train_acc:  0.839 test_loss:  0.4 test_acc:  0.801\n",
      "epoch:  82 train_loss:  0.321 train_acc:  0.842 test_loss:  0.399 test_acc:  0.804\n",
      "min loss epoch:82\n",
      "epoch:  83 train_loss:  0.322 train_acc:  0.84 test_loss:  0.4 test_acc:  0.803\n",
      "epoch:  84 train_loss:  0.321 train_acc:  0.841 test_loss:  0.399 test_acc:  0.804\n",
      "epoch:  85 train_loss:  0.32 train_acc:  0.842 test_loss:  0.398 test_acc:  0.803\n",
      "min loss epoch:85\n",
      "epoch:  86 train_loss:  0.319 train_acc:  0.843 test_loss:  0.397 test_acc:  0.805\n",
      "min loss epoch:86\n",
      "max acc epoch：86        max acc：0.805\n",
      "epoch:  87 train_loss:  0.318 train_acc:  0.843 test_loss:  0.396 test_acc:  0.803\n",
      "min loss epoch:87\n",
      "epoch:  88 train_loss:  0.318 train_acc:  0.842 test_loss:  0.396 test_acc:  0.804\n",
      "epoch:  89 train_loss:  0.316 train_acc:  0.843 test_loss:  0.395 test_acc:  0.806\n",
      "min loss epoch:89\n",
      "max acc epoch：89        max acc：0.806\n",
      "epoch:  90 train_loss:  0.315 train_acc:  0.845 test_loss:  0.394 test_acc:  0.806\n",
      "min loss epoch:90\n",
      "epoch:  91 train_loss:  0.315 train_acc:  0.844 test_loss:  0.394 test_acc:  0.809\n",
      "min loss epoch:91\n",
      "max acc epoch：91        max acc：0.809\n",
      "epoch:  92 train_loss:  0.313 train_acc:  0.844 test_loss:  0.392 test_acc:  0.806\n",
      "min loss epoch:92\n",
      "epoch:  93 train_loss:  0.313 train_acc:  0.843 test_loss:  0.393 test_acc:  0.805\n",
      "epoch:  94 train_loss:  0.312 train_acc:  0.844 test_loss:  0.392 test_acc:  0.804\n",
      "min loss epoch:94\n",
      "epoch:  95 train_loss:  0.311 train_acc:  0.844 test_loss:  0.391 test_acc:  0.806\n",
      "min loss epoch:95\n",
      "epoch:  96 train_loss:  0.311 train_acc:  0.846 test_loss:  0.391 test_acc:  0.801\n",
      "min loss epoch:96\n",
      "epoch:  97 train_loss:  0.31 train_acc:  0.845 test_loss:  0.39 test_acc:  0.803\n",
      "min loss epoch:97\n",
      "epoch:  98 train_loss:  0.309 train_acc:  0.847 test_loss:  0.389 test_acc:  0.801\n",
      "min loss epoch:98\n",
      "epoch:  99 train_loss:  0.308 train_acc:  0.847 test_loss:  0.388 test_acc:  0.801\n",
      "min loss epoch:99\n",
      "End max acc epoch：99        max acc：0.809\n",
      "['keypoint', 'brightness', 'edgeLength', 'Hue1', 'Hue2', 'Hue3', 'Hue4', 'Hue5', 'Hue6', 'Hue7', 'Saturation', 'Value']\n",
      "6697\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([1340, 12])\n",
      "epoch:  0 train_loss:  0.463 train_acc:  0.75 test_loss:  0.482 test_acc:  0.742\n",
      "min loss epoch:0\n",
      "max acc epoch：0        max acc：0.742\n",
      "epoch:  1 train_loss:  0.445 train_acc:  0.762 test_loss:  0.468 test_acc:  0.743\n",
      "min loss epoch:1\n",
      "max acc epoch：1        max acc：0.743\n",
      "epoch:  2 train_loss:  0.437 train_acc:  0.766 test_loss:  0.462 test_acc:  0.748\n",
      "min loss epoch:2\n",
      "max acc epoch：2        max acc：0.748\n",
      "epoch:  3 train_loss:  0.431 train_acc:  0.772 test_loss:  0.459 test_acc:  0.754\n",
      "min loss epoch:3\n",
      "max acc epoch：3        max acc：0.754\n",
      "epoch:  4 train_loss:  0.426 train_acc:  0.776 test_loss:  0.456 test_acc:  0.757\n",
      "min loss epoch:4\n",
      "max acc epoch：4        max acc：0.757\n",
      "epoch:  5 train_loss:  0.422 train_acc:  0.779 test_loss:  0.454 test_acc:  0.759\n",
      "min loss epoch:5\n",
      "max acc epoch：5        max acc：0.759\n",
      "epoch:  6 train_loss:  0.419 train_acc:  0.782 test_loss:  0.453 test_acc:  0.758\n",
      "min loss epoch:6\n",
      "epoch:  7 train_loss:  0.416 train_acc:  0.783 test_loss:  0.451 test_acc:  0.757\n",
      "min loss epoch:7\n",
      "epoch:  8 train_loss:  0.413 train_acc:  0.786 test_loss:  0.45 test_acc:  0.756\n",
      "min loss epoch:8\n",
      "epoch:  9 train_loss:  0.411 train_acc:  0.786 test_loss:  0.448 test_acc:  0.757\n",
      "min loss epoch:9\n",
      "epoch:  10 train_loss:  0.408 train_acc:  0.788 test_loss:  0.447 test_acc:  0.757\n",
      "min loss epoch:10\n",
      "epoch:  11 train_loss:  0.406 train_acc:  0.788 test_loss:  0.445 test_acc:  0.759\n",
      "min loss epoch:11\n",
      "epoch:  12 train_loss:  0.403 train_acc:  0.79 test_loss:  0.442 test_acc:  0.762\n",
      "min loss epoch:12\n",
      "max acc epoch：12        max acc：0.762\n",
      "epoch:  13 train_loss:  0.4 train_acc:  0.792 test_loss:  0.44 test_acc:  0.763\n",
      "min loss epoch:13\n",
      "max acc epoch：13        max acc：0.763\n",
      "epoch:  14 train_loss:  0.397 train_acc:  0.793 test_loss:  0.437 test_acc:  0.765\n",
      "min loss epoch:14\n",
      "max acc epoch：14        max acc：0.765\n",
      "epoch:  15 train_loss:  0.394 train_acc:  0.797 test_loss:  0.435 test_acc:  0.766\n",
      "min loss epoch:15\n",
      "max acc epoch：15        max acc：0.766\n",
      "epoch:  16 train_loss:  0.389 train_acc:  0.8 test_loss:  0.43 test_acc:  0.771\n",
      "min loss epoch:16\n",
      "max acc epoch：16        max acc：0.771\n",
      "epoch:  17 train_loss:  0.386 train_acc:  0.802 test_loss:  0.427 test_acc:  0.775\n",
      "min loss epoch:17\n",
      "max acc epoch：17        max acc：0.775\n",
      "epoch:  18 train_loss:  0.382 train_acc:  0.804 test_loss:  0.424 test_acc:  0.779\n",
      "min loss epoch:18\n",
      "max acc epoch：18        max acc：0.779\n",
      "epoch:  19 train_loss:  0.379 train_acc:  0.806 test_loss:  0.42 test_acc:  0.781\n",
      "min loss epoch:19\n",
      "max acc epoch：19        max acc：0.781\n",
      "epoch:  20 train_loss:  0.375 train_acc:  0.81 test_loss:  0.417 test_acc:  0.782\n",
      "min loss epoch:20\n",
      "max acc epoch：20        max acc：0.782\n",
      "epoch:  21 train_loss:  0.372 train_acc:  0.811 test_loss:  0.413 test_acc:  0.785\n",
      "min loss epoch:21\n",
      "max acc epoch：21        max acc：0.785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  22 train_loss:  0.369 train_acc:  0.814 test_loss:  0.411 test_acc:  0.782\n",
      "min loss epoch:22\n",
      "epoch:  23 train_loss:  0.365 train_acc:  0.816 test_loss:  0.407 test_acc:  0.783\n",
      "min loss epoch:23\n",
      "epoch:  24 train_loss:  0.363 train_acc:  0.82 test_loss:  0.404 test_acc:  0.786\n",
      "min loss epoch:24\n",
      "max acc epoch：24        max acc：0.786\n",
      "epoch:  25 train_loss:  0.359 train_acc:  0.823 test_loss:  0.401 test_acc:  0.79\n",
      "min loss epoch:25\n",
      "max acc epoch：25        max acc：0.79\n",
      "epoch:  26 train_loss:  0.357 train_acc:  0.824 test_loss:  0.4 test_acc:  0.79\n",
      "min loss epoch:26\n",
      "epoch:  27 train_loss:  0.353 train_acc:  0.823 test_loss:  0.396 test_acc:  0.794\n",
      "min loss epoch:27\n",
      "max acc epoch：27        max acc：0.794\n",
      "epoch:  28 train_loss:  0.35 train_acc:  0.826 test_loss:  0.394 test_acc:  0.796\n",
      "min loss epoch:28\n",
      "max acc epoch：28        max acc：0.796\n",
      "epoch:  29 train_loss:  0.347 train_acc:  0.826 test_loss:  0.392 test_acc:  0.8\n",
      "min loss epoch:29\n",
      "max acc epoch：29        max acc：0.8\n",
      "epoch:  30 train_loss:  0.344 train_acc:  0.827 test_loss:  0.389 test_acc:  0.799\n",
      "min loss epoch:30\n",
      "epoch:  31 train_loss:  0.341 train_acc:  0.828 test_loss:  0.387 test_acc:  0.799\n",
      "min loss epoch:31\n",
      "epoch:  32 train_loss:  0.339 train_acc:  0.829 test_loss:  0.385 test_acc:  0.801\n",
      "min loss epoch:32\n",
      "max acc epoch：32        max acc：0.801\n",
      "epoch:  33 train_loss:  0.337 train_acc:  0.83 test_loss:  0.383 test_acc:  0.801\n",
      "min loss epoch:33\n",
      "epoch:  34 train_loss:  0.335 train_acc:  0.833 test_loss:  0.383 test_acc:  0.803\n",
      "min loss epoch:34\n",
      "max acc epoch：34        max acc：0.803\n",
      "epoch:  35 train_loss:  0.332 train_acc:  0.835 test_loss:  0.38 test_acc:  0.801\n",
      "min loss epoch:35\n",
      "epoch:  36 train_loss:  0.331 train_acc:  0.836 test_loss:  0.379 test_acc:  0.804\n",
      "min loss epoch:36\n",
      "max acc epoch：36        max acc：0.804\n",
      "epoch:  37 train_loss:  0.329 train_acc:  0.837 test_loss:  0.378 test_acc:  0.805\n",
      "min loss epoch:37\n",
      "max acc epoch：37        max acc：0.805\n",
      "epoch:  38 train_loss:  0.327 train_acc:  0.839 test_loss:  0.377 test_acc:  0.805\n",
      "min loss epoch:38\n",
      "epoch:  39 train_loss:  0.326 train_acc:  0.838 test_loss:  0.376 test_acc:  0.805\n",
      "min loss epoch:39\n",
      "epoch:  40 train_loss:  0.325 train_acc:  0.84 test_loss:  0.376 test_acc:  0.801\n",
      "epoch:  41 train_loss:  0.324 train_acc:  0.84 test_loss:  0.376 test_acc:  0.806\n",
      "max acc epoch：41        max acc：0.806\n",
      "epoch:  42 train_loss:  0.323 train_acc:  0.839 test_loss:  0.375 test_acc:  0.804\n",
      "min loss epoch:42\n",
      "epoch:  43 train_loss:  0.32 train_acc:  0.841 test_loss:  0.373 test_acc:  0.809\n",
      "min loss epoch:43\n",
      "max acc epoch：43        max acc：0.809\n",
      "epoch:  44 train_loss:  0.319 train_acc:  0.842 test_loss:  0.372 test_acc:  0.81\n",
      "min loss epoch:44\n",
      "max acc epoch：44        max acc：0.81\n",
      "epoch:  45 train_loss:  0.317 train_acc:  0.842 test_loss:  0.37 test_acc:  0.813\n",
      "min loss epoch:45\n",
      "max acc epoch：45        max acc：0.813\n",
      "epoch:  46 train_loss:  0.316 train_acc:  0.843 test_loss:  0.37 test_acc:  0.812\n",
      "min loss epoch:46\n",
      "epoch:  47 train_loss:  0.316 train_acc:  0.845 test_loss:  0.37 test_acc:  0.813\n",
      "epoch:  48 train_loss:  0.313 train_acc:  0.845 test_loss:  0.368 test_acc:  0.814\n",
      "min loss epoch:48\n",
      "max acc epoch：48        max acc：0.814\n",
      "epoch:  49 train_loss:  0.312 train_acc:  0.846 test_loss:  0.367 test_acc:  0.813\n",
      "min loss epoch:49\n",
      "epoch:  50 train_loss:  0.311 train_acc:  0.846 test_loss:  0.367 test_acc:  0.816\n",
      "min loss epoch:50\n",
      "max acc epoch：50        max acc：0.816\n",
      "epoch:  51 train_loss:  0.309 train_acc:  0.846 test_loss:  0.366 test_acc:  0.816\n",
      "min loss epoch:51\n",
      "epoch:  52 train_loss:  0.309 train_acc:  0.846 test_loss:  0.365 test_acc:  0.816\n",
      "min loss epoch:52\n",
      "epoch:  53 train_loss:  0.308 train_acc:  0.846 test_loss:  0.365 test_acc:  0.815\n",
      "min loss epoch:53\n",
      "epoch:  54 train_loss:  0.307 train_acc:  0.847 test_loss:  0.365 test_acc:  0.819\n",
      "max acc epoch：54        max acc：0.819\n",
      "epoch:  55 train_loss:  0.306 train_acc:  0.846 test_loss:  0.364 test_acc:  0.816\n",
      "min loss epoch:55\n",
      "epoch:  56 train_loss:  0.305 train_acc:  0.847 test_loss:  0.362 test_acc:  0.819\n",
      "min loss epoch:56\n",
      "epoch:  57 train_loss:  0.304 train_acc:  0.846 test_loss:  0.362 test_acc:  0.818\n",
      "min loss epoch:57\n",
      "epoch:  58 train_loss:  0.303 train_acc:  0.847 test_loss:  0.363 test_acc:  0.819\n",
      "epoch:  59 train_loss:  0.303 train_acc:  0.846 test_loss:  0.362 test_acc:  0.82\n",
      "max acc epoch：59        max acc：0.82\n",
      "epoch:  60 train_loss:  0.302 train_acc:  0.846 test_loss:  0.362 test_acc:  0.822\n",
      "min loss epoch:60\n",
      "max acc epoch：60        max acc：0.822\n",
      "epoch:  61 train_loss:  0.3 train_acc:  0.845 test_loss:  0.361 test_acc:  0.822\n",
      "min loss epoch:61\n",
      "epoch:  62 train_loss:  0.3 train_acc:  0.845 test_loss:  0.362 test_acc:  0.824\n",
      "max acc epoch：62        max acc：0.824\n",
      "epoch:  63 train_loss:  0.298 train_acc:  0.848 test_loss:  0.361 test_acc:  0.823\n",
      "min loss epoch:63\n",
      "epoch:  64 train_loss:  0.297 train_acc:  0.846 test_loss:  0.361 test_acc:  0.825\n",
      "max acc epoch：64        max acc：0.825\n",
      "epoch:  65 train_loss:  0.296 train_acc:  0.848 test_loss:  0.362 test_acc:  0.823\n",
      "epoch:  66 train_loss:  0.295 train_acc:  0.848 test_loss:  0.36 test_acc:  0.825\n",
      "min loss epoch:66\n",
      "epoch:  67 train_loss:  0.295 train_acc:  0.847 test_loss:  0.361 test_acc:  0.825\n",
      "epoch:  68 train_loss:  0.295 train_acc:  0.847 test_loss:  0.362 test_acc:  0.826\n",
      "max acc epoch：68        max acc：0.826\n",
      "epoch:  69 train_loss:  0.294 train_acc:  0.847 test_loss:  0.361 test_acc:  0.823\n",
      "epoch:  70 train_loss:  0.293 train_acc:  0.848 test_loss:  0.359 test_acc:  0.825\n",
      "min loss epoch:70\n",
      "epoch:  71 train_loss:  0.292 train_acc:  0.848 test_loss:  0.36 test_acc:  0.828\n",
      "max acc epoch：71        max acc：0.828\n",
      "epoch:  72 train_loss:  0.291 train_acc:  0.849 test_loss:  0.359 test_acc:  0.825\n",
      "epoch:  73 train_loss:  0.291 train_acc:  0.848 test_loss:  0.359 test_acc:  0.826\n",
      "min loss epoch:73\n",
      "epoch:  74 train_loss:  0.29 train_acc:  0.85 test_loss:  0.359 test_acc:  0.824\n",
      "epoch:  75 train_loss:  0.289 train_acc:  0.85 test_loss:  0.357 test_acc:  0.826\n",
      "min loss epoch:75\n",
      "epoch:  76 train_loss:  0.289 train_acc:  0.85 test_loss:  0.358 test_acc:  0.826\n",
      "epoch:  77 train_loss:  0.288 train_acc:  0.852 test_loss:  0.358 test_acc:  0.827\n",
      "epoch:  78 train_loss:  0.288 train_acc:  0.852 test_loss:  0.357 test_acc:  0.826\n",
      "epoch:  79 train_loss:  0.287 train_acc:  0.853 test_loss:  0.358 test_acc:  0.826\n",
      "epoch:  80 train_loss:  0.286 train_acc:  0.852 test_loss:  0.358 test_acc:  0.827\n",
      "epoch:  81 train_loss:  0.285 train_acc:  0.853 test_loss:  0.357 test_acc:  0.827\n",
      "epoch:  82 train_loss:  0.285 train_acc:  0.854 test_loss:  0.357 test_acc:  0.828\n",
      "epoch:  83 train_loss:  0.284 train_acc:  0.855 test_loss:  0.357 test_acc:  0.825\n",
      "epoch:  84 train_loss:  0.283 train_acc:  0.853 test_loss:  0.356 test_acc:  0.831\n",
      "min loss epoch:84\n",
      "max acc epoch：84        max acc：0.831\n",
      "epoch:  85 train_loss:  0.283 train_acc:  0.856 test_loss:  0.357 test_acc:  0.828\n",
      "epoch:  86 train_loss:  0.282 train_acc:  0.857 test_loss:  0.356 test_acc:  0.828\n",
      "min loss epoch:86\n",
      "epoch:  87 train_loss:  0.282 train_acc:  0.858 test_loss:  0.357 test_acc:  0.828\n",
      "epoch:  88 train_loss:  0.281 train_acc:  0.859 test_loss:  0.355 test_acc:  0.83\n",
      "min loss epoch:88\n",
      "epoch:  89 train_loss:  0.28 train_acc:  0.86 test_loss:  0.356 test_acc:  0.828\n",
      "epoch:  90 train_loss:  0.28 train_acc:  0.859 test_loss:  0.356 test_acc:  0.828\n",
      "epoch:  91 train_loss:  0.279 train_acc:  0.86 test_loss:  0.356 test_acc:  0.827\n",
      "epoch:  92 train_loss:  0.278 train_acc:  0.861 test_loss:  0.355 test_acc:  0.829\n",
      "epoch:  93 train_loss:  0.277 train_acc:  0.863 test_loss:  0.353 test_acc:  0.832\n",
      "min loss epoch:93\n",
      "max acc epoch：93        max acc：0.832\n",
      "epoch:  94 train_loss:  0.277 train_acc:  0.861 test_loss:  0.354 test_acc:  0.831\n",
      "epoch:  95 train_loss:  0.277 train_acc:  0.861 test_loss:  0.354 test_acc:  0.831\n",
      "epoch:  96 train_loss:  0.275 train_acc:  0.864 test_loss:  0.352 test_acc:  0.834\n",
      "min loss epoch:96\n",
      "max acc epoch：96        max acc：0.834\n",
      "epoch:  97 train_loss:  0.274 train_acc:  0.863 test_loss:  0.352 test_acc:  0.83\n",
      "min loss epoch:97\n",
      "epoch:  98 train_loss:  0.274 train_acc:  0.864 test_loss:  0.352 test_acc:  0.834\n",
      "epoch:  99 train_loss:  0.273 train_acc:  0.864 test_loss:  0.352 test_acc:  0.833\n",
      "End max acc epoch：99        max acc：0.834\n",
      "['keypoint', 'brightness', 'contrast', 'Hue1', 'Hue2', 'Hue3', 'Hue4', 'Hue5', 'Hue6', 'Hue7', 'Saturation', 'Value']\n",
      "6697\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([1340, 12])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 train_loss:  0.471 train_acc:  0.754 test_loss:  0.493 test_acc:  0.743\n",
      "min loss epoch:0\n",
      "max acc epoch：0        max acc：0.743\n",
      "epoch:  1 train_loss:  0.454 train_acc:  0.759 test_loss:  0.479 test_acc:  0.744\n",
      "min loss epoch:1\n",
      "max acc epoch：1        max acc：0.744\n",
      "epoch:  2 train_loss:  0.445 train_acc:  0.764 test_loss:  0.474 test_acc:  0.745\n",
      "min loss epoch:2\n",
      "max acc epoch：2        max acc：0.745\n",
      "epoch:  3 train_loss:  0.437 train_acc:  0.771 test_loss:  0.469 test_acc:  0.75\n",
      "min loss epoch:3\n",
      "max acc epoch：3        max acc：0.75\n",
      "epoch:  4 train_loss:  0.432 train_acc:  0.777 test_loss:  0.466 test_acc:  0.756\n",
      "min loss epoch:4\n",
      "max acc epoch：4        max acc：0.756\n",
      "epoch:  5 train_loss:  0.428 train_acc:  0.779 test_loss:  0.463 test_acc:  0.757\n",
      "min loss epoch:5\n",
      "max acc epoch：5        max acc：0.757\n",
      "epoch:  6 train_loss:  0.423 train_acc:  0.783 test_loss:  0.461 test_acc:  0.758\n",
      "min loss epoch:6\n",
      "max acc epoch：6        max acc：0.758\n",
      "epoch:  7 train_loss:  0.42 train_acc:  0.783 test_loss:  0.459 test_acc:  0.756\n",
      "min loss epoch:7\n",
      "epoch:  8 train_loss:  0.416 train_acc:  0.785 test_loss:  0.456 test_acc:  0.758\n",
      "min loss epoch:8\n",
      "epoch:  9 train_loss:  0.412 train_acc:  0.786 test_loss:  0.454 test_acc:  0.756\n",
      "min loss epoch:9\n",
      "epoch:  10 train_loss:  0.408 train_acc:  0.787 test_loss:  0.452 test_acc:  0.759\n",
      "min loss epoch:10\n",
      "max acc epoch：10        max acc：0.759\n",
      "epoch:  11 train_loss:  0.405 train_acc:  0.79 test_loss:  0.45 test_acc:  0.761\n",
      "min loss epoch:11\n",
      "max acc epoch：11        max acc：0.761\n",
      "epoch:  12 train_loss:  0.402 train_acc:  0.791 test_loss:  0.449 test_acc:  0.761\n",
      "min loss epoch:12\n",
      "epoch:  13 train_loss:  0.399 train_acc:  0.793 test_loss:  0.447 test_acc:  0.765\n",
      "min loss epoch:13\n",
      "max acc epoch：13        max acc：0.765\n",
      "epoch:  14 train_loss:  0.397 train_acc:  0.794 test_loss:  0.446 test_acc:  0.762\n",
      "min loss epoch:14\n",
      "epoch:  15 train_loss:  0.395 train_acc:  0.794 test_loss:  0.445 test_acc:  0.765\n",
      "min loss epoch:15\n",
      "epoch:  16 train_loss:  0.392 train_acc:  0.797 test_loss:  0.444 test_acc:  0.764\n",
      "min loss epoch:16\n",
      "epoch:  17 train_loss:  0.39 train_acc:  0.798 test_loss:  0.443 test_acc:  0.762\n",
      "min loss epoch:17\n",
      "epoch:  18 train_loss:  0.388 train_acc:  0.799 test_loss:  0.443 test_acc:  0.766\n",
      "min loss epoch:18\n",
      "max acc epoch：18        max acc：0.766\n",
      "epoch:  19 train_loss:  0.386 train_acc:  0.8 test_loss:  0.442 test_acc:  0.766\n",
      "min loss epoch:19\n",
      "epoch:  20 train_loss:  0.384 train_acc:  0.801 test_loss:  0.442 test_acc:  0.769\n",
      "min loss epoch:20\n",
      "max acc epoch：20        max acc：0.769\n",
      "epoch:  21 train_loss:  0.383 train_acc:  0.803 test_loss:  0.441 test_acc:  0.77\n",
      "min loss epoch:21\n",
      "max acc epoch：21        max acc：0.77\n",
      "epoch:  22 train_loss:  0.381 train_acc:  0.803 test_loss:  0.44 test_acc:  0.77\n",
      "min loss epoch:22\n",
      "epoch:  23 train_loss:  0.379 train_acc:  0.803 test_loss:  0.439 test_acc:  0.774\n",
      "min loss epoch:23\n",
      "max acc epoch：23        max acc：0.774\n",
      "epoch:  24 train_loss:  0.377 train_acc:  0.804 test_loss:  0.438 test_acc:  0.77\n",
      "min loss epoch:24\n",
      "epoch:  25 train_loss:  0.376 train_acc:  0.804 test_loss:  0.438 test_acc:  0.775\n",
      "max acc epoch：25        max acc：0.775\n",
      "epoch:  26 train_loss:  0.375 train_acc:  0.806 test_loss:  0.437 test_acc:  0.773\n",
      "min loss epoch:26\n",
      "epoch:  27 train_loss:  0.374 train_acc:  0.807 test_loss:  0.437 test_acc:  0.777\n",
      "min loss epoch:27\n",
      "max acc epoch：27        max acc：0.777\n",
      "epoch:  28 train_loss:  0.373 train_acc:  0.808 test_loss:  0.435 test_acc:  0.778\n",
      "min loss epoch:28\n",
      "max acc epoch：28        max acc：0.778\n",
      "epoch:  29 train_loss:  0.372 train_acc:  0.806 test_loss:  0.436 test_acc:  0.776\n",
      "epoch:  30 train_loss:  0.371 train_acc:  0.808 test_loss:  0.434 test_acc:  0.775\n",
      "min loss epoch:30\n",
      "epoch:  31 train_loss:  0.369 train_acc:  0.809 test_loss:  0.434 test_acc:  0.777\n",
      "min loss epoch:31\n",
      "epoch:  32 train_loss:  0.368 train_acc:  0.809 test_loss:  0.434 test_acc:  0.775\n",
      "min loss epoch:32\n",
      "epoch:  33 train_loss:  0.367 train_acc:  0.809 test_loss:  0.432 test_acc:  0.781\n",
      "min loss epoch:33\n",
      "max acc epoch：33        max acc：0.781\n",
      "epoch:  34 train_loss:  0.367 train_acc:  0.812 test_loss:  0.433 test_acc:  0.775\n",
      "epoch:  35 train_loss:  0.366 train_acc:  0.811 test_loss:  0.433 test_acc:  0.778\n",
      "epoch:  36 train_loss:  0.364 train_acc:  0.812 test_loss:  0.431 test_acc:  0.779\n",
      "min loss epoch:36\n",
      "epoch:  37 train_loss:  0.363 train_acc:  0.814 test_loss:  0.429 test_acc:  0.778\n",
      "min loss epoch:37\n",
      "epoch:  38 train_loss:  0.362 train_acc:  0.815 test_loss:  0.429 test_acc:  0.776\n",
      "min loss epoch:38\n",
      "epoch:  39 train_loss:  0.361 train_acc:  0.817 test_loss:  0.428 test_acc:  0.779\n",
      "min loss epoch:39\n",
      "epoch:  40 train_loss:  0.359 train_acc:  0.817 test_loss:  0.428 test_acc:  0.778\n",
      "min loss epoch:40\n",
      "epoch:  41 train_loss:  0.358 train_acc:  0.817 test_loss:  0.427 test_acc:  0.781\n",
      "min loss epoch:41\n",
      "epoch:  42 train_loss:  0.357 train_acc:  0.818 test_loss:  0.426 test_acc:  0.781\n",
      "min loss epoch:42\n",
      "epoch:  43 train_loss:  0.357 train_acc:  0.817 test_loss:  0.426 test_acc:  0.783\n",
      "min loss epoch:43\n",
      "max acc epoch：43        max acc：0.783\n",
      "epoch:  44 train_loss:  0.355 train_acc:  0.821 test_loss:  0.424 test_acc:  0.784\n",
      "min loss epoch:44\n",
      "max acc epoch：44        max acc：0.784\n",
      "epoch:  45 train_loss:  0.354 train_acc:  0.822 test_loss:  0.423 test_acc:  0.782\n",
      "min loss epoch:45\n",
      "epoch:  46 train_loss:  0.354 train_acc:  0.821 test_loss:  0.423 test_acc:  0.787\n",
      "min loss epoch:46\n",
      "max acc epoch：46        max acc：0.787\n",
      "epoch:  47 train_loss:  0.352 train_acc:  0.821 test_loss:  0.422 test_acc:  0.785\n",
      "min loss epoch:47\n",
      "epoch:  48 train_loss:  0.351 train_acc:  0.824 test_loss:  0.42 test_acc:  0.785\n",
      "min loss epoch:48\n",
      "epoch:  49 train_loss:  0.35 train_acc:  0.825 test_loss:  0.418 test_acc:  0.787\n",
      "min loss epoch:49\n",
      "epoch:  50 train_loss:  0.348 train_acc:  0.824 test_loss:  0.417 test_acc:  0.787\n",
      "min loss epoch:50\n",
      "epoch:  51 train_loss:  0.348 train_acc:  0.826 test_loss:  0.417 test_acc:  0.787\n",
      "min loss epoch:51\n",
      "epoch:  52 train_loss:  0.347 train_acc:  0.828 test_loss:  0.415 test_acc:  0.786\n",
      "min loss epoch:52\n",
      "epoch:  53 train_loss:  0.347 train_acc:  0.828 test_loss:  0.415 test_acc:  0.787\n",
      "min loss epoch:53\n",
      "epoch:  54 train_loss:  0.346 train_acc:  0.828 test_loss:  0.415 test_acc:  0.788\n",
      "min loss epoch:54\n",
      "max acc epoch：54        max acc：0.788\n",
      "epoch:  55 train_loss:  0.345 train_acc:  0.831 test_loss:  0.414 test_acc:  0.788\n",
      "min loss epoch:55\n",
      "epoch:  56 train_loss:  0.344 train_acc:  0.831 test_loss:  0.413 test_acc:  0.792\n",
      "min loss epoch:56\n",
      "max acc epoch：56        max acc：0.792\n",
      "epoch:  57 train_loss:  0.343 train_acc:  0.831 test_loss:  0.413 test_acc:  0.788\n",
      "min loss epoch:57\n",
      "epoch:  58 train_loss:  0.343 train_acc:  0.832 test_loss:  0.412 test_acc:  0.789\n",
      "min loss epoch:58\n",
      "epoch:  59 train_loss:  0.343 train_acc:  0.834 test_loss:  0.412 test_acc:  0.79\n",
      "epoch:  60 train_loss:  0.342 train_acc:  0.834 test_loss:  0.412 test_acc:  0.792\n",
      "epoch:  61 train_loss:  0.341 train_acc:  0.833 test_loss:  0.414 test_acc:  0.79\n",
      "epoch:  62 train_loss:  0.341 train_acc:  0.836 test_loss:  0.412 test_acc:  0.788\n",
      "min loss epoch:62\n",
      "epoch:  63 train_loss:  0.34 train_acc:  0.835 test_loss:  0.413 test_acc:  0.79\n",
      "epoch:  64 train_loss:  0.339 train_acc:  0.836 test_loss:  0.413 test_acc:  0.792\n",
      "epoch:  65 train_loss:  0.338 train_acc:  0.835 test_loss:  0.413 test_acc:  0.792\n",
      "epoch:  66 train_loss:  0.337 train_acc:  0.836 test_loss:  0.413 test_acc:  0.792\n",
      "epoch:  67 train_loss:  0.336 train_acc:  0.838 test_loss:  0.413 test_acc:  0.792\n",
      "epoch:  68 train_loss:  0.336 train_acc:  0.838 test_loss:  0.414 test_acc:  0.792\n",
      "epoch:  69 train_loss:  0.335 train_acc:  0.839 test_loss:  0.415 test_acc:  0.793\n",
      "max acc epoch：69        max acc：0.793\n",
      "epoch:  70 train_loss:  0.334 train_acc:  0.837 test_loss:  0.416 test_acc:  0.79\n",
      "epoch:  71 train_loss:  0.333 train_acc:  0.839 test_loss:  0.416 test_acc:  0.791\n",
      "epoch:  72 train_loss:  0.332 train_acc:  0.839 test_loss:  0.416 test_acc:  0.793\n",
      "epoch:  73 train_loss:  0.332 train_acc:  0.838 test_loss:  0.415 test_acc:  0.792\n",
      "epoch:  74 train_loss:  0.331 train_acc:  0.839 test_loss:  0.415 test_acc:  0.791\n",
      "epoch:  75 train_loss:  0.33 train_acc:  0.841 test_loss:  0.414 test_acc:  0.796\n",
      "max acc epoch：75        max acc：0.796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  76 train_loss:  0.33 train_acc:  0.84 test_loss:  0.416 test_acc:  0.793\n",
      "epoch:  77 train_loss:  0.329 train_acc:  0.84 test_loss:  0.414 test_acc:  0.796\n",
      "epoch:  78 train_loss:  0.328 train_acc:  0.842 test_loss:  0.415 test_acc:  0.795\n",
      "epoch:  79 train_loss:  0.328 train_acc:  0.842 test_loss:  0.415 test_acc:  0.793\n",
      "epoch:  80 train_loss:  0.327 train_acc:  0.842 test_loss:  0.414 test_acc:  0.797\n",
      "max acc epoch：80        max acc：0.797\n",
      "epoch:  81 train_loss:  0.326 train_acc:  0.842 test_loss:  0.413 test_acc:  0.799\n",
      "max acc epoch：81        max acc：0.799\n",
      "epoch:  82 train_loss:  0.326 train_acc:  0.843 test_loss:  0.413 test_acc:  0.803\n",
      "max acc epoch：82        max acc：0.803\n",
      "epoch:  83 train_loss:  0.325 train_acc:  0.843 test_loss:  0.413 test_acc:  0.802\n",
      "epoch:  84 train_loss:  0.324 train_acc:  0.843 test_loss:  0.413 test_acc:  0.801\n",
      "epoch:  85 train_loss:  0.324 train_acc:  0.843 test_loss:  0.412 test_acc:  0.801\n",
      "epoch:  86 train_loss:  0.323 train_acc:  0.845 test_loss:  0.413 test_acc:  0.802\n",
      "epoch:  87 train_loss:  0.323 train_acc:  0.845 test_loss:  0.413 test_acc:  0.801\n",
      "epoch:  88 train_loss:  0.322 train_acc:  0.845 test_loss:  0.412 test_acc:  0.804\n",
      "max acc epoch：88        max acc：0.804\n",
      "epoch:  89 train_loss:  0.322 train_acc:  0.845 test_loss:  0.412 test_acc:  0.804\n",
      "epoch:  90 train_loss:  0.321 train_acc:  0.846 test_loss:  0.412 test_acc:  0.803\n",
      "epoch:  91 train_loss:  0.32 train_acc:  0.847 test_loss:  0.412 test_acc:  0.806\n",
      "min loss epoch:91\n",
      "max acc epoch：91        max acc：0.806\n",
      "epoch:  92 train_loss:  0.32 train_acc:  0.847 test_loss:  0.413 test_acc:  0.802\n",
      "epoch:  93 train_loss:  0.319 train_acc:  0.847 test_loss:  0.413 test_acc:  0.803\n",
      "epoch:  94 train_loss:  0.319 train_acc:  0.846 test_loss:  0.413 test_acc:  0.803\n",
      "epoch:  95 train_loss:  0.318 train_acc:  0.847 test_loss:  0.413 test_acc:  0.803\n",
      "epoch:  96 train_loss:  0.319 train_acc:  0.846 test_loss:  0.414 test_acc:  0.8\n",
      "epoch:  97 train_loss:  0.318 train_acc:  0.846 test_loss:  0.414 test_acc:  0.799\n",
      "epoch:  98 train_loss:  0.317 train_acc:  0.848 test_loss:  0.414 test_acc:  0.801\n",
      "epoch:  99 train_loss:  0.317 train_acc:  0.848 test_loss:  0.414 test_acc:  0.802\n",
      "End max acc epoch：99        max acc：0.806\n",
      "['keypoint', 'brightness', 'contrast', 'edgeLength', 'Hue2', 'Hue3', 'Hue4', 'Hue5', 'Hue6', 'Hue7', 'Saturation', 'Value']\n",
      "6697\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([1340, 12])\n",
      "epoch:  0 train_loss:  0.466 train_acc:  0.759 test_loss:  0.485 test_acc:  0.749\n",
      "min loss epoch:0\n",
      "max acc epoch：0        max acc：0.749\n",
      "epoch:  1 train_loss:  0.454 train_acc:  0.758 test_loss:  0.477 test_acc:  0.744\n",
      "min loss epoch:1\n",
      "epoch:  2 train_loss:  0.446 train_acc:  0.76 test_loss:  0.472 test_acc:  0.746\n",
      "min loss epoch:2\n",
      "epoch:  3 train_loss:  0.44 train_acc:  0.763 test_loss:  0.467 test_acc:  0.743\n",
      "min loss epoch:3\n",
      "epoch:  4 train_loss:  0.434 train_acc:  0.77 test_loss:  0.463 test_acc:  0.747\n",
      "min loss epoch:4\n",
      "epoch:  5 train_loss:  0.429 train_acc:  0.773 test_loss:  0.459 test_acc:  0.748\n",
      "min loss epoch:5\n",
      "epoch:  6 train_loss:  0.423 train_acc:  0.775 test_loss:  0.455 test_acc:  0.751\n",
      "min loss epoch:6\n",
      "max acc epoch：6        max acc：0.751\n",
      "epoch:  7 train_loss:  0.418 train_acc:  0.779 test_loss:  0.452 test_acc:  0.755\n",
      "min loss epoch:7\n",
      "max acc epoch：7        max acc：0.755\n",
      "epoch:  8 train_loss:  0.413 train_acc:  0.781 test_loss:  0.449 test_acc:  0.761\n",
      "min loss epoch:8\n",
      "max acc epoch：8        max acc：0.761\n",
      "epoch:  9 train_loss:  0.409 train_acc:  0.784 test_loss:  0.447 test_acc:  0.757\n",
      "min loss epoch:9\n",
      "epoch:  10 train_loss:  0.405 train_acc:  0.784 test_loss:  0.445 test_acc:  0.763\n",
      "min loss epoch:10\n",
      "max acc epoch：10        max acc：0.763\n",
      "epoch:  11 train_loss:  0.401 train_acc:  0.788 test_loss:  0.444 test_acc:  0.76\n",
      "min loss epoch:11\n",
      "epoch:  12 train_loss:  0.398 train_acc:  0.791 test_loss:  0.443 test_acc:  0.761\n",
      "min loss epoch:12\n",
      "epoch:  13 train_loss:  0.394 train_acc:  0.793 test_loss:  0.442 test_acc:  0.763\n",
      "min loss epoch:13\n",
      "epoch:  14 train_loss:  0.392 train_acc:  0.795 test_loss:  0.441 test_acc:  0.763\n",
      "min loss epoch:14\n",
      "epoch:  15 train_loss:  0.389 train_acc:  0.799 test_loss:  0.44 test_acc:  0.767\n",
      "min loss epoch:15\n",
      "max acc epoch：15        max acc：0.767\n",
      "epoch:  16 train_loss:  0.387 train_acc:  0.801 test_loss:  0.44 test_acc:  0.766\n",
      "min loss epoch:16\n",
      "epoch:  17 train_loss:  0.385 train_acc:  0.803 test_loss:  0.438 test_acc:  0.767\n",
      "min loss epoch:17\n",
      "epoch:  18 train_loss:  0.382 train_acc:  0.805 test_loss:  0.437 test_acc:  0.77\n",
      "min loss epoch:18\n",
      "max acc epoch：18        max acc：0.77\n",
      "epoch:  19 train_loss:  0.38 train_acc:  0.806 test_loss:  0.436 test_acc:  0.77\n",
      "min loss epoch:19\n",
      "epoch:  20 train_loss:  0.378 train_acc:  0.807 test_loss:  0.434 test_acc:  0.772\n",
      "min loss epoch:20\n",
      "max acc epoch：20        max acc：0.772\n",
      "epoch:  21 train_loss:  0.376 train_acc:  0.809 test_loss:  0.433 test_acc:  0.772\n",
      "min loss epoch:21\n",
      "epoch:  22 train_loss:  0.374 train_acc:  0.81 test_loss:  0.432 test_acc:  0.77\n",
      "min loss epoch:22\n",
      "epoch:  23 train_loss:  0.373 train_acc:  0.811 test_loss:  0.431 test_acc:  0.771\n",
      "min loss epoch:23\n",
      "epoch:  24 train_loss:  0.371 train_acc:  0.812 test_loss:  0.43 test_acc:  0.77\n",
      "min loss epoch:24\n",
      "epoch:  25 train_loss:  0.37 train_acc:  0.813 test_loss:  0.429 test_acc:  0.769\n",
      "min loss epoch:25\n",
      "epoch:  26 train_loss:  0.368 train_acc:  0.812 test_loss:  0.428 test_acc:  0.769\n",
      "min loss epoch:26\n",
      "epoch:  27 train_loss:  0.366 train_acc:  0.814 test_loss:  0.427 test_acc:  0.767\n",
      "min loss epoch:27\n",
      "epoch:  28 train_loss:  0.365 train_acc:  0.816 test_loss:  0.427 test_acc:  0.768\n",
      "min loss epoch:28\n",
      "epoch:  29 train_loss:  0.364 train_acc:  0.817 test_loss:  0.426 test_acc:  0.77\n",
      "min loss epoch:29\n",
      "epoch:  30 train_loss:  0.362 train_acc:  0.817 test_loss:  0.425 test_acc:  0.768\n",
      "min loss epoch:30\n",
      "epoch:  31 train_loss:  0.361 train_acc:  0.819 test_loss:  0.424 test_acc:  0.769\n",
      "min loss epoch:31\n",
      "epoch:  32 train_loss:  0.359 train_acc:  0.82 test_loss:  0.423 test_acc:  0.769\n",
      "min loss epoch:32\n",
      "epoch:  33 train_loss:  0.358 train_acc:  0.82 test_loss:  0.422 test_acc:  0.769\n",
      "min loss epoch:33\n",
      "epoch:  34 train_loss:  0.357 train_acc:  0.82 test_loss:  0.421 test_acc:  0.77\n",
      "min loss epoch:34\n",
      "epoch:  35 train_loss:  0.356 train_acc:  0.82 test_loss:  0.42 test_acc:  0.769\n",
      "min loss epoch:35\n",
      "epoch:  36 train_loss:  0.355 train_acc:  0.822 test_loss:  0.42 test_acc:  0.773\n",
      "min loss epoch:36\n",
      "max acc epoch：36        max acc：0.773\n",
      "epoch:  37 train_loss:  0.353 train_acc:  0.824 test_loss:  0.419 test_acc:  0.775\n",
      "min loss epoch:37\n",
      "max acc epoch：37        max acc：0.775\n",
      "epoch:  38 train_loss:  0.353 train_acc:  0.822 test_loss:  0.418 test_acc:  0.774\n",
      "min loss epoch:38\n",
      "epoch:  39 train_loss:  0.351 train_acc:  0.822 test_loss:  0.417 test_acc:  0.775\n",
      "min loss epoch:39\n",
      "epoch:  40 train_loss:  0.35 train_acc:  0.822 test_loss:  0.416 test_acc:  0.779\n",
      "min loss epoch:40\n",
      "max acc epoch：40        max acc：0.779\n",
      "epoch:  41 train_loss:  0.349 train_acc:  0.823 test_loss:  0.415 test_acc:  0.782\n",
      "min loss epoch:41\n",
      "max acc epoch：41        max acc：0.782\n",
      "epoch:  42 train_loss:  0.348 train_acc:  0.823 test_loss:  0.413 test_acc:  0.779\n",
      "min loss epoch:42\n",
      "epoch:  43 train_loss:  0.347 train_acc:  0.825 test_loss:  0.413 test_acc:  0.781\n",
      "min loss epoch:43\n",
      "epoch:  44 train_loss:  0.346 train_acc:  0.826 test_loss:  0.412 test_acc:  0.784\n",
      "min loss epoch:44\n",
      "max acc epoch：44        max acc：0.784\n",
      "epoch:  45 train_loss:  0.345 train_acc:  0.828 test_loss:  0.412 test_acc:  0.783\n",
      "min loss epoch:45\n",
      "epoch:  46 train_loss:  0.344 train_acc:  0.828 test_loss:  0.41 test_acc:  0.787\n",
      "min loss epoch:46\n",
      "max acc epoch：46        max acc：0.787\n",
      "epoch:  47 train_loss:  0.343 train_acc:  0.83 test_loss:  0.41 test_acc:  0.782\n",
      "min loss epoch:47\n",
      "epoch:  48 train_loss:  0.342 train_acc:  0.829 test_loss:  0.409 test_acc:  0.789\n",
      "min loss epoch:48\n",
      "max acc epoch：48        max acc：0.789\n",
      "epoch:  49 train_loss:  0.341 train_acc:  0.829 test_loss:  0.408 test_acc:  0.79\n",
      "min loss epoch:49\n",
      "max acc epoch：49        max acc：0.79\n",
      "epoch:  50 train_loss:  0.34 train_acc:  0.831 test_loss:  0.406 test_acc:  0.79\n",
      "min loss epoch:50\n",
      "epoch:  51 train_loss:  0.338 train_acc:  0.833 test_loss:  0.405 test_acc:  0.789\n",
      "min loss epoch:51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  52 train_loss:  0.338 train_acc:  0.833 test_loss:  0.404 test_acc:  0.787\n",
      "min loss epoch:52\n",
      "epoch:  53 train_loss:  0.336 train_acc:  0.838 test_loss:  0.403 test_acc:  0.789\n",
      "min loss epoch:53\n",
      "epoch:  54 train_loss:  0.335 train_acc:  0.837 test_loss:  0.403 test_acc:  0.793\n",
      "max acc epoch：54        max acc：0.793\n",
      "epoch:  55 train_loss:  0.334 train_acc:  0.837 test_loss:  0.403 test_acc:  0.793\n",
      "min loss epoch:55\n",
      "epoch:  56 train_loss:  0.333 train_acc:  0.839 test_loss:  0.402 test_acc:  0.793\n",
      "min loss epoch:56\n",
      "epoch:  57 train_loss:  0.333 train_acc:  0.839 test_loss:  0.402 test_acc:  0.792\n",
      "min loss epoch:57\n",
      "epoch:  58 train_loss:  0.331 train_acc:  0.84 test_loss:  0.4 test_acc:  0.797\n",
      "min loss epoch:58\n",
      "max acc epoch：58        max acc：0.797\n",
      "epoch:  59 train_loss:  0.33 train_acc:  0.84 test_loss:  0.4 test_acc:  0.793\n",
      "epoch:  60 train_loss:  0.329 train_acc:  0.84 test_loss:  0.4 test_acc:  0.796\n",
      "min loss epoch:60\n",
      "epoch:  61 train_loss:  0.328 train_acc:  0.84 test_loss:  0.398 test_acc:  0.799\n",
      "min loss epoch:61\n",
      "max acc epoch：61        max acc：0.799\n",
      "epoch:  62 train_loss:  0.327 train_acc:  0.841 test_loss:  0.398 test_acc:  0.801\n",
      "max acc epoch：62        max acc：0.801\n",
      "epoch:  63 train_loss:  0.326 train_acc:  0.841 test_loss:  0.397 test_acc:  0.796\n",
      "min loss epoch:63\n",
      "epoch:  64 train_loss:  0.325 train_acc:  0.842 test_loss:  0.396 test_acc:  0.799\n",
      "min loss epoch:64\n",
      "epoch:  65 train_loss:  0.323 train_acc:  0.843 test_loss:  0.393 test_acc:  0.801\n",
      "min loss epoch:65\n",
      "epoch:  66 train_loss:  0.322 train_acc:  0.841 test_loss:  0.395 test_acc:  0.799\n",
      "epoch:  67 train_loss:  0.32 train_acc:  0.844 test_loss:  0.394 test_acc:  0.799\n",
      "epoch:  68 train_loss:  0.318 train_acc:  0.845 test_loss:  0.392 test_acc:  0.802\n",
      "min loss epoch:68\n",
      "max acc epoch：68        max acc：0.802\n",
      "epoch:  69 train_loss:  0.317 train_acc:  0.843 test_loss:  0.391 test_acc:  0.804\n",
      "min loss epoch:69\n",
      "max acc epoch：69        max acc：0.804\n",
      "epoch:  70 train_loss:  0.316 train_acc:  0.845 test_loss:  0.391 test_acc:  0.802\n",
      "min loss epoch:70\n",
      "epoch:  71 train_loss:  0.314 train_acc:  0.845 test_loss:  0.389 test_acc:  0.801\n",
      "min loss epoch:71\n",
      "epoch:  72 train_loss:  0.313 train_acc:  0.846 test_loss:  0.389 test_acc:  0.802\n",
      "epoch:  73 train_loss:  0.312 train_acc:  0.847 test_loss:  0.387 test_acc:  0.801\n",
      "min loss epoch:73\n",
      "epoch:  74 train_loss:  0.311 train_acc:  0.848 test_loss:  0.387 test_acc:  0.806\n",
      "min loss epoch:74\n",
      "max acc epoch：74        max acc：0.806\n",
      "epoch:  75 train_loss:  0.309 train_acc:  0.849 test_loss:  0.385 test_acc:  0.807\n",
      "min loss epoch:75\n",
      "max acc epoch：75        max acc：0.807\n",
      "epoch:  76 train_loss:  0.308 train_acc:  0.851 test_loss:  0.385 test_acc:  0.806\n",
      "min loss epoch:76\n",
      "epoch:  77 train_loss:  0.306 train_acc:  0.851 test_loss:  0.383 test_acc:  0.807\n",
      "min loss epoch:77\n",
      "epoch:  78 train_loss:  0.305 train_acc:  0.851 test_loss:  0.383 test_acc:  0.807\n",
      "epoch:  79 train_loss:  0.304 train_acc:  0.851 test_loss:  0.384 test_acc:  0.806\n",
      "epoch:  80 train_loss:  0.302 train_acc:  0.852 test_loss:  0.38 test_acc:  0.809\n",
      "min loss epoch:80\n",
      "max acc epoch：80        max acc：0.809\n",
      "epoch:  81 train_loss:  0.302 train_acc:  0.851 test_loss:  0.382 test_acc:  0.808\n",
      "epoch:  82 train_loss:  0.302 train_acc:  0.852 test_loss:  0.382 test_acc:  0.807\n",
      "epoch:  83 train_loss:  0.299 train_acc:  0.852 test_loss:  0.379 test_acc:  0.81\n",
      "min loss epoch:83\n",
      "max acc epoch：83        max acc：0.81\n",
      "epoch:  84 train_loss:  0.3 train_acc:  0.853 test_loss:  0.382 test_acc:  0.814\n",
      "max acc epoch：84        max acc：0.814\n",
      "epoch:  85 train_loss:  0.298 train_acc:  0.854 test_loss:  0.381 test_acc:  0.813\n",
      "epoch:  86 train_loss:  0.298 train_acc:  0.854 test_loss:  0.382 test_acc:  0.811\n",
      "epoch:  87 train_loss:  0.296 train_acc:  0.856 test_loss:  0.38 test_acc:  0.816\n",
      "max acc epoch：87        max acc：0.816\n",
      "epoch:  88 train_loss:  0.295 train_acc:  0.856 test_loss:  0.379 test_acc:  0.816\n",
      "min loss epoch:88\n",
      "epoch:  89 train_loss:  0.292 train_acc:  0.857 test_loss:  0.375 test_acc:  0.819\n",
      "min loss epoch:89\n",
      "max acc epoch：89        max acc：0.819\n",
      "epoch:  90 train_loss:  0.292 train_acc:  0.857 test_loss:  0.375 test_acc:  0.816\n",
      "min loss epoch:90\n",
      "epoch:  91 train_loss:  0.291 train_acc:  0.859 test_loss:  0.372 test_acc:  0.819\n",
      "min loss epoch:91\n",
      "epoch:  92 train_loss:  0.291 train_acc:  0.859 test_loss:  0.373 test_acc:  0.816\n",
      "epoch:  93 train_loss:  0.29 train_acc:  0.858 test_loss:  0.373 test_acc:  0.817\n",
      "epoch:  94 train_loss:  0.292 train_acc:  0.858 test_loss:  0.377 test_acc:  0.815\n",
      "epoch:  95 train_loss:  0.29 train_acc:  0.859 test_loss:  0.373 test_acc:  0.818\n",
      "epoch:  96 train_loss:  0.289 train_acc:  0.859 test_loss:  0.372 test_acc:  0.819\n",
      "min loss epoch:96\n",
      "epoch:  97 train_loss:  0.29 train_acc:  0.86 test_loss:  0.375 test_acc:  0.816\n",
      "epoch:  98 train_loss:  0.289 train_acc:  0.861 test_loss:  0.373 test_acc:  0.816\n",
      "epoch:  99 train_loss:  0.287 train_acc:  0.861 test_loss:  0.371 test_acc:  0.818\n",
      "min loss epoch:99\n",
      "End max acc epoch：99        max acc：0.819\n",
      "['keypoint', 'brightness', 'contrast', 'edgeLength', 'Hue1', 'Hue3', 'Hue4', 'Hue5', 'Hue6', 'Hue7', 'Saturation', 'Value']\n",
      "6697\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([1340, 12])\n",
      "epoch:  0 train_loss:  0.468 train_acc:  0.754 test_loss:  0.488 test_acc:  0.745\n",
      "min loss epoch:0\n",
      "max acc epoch：0        max acc：0.745\n",
      "epoch:  1 train_loss:  0.454 train_acc:  0.757 test_loss:  0.478 test_acc:  0.744\n",
      "min loss epoch:1\n",
      "epoch:  2 train_loss:  0.444 train_acc:  0.763 test_loss:  0.472 test_acc:  0.749\n",
      "min loss epoch:2\n",
      "max acc epoch：2        max acc：0.749\n",
      "epoch:  3 train_loss:  0.437 train_acc:  0.766 test_loss:  0.467 test_acc:  0.752\n",
      "min loss epoch:3\n",
      "max acc epoch：3        max acc：0.752\n",
      "epoch:  4 train_loss:  0.43 train_acc:  0.774 test_loss:  0.462 test_acc:  0.755\n",
      "min loss epoch:4\n",
      "max acc epoch：4        max acc：0.755\n",
      "epoch:  5 train_loss:  0.423 train_acc:  0.778 test_loss:  0.457 test_acc:  0.758\n",
      "min loss epoch:5\n",
      "max acc epoch：5        max acc：0.758\n",
      "epoch:  6 train_loss:  0.418 train_acc:  0.781 test_loss:  0.453 test_acc:  0.756\n",
      "min loss epoch:6\n",
      "epoch:  7 train_loss:  0.412 train_acc:  0.784 test_loss:  0.45 test_acc:  0.758\n",
      "min loss epoch:7\n",
      "epoch:  8 train_loss:  0.407 train_acc:  0.789 test_loss:  0.446 test_acc:  0.76\n",
      "min loss epoch:8\n",
      "max acc epoch：8        max acc：0.76\n",
      "epoch:  9 train_loss:  0.403 train_acc:  0.794 test_loss:  0.443 test_acc:  0.767\n",
      "min loss epoch:9\n",
      "max acc epoch：9        max acc：0.767\n",
      "epoch:  10 train_loss:  0.398 train_acc:  0.797 test_loss:  0.441 test_acc:  0.773\n",
      "min loss epoch:10\n",
      "max acc epoch：10        max acc：0.773\n",
      "epoch:  11 train_loss:  0.394 train_acc:  0.801 test_loss:  0.438 test_acc:  0.773\n",
      "min loss epoch:11\n",
      "epoch:  12 train_loss:  0.39 train_acc:  0.806 test_loss:  0.436 test_acc:  0.776\n",
      "min loss epoch:12\n",
      "max acc epoch：12        max acc：0.776\n",
      "epoch:  13 train_loss:  0.387 train_acc:  0.805 test_loss:  0.434 test_acc:  0.78\n",
      "min loss epoch:13\n",
      "max acc epoch：13        max acc：0.78\n",
      "epoch:  14 train_loss:  0.383 train_acc:  0.808 test_loss:  0.431 test_acc:  0.781\n",
      "min loss epoch:14\n",
      "max acc epoch：14        max acc：0.781\n",
      "epoch:  15 train_loss:  0.38 train_acc:  0.809 test_loss:  0.428 test_acc:  0.784\n",
      "min loss epoch:15\n",
      "max acc epoch：15        max acc：0.784\n",
      "epoch:  16 train_loss:  0.376 train_acc:  0.811 test_loss:  0.426 test_acc:  0.783\n",
      "min loss epoch:16\n",
      "epoch:  17 train_loss:  0.373 train_acc:  0.812 test_loss:  0.423 test_acc:  0.786\n",
      "min loss epoch:17\n",
      "max acc epoch：17        max acc：0.786\n",
      "epoch:  18 train_loss:  0.37 train_acc:  0.815 test_loss:  0.422 test_acc:  0.79\n",
      "min loss epoch:18\n",
      "max acc epoch：18        max acc：0.79\n",
      "epoch:  19 train_loss:  0.367 train_acc:  0.815 test_loss:  0.42 test_acc:  0.789\n",
      "min loss epoch:19\n",
      "epoch:  20 train_loss:  0.365 train_acc:  0.818 test_loss:  0.419 test_acc:  0.79\n",
      "min loss epoch:20\n",
      "epoch:  21 train_loss:  0.362 train_acc:  0.819 test_loss:  0.417 test_acc:  0.79\n",
      "min loss epoch:21\n",
      "epoch:  22 train_loss:  0.36 train_acc:  0.82 test_loss:  0.416 test_acc:  0.792\n",
      "min loss epoch:22\n",
      "max acc epoch：22        max acc：0.792\n",
      "epoch:  23 train_loss:  0.358 train_acc:  0.821 test_loss:  0.414 test_acc:  0.792\n",
      "min loss epoch:23\n",
      "epoch:  24 train_loss:  0.355 train_acc:  0.823 test_loss:  0.412 test_acc:  0.795\n",
      "min loss epoch:24\n",
      "max acc epoch：24        max acc：0.795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  25 train_loss:  0.353 train_acc:  0.826 test_loss:  0.41 test_acc:  0.794\n",
      "min loss epoch:25\n",
      "epoch:  26 train_loss:  0.351 train_acc:  0.827 test_loss:  0.409 test_acc:  0.792\n",
      "min loss epoch:26\n",
      "epoch:  27 train_loss:  0.348 train_acc:  0.829 test_loss:  0.408 test_acc:  0.79\n",
      "min loss epoch:27\n",
      "epoch:  28 train_loss:  0.347 train_acc:  0.828 test_loss:  0.407 test_acc:  0.795\n",
      "min loss epoch:28\n",
      "epoch:  29 train_loss:  0.344 train_acc:  0.831 test_loss:  0.404 test_acc:  0.795\n",
      "min loss epoch:29\n",
      "epoch:  30 train_loss:  0.342 train_acc:  0.83 test_loss:  0.403 test_acc:  0.796\n",
      "min loss epoch:30\n",
      "max acc epoch：30        max acc：0.796\n",
      "epoch:  31 train_loss:  0.34 train_acc:  0.831 test_loss:  0.402 test_acc:  0.797\n",
      "min loss epoch:31\n",
      "max acc epoch：31        max acc：0.797\n",
      "epoch:  32 train_loss:  0.339 train_acc:  0.832 test_loss:  0.402 test_acc:  0.796\n",
      "epoch:  33 train_loss:  0.338 train_acc:  0.83 test_loss:  0.401 test_acc:  0.794\n",
      "min loss epoch:33\n",
      "epoch:  34 train_loss:  0.335 train_acc:  0.832 test_loss:  0.399 test_acc:  0.793\n",
      "min loss epoch:34\n",
      "epoch:  35 train_loss:  0.335 train_acc:  0.834 test_loss:  0.399 test_acc:  0.794\n",
      "epoch:  36 train_loss:  0.334 train_acc:  0.834 test_loss:  0.398 test_acc:  0.797\n",
      "min loss epoch:36\n",
      "epoch:  37 train_loss:  0.33 train_acc:  0.836 test_loss:  0.396 test_acc:  0.798\n",
      "min loss epoch:37\n",
      "max acc epoch：37        max acc：0.798\n",
      "epoch:  38 train_loss:  0.329 train_acc:  0.835 test_loss:  0.396 test_acc:  0.796\n",
      "min loss epoch:38\n",
      "epoch:  39 train_loss:  0.328 train_acc:  0.837 test_loss:  0.394 test_acc:  0.795\n",
      "min loss epoch:39\n",
      "epoch:  40 train_loss:  0.328 train_acc:  0.837 test_loss:  0.395 test_acc:  0.798\n",
      "epoch:  41 train_loss:  0.326 train_acc:  0.84 test_loss:  0.393 test_acc:  0.796\n",
      "min loss epoch:41\n",
      "epoch:  42 train_loss:  0.324 train_acc:  0.84 test_loss:  0.392 test_acc:  0.795\n",
      "min loss epoch:42\n",
      "epoch:  43 train_loss:  0.322 train_acc:  0.842 test_loss:  0.391 test_acc:  0.796\n",
      "min loss epoch:43\n",
      "epoch:  44 train_loss:  0.322 train_acc:  0.842 test_loss:  0.392 test_acc:  0.795\n",
      "epoch:  45 train_loss:  0.32 train_acc:  0.843 test_loss:  0.39 test_acc:  0.797\n",
      "min loss epoch:45\n",
      "epoch:  46 train_loss:  0.32 train_acc:  0.844 test_loss:  0.391 test_acc:  0.797\n",
      "epoch:  47 train_loss:  0.317 train_acc:  0.845 test_loss:  0.388 test_acc:  0.797\n",
      "min loss epoch:47\n",
      "epoch:  48 train_loss:  0.318 train_acc:  0.844 test_loss:  0.39 test_acc:  0.8\n",
      "max acc epoch：48        max acc：0.8\n",
      "epoch:  49 train_loss:  0.315 train_acc:  0.847 test_loss:  0.386 test_acc:  0.8\n",
      "min loss epoch:49\n",
      "epoch:  50 train_loss:  0.316 train_acc:  0.846 test_loss:  0.388 test_acc:  0.8\n",
      "epoch:  51 train_loss:  0.315 train_acc:  0.848 test_loss:  0.388 test_acc:  0.802\n",
      "max acc epoch：51        max acc：0.802\n",
      "epoch:  52 train_loss:  0.313 train_acc:  0.849 test_loss:  0.387 test_acc:  0.799\n",
      "epoch:  53 train_loss:  0.312 train_acc:  0.849 test_loss:  0.386 test_acc:  0.799\n",
      "epoch:  54 train_loss:  0.31 train_acc:  0.849 test_loss:  0.385 test_acc:  0.801\n",
      "min loss epoch:54\n",
      "epoch:  55 train_loss:  0.31 train_acc:  0.849 test_loss:  0.385 test_acc:  0.804\n",
      "min loss epoch:55\n",
      "max acc epoch：55        max acc：0.804\n",
      "epoch:  56 train_loss:  0.309 train_acc:  0.85 test_loss:  0.384 test_acc:  0.804\n",
      "min loss epoch:56\n",
      "epoch:  57 train_loss:  0.308 train_acc:  0.851 test_loss:  0.383 test_acc:  0.804\n",
      "min loss epoch:57\n",
      "epoch:  58 train_loss:  0.307 train_acc:  0.851 test_loss:  0.384 test_acc:  0.804\n",
      "epoch:  59 train_loss:  0.306 train_acc:  0.851 test_loss:  0.383 test_acc:  0.804\n",
      "min loss epoch:59\n",
      "epoch:  60 train_loss:  0.305 train_acc:  0.853 test_loss:  0.383 test_acc:  0.804\n",
      "min loss epoch:60\n",
      "epoch:  61 train_loss:  0.305 train_acc:  0.853 test_loss:  0.383 test_acc:  0.803\n",
      "epoch:  62 train_loss:  0.304 train_acc:  0.853 test_loss:  0.381 test_acc:  0.802\n",
      "min loss epoch:62\n",
      "epoch:  63 train_loss:  0.303 train_acc:  0.853 test_loss:  0.381 test_acc:  0.804\n",
      "min loss epoch:63\n",
      "epoch:  64 train_loss:  0.302 train_acc:  0.853 test_loss:  0.38 test_acc:  0.802\n",
      "min loss epoch:64\n",
      "epoch:  65 train_loss:  0.303 train_acc:  0.851 test_loss:  0.381 test_acc:  0.801\n",
      "epoch:  66 train_loss:  0.3 train_acc:  0.854 test_loss:  0.378 test_acc:  0.801\n",
      "min loss epoch:66\n",
      "epoch:  67 train_loss:  0.299 train_acc:  0.854 test_loss:  0.377 test_acc:  0.801\n",
      "min loss epoch:67\n",
      "epoch:  68 train_loss:  0.298 train_acc:  0.855 test_loss:  0.377 test_acc:  0.804\n",
      "epoch:  69 train_loss:  0.296 train_acc:  0.856 test_loss:  0.377 test_acc:  0.803\n",
      "min loss epoch:69\n",
      "epoch:  70 train_loss:  0.297 train_acc:  0.856 test_loss:  0.377 test_acc:  0.804\n",
      "epoch:  71 train_loss:  0.299 train_acc:  0.852 test_loss:  0.379 test_acc:  0.804\n",
      "epoch:  72 train_loss:  0.294 train_acc:  0.856 test_loss:  0.375 test_acc:  0.804\n",
      "min loss epoch:72\n",
      "epoch:  73 train_loss:  0.293 train_acc:  0.856 test_loss:  0.375 test_acc:  0.807\n",
      "min loss epoch:73\n",
      "max acc epoch：73        max acc：0.807\n",
      "epoch:  74 train_loss:  0.292 train_acc:  0.858 test_loss:  0.374 test_acc:  0.805\n",
      "min loss epoch:74\n",
      "epoch:  75 train_loss:  0.29 train_acc:  0.857 test_loss:  0.372 test_acc:  0.807\n",
      "min loss epoch:75\n",
      "epoch:  76 train_loss:  0.29 train_acc:  0.857 test_loss:  0.374 test_acc:  0.807\n",
      "epoch:  77 train_loss:  0.29 train_acc:  0.857 test_loss:  0.374 test_acc:  0.811\n",
      "max acc epoch：77        max acc：0.811\n",
      "epoch:  78 train_loss:  0.289 train_acc:  0.857 test_loss:  0.374 test_acc:  0.807\n",
      "epoch:  79 train_loss:  0.288 train_acc:  0.858 test_loss:  0.372 test_acc:  0.806\n",
      "min loss epoch:79\n",
      "epoch:  80 train_loss:  0.286 train_acc:  0.859 test_loss:  0.372 test_acc:  0.808\n",
      "min loss epoch:80\n",
      "epoch:  81 train_loss:  0.285 train_acc:  0.86 test_loss:  0.372 test_acc:  0.809\n",
      "min loss epoch:81\n",
      "epoch:  82 train_loss:  0.285 train_acc:  0.859 test_loss:  0.371 test_acc:  0.81\n",
      "min loss epoch:82\n",
      "epoch:  83 train_loss:  0.284 train_acc:  0.86 test_loss:  0.37 test_acc:  0.809\n",
      "min loss epoch:83\n",
      "epoch:  84 train_loss:  0.284 train_acc:  0.86 test_loss:  0.372 test_acc:  0.809\n",
      "epoch:  85 train_loss:  0.283 train_acc:  0.86 test_loss:  0.37 test_acc:  0.81\n",
      "min loss epoch:85\n",
      "epoch:  86 train_loss:  0.28 train_acc:  0.861 test_loss:  0.367 test_acc:  0.807\n",
      "min loss epoch:86\n",
      "epoch:  87 train_loss:  0.282 train_acc:  0.861 test_loss:  0.37 test_acc:  0.81\n",
      "epoch:  88 train_loss:  0.279 train_acc:  0.861 test_loss:  0.366 test_acc:  0.81\n",
      "min loss epoch:88\n",
      "epoch:  89 train_loss:  0.282 train_acc:  0.859 test_loss:  0.37 test_acc:  0.808\n",
      "epoch:  90 train_loss:  0.279 train_acc:  0.86 test_loss:  0.366 test_acc:  0.81\n",
      "min loss epoch:90\n",
      "epoch:  91 train_loss:  0.279 train_acc:  0.859 test_loss:  0.366 test_acc:  0.81\n",
      "epoch:  92 train_loss:  0.276 train_acc:  0.862 test_loss:  0.362 test_acc:  0.813\n",
      "min loss epoch:92\n",
      "max acc epoch：92        max acc：0.813\n",
      "epoch:  93 train_loss:  0.277 train_acc:  0.862 test_loss:  0.363 test_acc:  0.811\n",
      "epoch:  94 train_loss:  0.278 train_acc:  0.861 test_loss:  0.366 test_acc:  0.81\n",
      "epoch:  95 train_loss:  0.274 train_acc:  0.864 test_loss:  0.361 test_acc:  0.814\n",
      "min loss epoch:95\n",
      "max acc epoch：95        max acc：0.814\n",
      "epoch:  96 train_loss:  0.275 train_acc:  0.863 test_loss:  0.362 test_acc:  0.817\n",
      "max acc epoch：96        max acc：0.817\n",
      "epoch:  97 train_loss:  0.276 train_acc:  0.861 test_loss:  0.364 test_acc:  0.816\n",
      "epoch:  98 train_loss:  0.276 train_acc:  0.864 test_loss:  0.364 test_acc:  0.815\n",
      "epoch:  99 train_loss:  0.276 train_acc:  0.865 test_loss:  0.364 test_acc:  0.813\n",
      "End max acc epoch：99        max acc：0.817\n",
      "['keypoint', 'brightness', 'contrast', 'edgeLength', 'Hue1', 'Hue2', 'Hue4', 'Hue5', 'Hue6', 'Hue7', 'Saturation', 'Value']\n",
      "6697\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([1340, 12])\n",
      "epoch:  0 train_loss:  0.461 train_acc:  0.758 test_loss:  0.483 test_acc:  0.751\n",
      "min loss epoch:0\n",
      "max acc epoch：0        max acc：0.751\n",
      "epoch:  1 train_loss:  0.445 train_acc:  0.765 test_loss:  0.473 test_acc:  0.748\n",
      "min loss epoch:1\n",
      "epoch:  2 train_loss:  0.436 train_acc:  0.772 test_loss:  0.467 test_acc:  0.747\n",
      "min loss epoch:2\n",
      "epoch:  3 train_loss:  0.428 train_acc:  0.776 test_loss:  0.463 test_acc:  0.753\n",
      "min loss epoch:3\n",
      "max acc epoch：3        max acc：0.753\n",
      "epoch:  4 train_loss:  0.421 train_acc:  0.783 test_loss:  0.459 test_acc:  0.755\n",
      "min loss epoch:4\n",
      "max acc epoch：4        max acc：0.755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  5 train_loss:  0.416 train_acc:  0.786 test_loss:  0.456 test_acc:  0.755\n",
      "min loss epoch:5\n",
      "epoch:  6 train_loss:  0.411 train_acc:  0.79 test_loss:  0.454 test_acc:  0.751\n",
      "min loss epoch:6\n",
      "epoch:  7 train_loss:  0.407 train_acc:  0.791 test_loss:  0.451 test_acc:  0.758\n",
      "min loss epoch:7\n",
      "max acc epoch：7        max acc：0.758\n",
      "epoch:  8 train_loss:  0.403 train_acc:  0.791 test_loss:  0.449 test_acc:  0.76\n",
      "min loss epoch:8\n",
      "max acc epoch：8        max acc：0.76\n",
      "epoch:  9 train_loss:  0.4 train_acc:  0.794 test_loss:  0.447 test_acc:  0.761\n",
      "min loss epoch:9\n",
      "max acc epoch：9        max acc：0.761\n",
      "epoch:  10 train_loss:  0.397 train_acc:  0.796 test_loss:  0.446 test_acc:  0.761\n",
      "min loss epoch:10\n",
      "epoch:  11 train_loss:  0.395 train_acc:  0.797 test_loss:  0.445 test_acc:  0.764\n",
      "min loss epoch:11\n",
      "max acc epoch：11        max acc：0.764\n",
      "epoch:  12 train_loss:  0.392 train_acc:  0.798 test_loss:  0.444 test_acc:  0.766\n",
      "min loss epoch:12\n",
      "max acc epoch：12        max acc：0.766\n",
      "epoch:  13 train_loss:  0.39 train_acc:  0.8 test_loss:  0.443 test_acc:  0.766\n",
      "min loss epoch:13\n",
      "epoch:  14 train_loss:  0.387 train_acc:  0.801 test_loss:  0.441 test_acc:  0.766\n",
      "min loss epoch:14\n",
      "epoch:  15 train_loss:  0.385 train_acc:  0.803 test_loss:  0.44 test_acc:  0.766\n",
      "min loss epoch:15\n",
      "epoch:  16 train_loss:  0.383 train_acc:  0.806 test_loss:  0.439 test_acc:  0.769\n",
      "min loss epoch:16\n",
      "max acc epoch：16        max acc：0.769\n",
      "epoch:  17 train_loss:  0.381 train_acc:  0.805 test_loss:  0.438 test_acc:  0.771\n",
      "min loss epoch:17\n",
      "max acc epoch：17        max acc：0.771\n",
      "epoch:  18 train_loss:  0.378 train_acc:  0.808 test_loss:  0.437 test_acc:  0.775\n",
      "min loss epoch:18\n",
      "max acc epoch：18        max acc：0.775\n",
      "epoch:  19 train_loss:  0.376 train_acc:  0.809 test_loss:  0.435 test_acc:  0.778\n",
      "min loss epoch:19\n",
      "max acc epoch：19        max acc：0.778\n",
      "epoch:  20 train_loss:  0.375 train_acc:  0.811 test_loss:  0.434 test_acc:  0.778\n",
      "min loss epoch:20\n",
      "epoch:  21 train_loss:  0.372 train_acc:  0.813 test_loss:  0.432 test_acc:  0.779\n",
      "min loss epoch:21\n",
      "max acc epoch：21        max acc：0.779\n",
      "epoch:  22 train_loss:  0.37 train_acc:  0.815 test_loss:  0.43 test_acc:  0.778\n",
      "min loss epoch:22\n",
      "epoch:  23 train_loss:  0.368 train_acc:  0.815 test_loss:  0.428 test_acc:  0.78\n",
      "min loss epoch:23\n",
      "max acc epoch：23        max acc：0.78\n",
      "epoch:  24 train_loss:  0.366 train_acc:  0.817 test_loss:  0.427 test_acc:  0.778\n",
      "min loss epoch:24\n",
      "epoch:  25 train_loss:  0.364 train_acc:  0.816 test_loss:  0.425 test_acc:  0.78\n",
      "min loss epoch:25\n",
      "epoch:  26 train_loss:  0.363 train_acc:  0.819 test_loss:  0.424 test_acc:  0.784\n",
      "min loss epoch:26\n",
      "max acc epoch：26        max acc：0.784\n",
      "epoch:  27 train_loss:  0.361 train_acc:  0.82 test_loss:  0.423 test_acc:  0.784\n",
      "min loss epoch:27\n",
      "epoch:  28 train_loss:  0.36 train_acc:  0.819 test_loss:  0.422 test_acc:  0.784\n",
      "min loss epoch:28\n",
      "epoch:  29 train_loss:  0.358 train_acc:  0.821 test_loss:  0.42 test_acc:  0.787\n",
      "min loss epoch:29\n",
      "max acc epoch：29        max acc：0.787\n",
      "epoch:  30 train_loss:  0.356 train_acc:  0.823 test_loss:  0.419 test_acc:  0.788\n",
      "min loss epoch:30\n",
      "max acc epoch：30        max acc：0.788\n",
      "epoch:  31 train_loss:  0.355 train_acc:  0.824 test_loss:  0.418 test_acc:  0.789\n",
      "min loss epoch:31\n",
      "max acc epoch：31        max acc：0.789\n",
      "epoch:  32 train_loss:  0.353 train_acc:  0.825 test_loss:  0.417 test_acc:  0.79\n",
      "min loss epoch:32\n",
      "max acc epoch：32        max acc：0.79\n",
      "epoch:  33 train_loss:  0.351 train_acc:  0.827 test_loss:  0.415 test_acc:  0.795\n",
      "min loss epoch:33\n",
      "max acc epoch：33        max acc：0.795\n",
      "epoch:  34 train_loss:  0.35 train_acc:  0.827 test_loss:  0.414 test_acc:  0.792\n",
      "min loss epoch:34\n",
      "epoch:  35 train_loss:  0.349 train_acc:  0.829 test_loss:  0.413 test_acc:  0.793\n",
      "min loss epoch:35\n",
      "epoch:  36 train_loss:  0.347 train_acc:  0.831 test_loss:  0.412 test_acc:  0.795\n",
      "min loss epoch:36\n",
      "epoch:  37 train_loss:  0.346 train_acc:  0.83 test_loss:  0.411 test_acc:  0.796\n",
      "min loss epoch:37\n",
      "max acc epoch：37        max acc：0.796\n",
      "epoch:  38 train_loss:  0.345 train_acc:  0.831 test_loss:  0.409 test_acc:  0.795\n",
      "min loss epoch:38\n",
      "epoch:  39 train_loss:  0.343 train_acc:  0.831 test_loss:  0.408 test_acc:  0.796\n",
      "min loss epoch:39\n",
      "epoch:  40 train_loss:  0.342 train_acc:  0.833 test_loss:  0.408 test_acc:  0.8\n",
      "min loss epoch:40\n",
      "max acc epoch：40        max acc：0.8\n",
      "epoch:  41 train_loss:  0.341 train_acc:  0.835 test_loss:  0.408 test_acc:  0.799\n",
      "min loss epoch:41\n",
      "epoch:  42 train_loss:  0.339 train_acc:  0.836 test_loss:  0.406 test_acc:  0.799\n",
      "min loss epoch:42\n",
      "epoch:  43 train_loss:  0.338 train_acc:  0.836 test_loss:  0.405 test_acc:  0.797\n",
      "min loss epoch:43\n",
      "epoch:  44 train_loss:  0.337 train_acc:  0.837 test_loss:  0.405 test_acc:  0.8\n",
      "min loss epoch:44\n",
      "epoch:  45 train_loss:  0.335 train_acc:  0.838 test_loss:  0.403 test_acc:  0.799\n",
      "min loss epoch:45\n",
      "epoch:  46 train_loss:  0.333 train_acc:  0.839 test_loss:  0.401 test_acc:  0.799\n",
      "min loss epoch:46\n",
      "epoch:  47 train_loss:  0.332 train_acc:  0.84 test_loss:  0.399 test_acc:  0.799\n",
      "min loss epoch:47\n",
      "epoch:  48 train_loss:  0.331 train_acc:  0.84 test_loss:  0.399 test_acc:  0.798\n",
      "min loss epoch:48\n",
      "epoch:  49 train_loss:  0.33 train_acc:  0.84 test_loss:  0.398 test_acc:  0.799\n",
      "min loss epoch:49\n",
      "epoch:  50 train_loss:  0.329 train_acc:  0.841 test_loss:  0.397 test_acc:  0.797\n",
      "min loss epoch:50\n",
      "epoch:  51 train_loss:  0.328 train_acc:  0.842 test_loss:  0.396 test_acc:  0.799\n",
      "min loss epoch:51\n",
      "epoch:  52 train_loss:  0.327 train_acc:  0.842 test_loss:  0.395 test_acc:  0.799\n",
      "min loss epoch:52\n",
      "epoch:  53 train_loss:  0.326 train_acc:  0.842 test_loss:  0.394 test_acc:  0.798\n",
      "min loss epoch:53\n",
      "epoch:  54 train_loss:  0.326 train_acc:  0.841 test_loss:  0.395 test_acc:  0.796\n",
      "epoch:  55 train_loss:  0.325 train_acc:  0.842 test_loss:  0.392 test_acc:  0.796\n",
      "min loss epoch:55\n",
      "epoch:  56 train_loss:  0.323 train_acc:  0.843 test_loss:  0.39 test_acc:  0.798\n",
      "min loss epoch:56\n",
      "epoch:  57 train_loss:  0.322 train_acc:  0.843 test_loss:  0.39 test_acc:  0.797\n",
      "min loss epoch:57\n",
      "epoch:  58 train_loss:  0.322 train_acc:  0.845 test_loss:  0.39 test_acc:  0.798\n",
      "min loss epoch:58\n",
      "epoch:  59 train_loss:  0.321 train_acc:  0.844 test_loss:  0.389 test_acc:  0.797\n",
      "min loss epoch:59\n",
      "epoch:  60 train_loss:  0.319 train_acc:  0.845 test_loss:  0.387 test_acc:  0.798\n",
      "min loss epoch:60\n",
      "epoch:  61 train_loss:  0.319 train_acc:  0.845 test_loss:  0.387 test_acc:  0.796\n",
      "epoch:  62 train_loss:  0.318 train_acc:  0.845 test_loss:  0.387 test_acc:  0.799\n",
      "min loss epoch:62\n",
      "epoch:  63 train_loss:  0.317 train_acc:  0.846 test_loss:  0.386 test_acc:  0.799\n",
      "min loss epoch:63\n",
      "epoch:  64 train_loss:  0.316 train_acc:  0.846 test_loss:  0.384 test_acc:  0.801\n",
      "min loss epoch:64\n",
      "max acc epoch：64        max acc：0.801\n",
      "epoch:  65 train_loss:  0.316 train_acc:  0.846 test_loss:  0.385 test_acc:  0.8\n",
      "epoch:  66 train_loss:  0.314 train_acc:  0.847 test_loss:  0.384 test_acc:  0.803\n",
      "min loss epoch:66\n",
      "max acc epoch：66        max acc：0.803\n",
      "epoch:  67 train_loss:  0.314 train_acc:  0.848 test_loss:  0.384 test_acc:  0.804\n",
      "min loss epoch:67\n",
      "max acc epoch：67        max acc：0.804\n",
      "epoch:  68 train_loss:  0.314 train_acc:  0.848 test_loss:  0.384 test_acc:  0.805\n",
      "max acc epoch：68        max acc：0.805\n",
      "epoch:  69 train_loss:  0.313 train_acc:  0.848 test_loss:  0.383 test_acc:  0.803\n",
      "min loss epoch:69\n",
      "epoch:  70 train_loss:  0.313 train_acc:  0.847 test_loss:  0.384 test_acc:  0.803\n",
      "epoch:  71 train_loss:  0.312 train_acc:  0.849 test_loss:  0.383 test_acc:  0.803\n",
      "epoch:  72 train_loss:  0.311 train_acc:  0.848 test_loss:  0.383 test_acc:  0.804\n",
      "min loss epoch:72\n",
      "epoch:  73 train_loss:  0.309 train_acc:  0.849 test_loss:  0.381 test_acc:  0.803\n",
      "min loss epoch:73\n",
      "epoch:  74 train_loss:  0.309 train_acc:  0.849 test_loss:  0.382 test_acc:  0.803\n",
      "epoch:  75 train_loss:  0.308 train_acc:  0.85 test_loss:  0.381 test_acc:  0.806\n",
      "min loss epoch:75\n",
      "max acc epoch：75        max acc：0.806\n",
      "epoch:  76 train_loss:  0.307 train_acc:  0.85 test_loss:  0.38 test_acc:  0.808\n",
      "min loss epoch:76\n",
      "max acc epoch：76        max acc：0.808\n",
      "epoch:  77 train_loss:  0.307 train_acc:  0.849 test_loss:  0.381 test_acc:  0.806\n",
      "epoch:  78 train_loss:  0.306 train_acc:  0.851 test_loss:  0.38 test_acc:  0.806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  79 train_loss:  0.304 train_acc:  0.851 test_loss:  0.378 test_acc:  0.805\n",
      "min loss epoch:79\n",
      "epoch:  80 train_loss:  0.304 train_acc:  0.851 test_loss:  0.378 test_acc:  0.807\n",
      "epoch:  81 train_loss:  0.303 train_acc:  0.852 test_loss:  0.377 test_acc:  0.808\n",
      "min loss epoch:81\n",
      "epoch:  82 train_loss:  0.301 train_acc:  0.852 test_loss:  0.376 test_acc:  0.81\n",
      "min loss epoch:82\n",
      "max acc epoch：82        max acc：0.81\n",
      "epoch:  83 train_loss:  0.301 train_acc:  0.854 test_loss:  0.376 test_acc:  0.81\n",
      "min loss epoch:83\n",
      "epoch:  84 train_loss:  0.3 train_acc:  0.854 test_loss:  0.375 test_acc:  0.81\n",
      "min loss epoch:84\n",
      "epoch:  85 train_loss:  0.299 train_acc:  0.854 test_loss:  0.374 test_acc:  0.809\n",
      "min loss epoch:85\n",
      "epoch:  86 train_loss:  0.298 train_acc:  0.856 test_loss:  0.374 test_acc:  0.811\n",
      "min loss epoch:86\n",
      "max acc epoch：86        max acc：0.811\n",
      "epoch:  87 train_loss:  0.298 train_acc:  0.855 test_loss:  0.374 test_acc:  0.811\n",
      "min loss epoch:87\n",
      "epoch:  88 train_loss:  0.297 train_acc:  0.855 test_loss:  0.374 test_acc:  0.81\n",
      "min loss epoch:88\n",
      "epoch:  89 train_loss:  0.297 train_acc:  0.855 test_loss:  0.373 test_acc:  0.812\n",
      "min loss epoch:89\n",
      "max acc epoch：89        max acc：0.812\n",
      "epoch:  90 train_loss:  0.297 train_acc:  0.855 test_loss:  0.373 test_acc:  0.811\n",
      "min loss epoch:90\n",
      "epoch:  91 train_loss:  0.295 train_acc:  0.855 test_loss:  0.372 test_acc:  0.812\n",
      "min loss epoch:91\n",
      "epoch:  92 train_loss:  0.295 train_acc:  0.855 test_loss:  0.371 test_acc:  0.81\n",
      "min loss epoch:92\n",
      "epoch:  93 train_loss:  0.295 train_acc:  0.855 test_loss:  0.372 test_acc:  0.81\n",
      "epoch:  94 train_loss:  0.294 train_acc:  0.854 test_loss:  0.371 test_acc:  0.81\n",
      "min loss epoch:94\n",
      "epoch:  95 train_loss:  0.293 train_acc:  0.854 test_loss:  0.369 test_acc:  0.812\n",
      "min loss epoch:95\n",
      "epoch:  96 train_loss:  0.293 train_acc:  0.855 test_loss:  0.37 test_acc:  0.813\n",
      "max acc epoch：96        max acc：0.813\n",
      "epoch:  97 train_loss:  0.292 train_acc:  0.856 test_loss:  0.369 test_acc:  0.813\n",
      "min loss epoch:97\n",
      "epoch:  98 train_loss:  0.292 train_acc:  0.855 test_loss:  0.368 test_acc:  0.813\n",
      "min loss epoch:98\n",
      "epoch:  99 train_loss:  0.291 train_acc:  0.855 test_loss:  0.368 test_acc:  0.813\n",
      "min loss epoch:99\n",
      "End max acc epoch：99        max acc：0.813\n",
      "['keypoint', 'brightness', 'contrast', 'edgeLength', 'Hue1', 'Hue2', 'Hue3', 'Hue5', 'Hue6', 'Hue7', 'Saturation', 'Value']\n",
      "6697\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([1340, 12])\n",
      "epoch:  0 train_loss:  0.461 train_acc:  0.757 test_loss:  0.483 test_acc:  0.749\n",
      "min loss epoch:0\n",
      "max acc epoch：0        max acc：0.749\n",
      "epoch:  1 train_loss:  0.449 train_acc:  0.76 test_loss:  0.474 test_acc:  0.745\n",
      "min loss epoch:1\n",
      "epoch:  2 train_loss:  0.441 train_acc:  0.763 test_loss:  0.47 test_acc:  0.746\n",
      "min loss epoch:2\n",
      "epoch:  3 train_loss:  0.435 train_acc:  0.77 test_loss:  0.466 test_acc:  0.746\n",
      "min loss epoch:3\n",
      "epoch:  4 train_loss:  0.429 train_acc:  0.771 test_loss:  0.462 test_acc:  0.746\n",
      "min loss epoch:4\n",
      "epoch:  5 train_loss:  0.423 train_acc:  0.775 test_loss:  0.459 test_acc:  0.749\n",
      "min loss epoch:5\n",
      "epoch:  6 train_loss:  0.419 train_acc:  0.78 test_loss:  0.458 test_acc:  0.747\n",
      "min loss epoch:6\n",
      "epoch:  7 train_loss:  0.415 train_acc:  0.784 test_loss:  0.455 test_acc:  0.748\n",
      "min loss epoch:7\n",
      "epoch:  8 train_loss:  0.41 train_acc:  0.782 test_loss:  0.453 test_acc:  0.751\n",
      "min loss epoch:8\n",
      "max acc epoch：8        max acc：0.751\n",
      "epoch:  9 train_loss:  0.406 train_acc:  0.783 test_loss:  0.45 test_acc:  0.754\n",
      "min loss epoch:9\n",
      "max acc epoch：9        max acc：0.754\n",
      "epoch:  10 train_loss:  0.402 train_acc:  0.787 test_loss:  0.447 test_acc:  0.757\n",
      "min loss epoch:10\n",
      "max acc epoch：10        max acc：0.757\n",
      "epoch:  11 train_loss:  0.398 train_acc:  0.79 test_loss:  0.443 test_acc:  0.756\n",
      "min loss epoch:11\n",
      "epoch:  12 train_loss:  0.394 train_acc:  0.792 test_loss:  0.441 test_acc:  0.759\n",
      "min loss epoch:12\n",
      "max acc epoch：12        max acc：0.759\n",
      "epoch:  13 train_loss:  0.391 train_acc:  0.795 test_loss:  0.439 test_acc:  0.762\n",
      "min loss epoch:13\n",
      "max acc epoch：13        max acc：0.762\n",
      "epoch:  14 train_loss:  0.388 train_acc:  0.799 test_loss:  0.437 test_acc:  0.764\n",
      "min loss epoch:14\n",
      "max acc epoch：14        max acc：0.764\n",
      "epoch:  15 train_loss:  0.385 train_acc:  0.802 test_loss:  0.435 test_acc:  0.765\n",
      "min loss epoch:15\n",
      "max acc epoch：15        max acc：0.765\n",
      "epoch:  16 train_loss:  0.382 train_acc:  0.806 test_loss:  0.433 test_acc:  0.773\n",
      "min loss epoch:16\n",
      "max acc epoch：16        max acc：0.773\n",
      "epoch:  17 train_loss:  0.38 train_acc:  0.808 test_loss:  0.432 test_acc:  0.779\n",
      "min loss epoch:17\n",
      "max acc epoch：17        max acc：0.779\n",
      "epoch:  18 train_loss:  0.378 train_acc:  0.809 test_loss:  0.431 test_acc:  0.782\n",
      "min loss epoch:18\n",
      "max acc epoch：18        max acc：0.782\n",
      "epoch:  19 train_loss:  0.376 train_acc:  0.811 test_loss:  0.43 test_acc:  0.784\n",
      "min loss epoch:19\n",
      "max acc epoch：19        max acc：0.784\n",
      "epoch:  20 train_loss:  0.373 train_acc:  0.815 test_loss:  0.428 test_acc:  0.786\n",
      "min loss epoch:20\n",
      "max acc epoch：20        max acc：0.786\n",
      "epoch:  21 train_loss:  0.371 train_acc:  0.816 test_loss:  0.428 test_acc:  0.787\n",
      "min loss epoch:21\n",
      "max acc epoch：21        max acc：0.787\n",
      "epoch:  22 train_loss:  0.369 train_acc:  0.816 test_loss:  0.427 test_acc:  0.784\n",
      "min loss epoch:22\n",
      "epoch:  23 train_loss:  0.368 train_acc:  0.817 test_loss:  0.427 test_acc:  0.785\n",
      "min loss epoch:23\n",
      "epoch:  24 train_loss:  0.365 train_acc:  0.819 test_loss:  0.425 test_acc:  0.784\n",
      "min loss epoch:24\n",
      "epoch:  25 train_loss:  0.364 train_acc:  0.822 test_loss:  0.424 test_acc:  0.784\n",
      "min loss epoch:25\n",
      "epoch:  26 train_loss:  0.361 train_acc:  0.824 test_loss:  0.422 test_acc:  0.784\n",
      "min loss epoch:26\n",
      "epoch:  27 train_loss:  0.36 train_acc:  0.824 test_loss:  0.422 test_acc:  0.784\n",
      "min loss epoch:27\n",
      "epoch:  28 train_loss:  0.358 train_acc:  0.826 test_loss:  0.421 test_acc:  0.784\n",
      "min loss epoch:28\n",
      "epoch:  29 train_loss:  0.357 train_acc:  0.826 test_loss:  0.421 test_acc:  0.784\n",
      "min loss epoch:29\n",
      "epoch:  30 train_loss:  0.357 train_acc:  0.825 test_loss:  0.421 test_acc:  0.784\n",
      "epoch:  31 train_loss:  0.354 train_acc:  0.828 test_loss:  0.42 test_acc:  0.783\n",
      "min loss epoch:31\n",
      "epoch:  32 train_loss:  0.353 train_acc:  0.827 test_loss:  0.419 test_acc:  0.785\n",
      "min loss epoch:32\n",
      "epoch:  33 train_loss:  0.352 train_acc:  0.828 test_loss:  0.418 test_acc:  0.787\n",
      "min loss epoch:33\n",
      "epoch:  34 train_loss:  0.351 train_acc:  0.828 test_loss:  0.417 test_acc:  0.788\n",
      "min loss epoch:34\n",
      "max acc epoch：34        max acc：0.788\n",
      "epoch:  35 train_loss:  0.35 train_acc:  0.828 test_loss:  0.417 test_acc:  0.788\n",
      "epoch:  36 train_loss:  0.349 train_acc:  0.828 test_loss:  0.418 test_acc:  0.788\n",
      "epoch:  37 train_loss:  0.347 train_acc:  0.83 test_loss:  0.417 test_acc:  0.787\n",
      "epoch:  38 train_loss:  0.346 train_acc:  0.83 test_loss:  0.417 test_acc:  0.79\n",
      "min loss epoch:38\n",
      "max acc epoch：38        max acc：0.79\n",
      "epoch:  39 train_loss:  0.345 train_acc:  0.831 test_loss:  0.417 test_acc:  0.79\n",
      "epoch:  40 train_loss:  0.344 train_acc:  0.831 test_loss:  0.418 test_acc:  0.788\n",
      "epoch:  41 train_loss:  0.343 train_acc:  0.831 test_loss:  0.417 test_acc:  0.789\n",
      "min loss epoch:41\n",
      "epoch:  42 train_loss:  0.342 train_acc:  0.831 test_loss:  0.416 test_acc:  0.791\n",
      "min loss epoch:42\n",
      "max acc epoch：42        max acc：0.791\n",
      "epoch:  43 train_loss:  0.341 train_acc:  0.832 test_loss:  0.415 test_acc:  0.79\n",
      "min loss epoch:43\n",
      "epoch:  44 train_loss:  0.34 train_acc:  0.832 test_loss:  0.415 test_acc:  0.79\n",
      "min loss epoch:44\n",
      "epoch:  45 train_loss:  0.339 train_acc:  0.83 test_loss:  0.415 test_acc:  0.791\n",
      "epoch:  46 train_loss:  0.338 train_acc:  0.832 test_loss:  0.415 test_acc:  0.793\n",
      "min loss epoch:46\n",
      "max acc epoch：46        max acc：0.793\n",
      "epoch:  47 train_loss:  0.337 train_acc:  0.831 test_loss:  0.415 test_acc:  0.79\n",
      "epoch:  48 train_loss:  0.336 train_acc:  0.832 test_loss:  0.415 test_acc:  0.785\n",
      "epoch:  49 train_loss:  0.334 train_acc:  0.832 test_loss:  0.414 test_acc:  0.787\n",
      "min loss epoch:49\n",
      "epoch:  50 train_loss:  0.334 train_acc:  0.833 test_loss:  0.414 test_acc:  0.79\n",
      "min loss epoch:50\n",
      "epoch:  51 train_loss:  0.333 train_acc:  0.833 test_loss:  0.413 test_acc:  0.793\n",
      "min loss epoch:51\n",
      "epoch:  52 train_loss:  0.332 train_acc:  0.834 test_loss:  0.413 test_acc:  0.79\n",
      "min loss epoch:52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  53 train_loss:  0.332 train_acc:  0.834 test_loss:  0.413 test_acc:  0.79\n",
      "epoch:  54 train_loss:  0.331 train_acc:  0.834 test_loss:  0.414 test_acc:  0.79\n",
      "epoch:  55 train_loss:  0.33 train_acc:  0.836 test_loss:  0.414 test_acc:  0.79\n",
      "epoch:  56 train_loss:  0.33 train_acc:  0.836 test_loss:  0.414 test_acc:  0.793\n",
      "epoch:  57 train_loss:  0.329 train_acc:  0.835 test_loss:  0.412 test_acc:  0.794\n",
      "min loss epoch:57\n",
      "max acc epoch：57        max acc：0.794\n",
      "epoch:  58 train_loss:  0.329 train_acc:  0.836 test_loss:  0.413 test_acc:  0.793\n",
      "epoch:  59 train_loss:  0.328 train_acc:  0.836 test_loss:  0.412 test_acc:  0.794\n",
      "min loss epoch:59\n",
      "epoch:  60 train_loss:  0.327 train_acc:  0.836 test_loss:  0.412 test_acc:  0.795\n",
      "max acc epoch：60        max acc：0.795\n",
      "epoch:  61 train_loss:  0.326 train_acc:  0.837 test_loss:  0.413 test_acc:  0.794\n",
      "epoch:  62 train_loss:  0.326 train_acc:  0.838 test_loss:  0.413 test_acc:  0.796\n",
      "max acc epoch：62        max acc：0.796\n",
      "epoch:  63 train_loss:  0.325 train_acc:  0.837 test_loss:  0.412 test_acc:  0.797\n",
      "min loss epoch:63\n",
      "max acc epoch：63        max acc：0.797\n",
      "epoch:  64 train_loss:  0.325 train_acc:  0.837 test_loss:  0.413 test_acc:  0.795\n",
      "epoch:  65 train_loss:  0.325 train_acc:  0.836 test_loss:  0.413 test_acc:  0.796\n",
      "epoch:  66 train_loss:  0.324 train_acc:  0.838 test_loss:  0.411 test_acc:  0.797\n",
      "min loss epoch:66\n",
      "epoch:  67 train_loss:  0.323 train_acc:  0.836 test_loss:  0.412 test_acc:  0.796\n",
      "epoch:  68 train_loss:  0.323 train_acc:  0.836 test_loss:  0.413 test_acc:  0.793\n",
      "epoch:  69 train_loss:  0.323 train_acc:  0.836 test_loss:  0.413 test_acc:  0.796\n",
      "epoch:  70 train_loss:  0.322 train_acc:  0.837 test_loss:  0.413 test_acc:  0.795\n",
      "epoch:  71 train_loss:  0.322 train_acc:  0.836 test_loss:  0.413 test_acc:  0.797\n",
      "epoch:  72 train_loss:  0.321 train_acc:  0.837 test_loss:  0.412 test_acc:  0.798\n",
      "max acc epoch：72        max acc：0.798\n",
      "epoch:  73 train_loss:  0.32 train_acc:  0.838 test_loss:  0.411 test_acc:  0.796\n",
      "min loss epoch:73\n",
      "epoch:  74 train_loss:  0.321 train_acc:  0.837 test_loss:  0.413 test_acc:  0.798\n",
      "epoch:  75 train_loss:  0.32 train_acc:  0.838 test_loss:  0.413 test_acc:  0.798\n",
      "epoch:  76 train_loss:  0.32 train_acc:  0.838 test_loss:  0.413 test_acc:  0.797\n",
      "epoch:  77 train_loss:  0.319 train_acc:  0.84 test_loss:  0.412 test_acc:  0.798\n",
      "epoch:  78 train_loss:  0.319 train_acc:  0.838 test_loss:  0.413 test_acc:  0.796\n",
      "epoch:  79 train_loss:  0.318 train_acc:  0.839 test_loss:  0.412 test_acc:  0.8\n",
      "max acc epoch：79        max acc：0.8\n",
      "epoch:  80 train_loss:  0.318 train_acc:  0.839 test_loss:  0.412 test_acc:  0.799\n",
      "epoch:  81 train_loss:  0.317 train_acc:  0.841 test_loss:  0.412 test_acc:  0.8\n",
      "epoch:  82 train_loss:  0.316 train_acc:  0.841 test_loss:  0.412 test_acc:  0.799\n",
      "epoch:  83 train_loss:  0.317 train_acc:  0.84 test_loss:  0.414 test_acc:  0.798\n",
      "epoch:  84 train_loss:  0.316 train_acc:  0.84 test_loss:  0.413 test_acc:  0.799\n",
      "epoch:  85 train_loss:  0.316 train_acc:  0.84 test_loss:  0.413 test_acc:  0.799\n",
      "epoch:  86 train_loss:  0.315 train_acc:  0.84 test_loss:  0.412 test_acc:  0.797\n",
      "epoch:  87 train_loss:  0.315 train_acc:  0.84 test_loss:  0.412 test_acc:  0.799\n",
      "epoch:  88 train_loss:  0.314 train_acc:  0.841 test_loss:  0.412 test_acc:  0.797\n",
      "epoch:  89 train_loss:  0.314 train_acc:  0.84 test_loss:  0.412 test_acc:  0.8\n",
      "epoch:  90 train_loss:  0.314 train_acc:  0.841 test_loss:  0.412 test_acc:  0.797\n",
      "epoch:  91 train_loss:  0.314 train_acc:  0.842 test_loss:  0.412 test_acc:  0.797\n",
      "epoch:  92 train_loss:  0.313 train_acc:  0.842 test_loss:  0.413 test_acc:  0.799\n",
      "epoch:  93 train_loss:  0.313 train_acc:  0.843 test_loss:  0.412 test_acc:  0.798\n",
      "epoch:  94 train_loss:  0.312 train_acc:  0.841 test_loss:  0.411 test_acc:  0.797\n",
      "epoch:  95 train_loss:  0.313 train_acc:  0.841 test_loss:  0.413 test_acc:  0.796\n",
      "epoch:  96 train_loss:  0.313 train_acc:  0.842 test_loss:  0.413 test_acc:  0.798\n",
      "epoch:  97 train_loss:  0.312 train_acc:  0.842 test_loss:  0.413 test_acc:  0.797\n",
      "epoch:  98 train_loss:  0.312 train_acc:  0.842 test_loss:  0.413 test_acc:  0.796\n",
      "epoch:  99 train_loss:  0.311 train_acc:  0.842 test_loss:  0.413 test_acc:  0.797\n",
      "End max acc epoch：99        max acc：0.8\n",
      "['keypoint', 'brightness', 'contrast', 'edgeLength', 'Hue1', 'Hue2', 'Hue3', 'Hue4', 'Hue6', 'Hue7', 'Saturation', 'Value']\n",
      "6697\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([1340, 12])\n",
      "epoch:  0 train_loss:  0.458 train_acc:  0.763 test_loss:  0.483 test_acc:  0.749\n",
      "min loss epoch:0\n",
      "max acc epoch：0        max acc：0.749\n",
      "epoch:  1 train_loss:  0.445 train_acc:  0.767 test_loss:  0.475 test_acc:  0.746\n",
      "min loss epoch:1\n",
      "epoch:  2 train_loss:  0.436 train_acc:  0.772 test_loss:  0.469 test_acc:  0.75\n",
      "min loss epoch:2\n",
      "max acc epoch：2        max acc：0.75\n",
      "epoch:  3 train_loss:  0.431 train_acc:  0.774 test_loss:  0.464 test_acc:  0.749\n",
      "min loss epoch:3\n",
      "epoch:  4 train_loss:  0.426 train_acc:  0.778 test_loss:  0.461 test_acc:  0.756\n",
      "min loss epoch:4\n",
      "max acc epoch：4        max acc：0.756\n",
      "epoch:  5 train_loss:  0.422 train_acc:  0.78 test_loss:  0.458 test_acc:  0.755\n",
      "min loss epoch:5\n",
      "epoch:  6 train_loss:  0.419 train_acc:  0.783 test_loss:  0.455 test_acc:  0.754\n",
      "min loss epoch:6\n",
      "epoch:  7 train_loss:  0.416 train_acc:  0.783 test_loss:  0.452 test_acc:  0.757\n",
      "min loss epoch:7\n",
      "max acc epoch：7        max acc：0.757\n",
      "epoch:  8 train_loss:  0.413 train_acc:  0.785 test_loss:  0.45 test_acc:  0.76\n",
      "min loss epoch:8\n",
      "max acc epoch：8        max acc：0.76\n",
      "epoch:  9 train_loss:  0.41 train_acc:  0.786 test_loss:  0.448 test_acc:  0.766\n",
      "min loss epoch:9\n",
      "max acc epoch：9        max acc：0.766\n",
      "epoch:  10 train_loss:  0.407 train_acc:  0.789 test_loss:  0.446 test_acc:  0.766\n",
      "min loss epoch:10\n",
      "epoch:  11 train_loss:  0.405 train_acc:  0.791 test_loss:  0.445 test_acc:  0.763\n",
      "min loss epoch:11\n",
      "epoch:  12 train_loss:  0.403 train_acc:  0.791 test_loss:  0.444 test_acc:  0.763\n",
      "min loss epoch:12\n",
      "epoch:  13 train_loss:  0.401 train_acc:  0.793 test_loss:  0.442 test_acc:  0.764\n",
      "min loss epoch:13\n",
      "epoch:  14 train_loss:  0.398 train_acc:  0.792 test_loss:  0.44 test_acc:  0.762\n",
      "min loss epoch:14\n",
      "epoch:  15 train_loss:  0.396 train_acc:  0.792 test_loss:  0.439 test_acc:  0.763\n",
      "min loss epoch:15\n",
      "epoch:  16 train_loss:  0.394 train_acc:  0.795 test_loss:  0.436 test_acc:  0.766\n",
      "min loss epoch:16\n",
      "epoch:  17 train_loss:  0.392 train_acc:  0.797 test_loss:  0.435 test_acc:  0.769\n",
      "min loss epoch:17\n",
      "max acc epoch：17        max acc：0.769\n",
      "epoch:  18 train_loss:  0.39 train_acc:  0.798 test_loss:  0.433 test_acc:  0.768\n",
      "min loss epoch:18\n",
      "epoch:  19 train_loss:  0.388 train_acc:  0.798 test_loss:  0.432 test_acc:  0.768\n",
      "min loss epoch:19\n",
      "epoch:  20 train_loss:  0.386 train_acc:  0.796 test_loss:  0.431 test_acc:  0.769\n",
      "min loss epoch:20\n",
      "epoch:  21 train_loss:  0.384 train_acc:  0.798 test_loss:  0.429 test_acc:  0.773\n",
      "min loss epoch:21\n",
      "max acc epoch：21        max acc：0.773\n",
      "epoch:  22 train_loss:  0.382 train_acc:  0.799 test_loss:  0.427 test_acc:  0.773\n",
      "min loss epoch:22\n",
      "epoch:  23 train_loss:  0.38 train_acc:  0.801 test_loss:  0.426 test_acc:  0.778\n",
      "min loss epoch:23\n",
      "max acc epoch：23        max acc：0.778\n",
      "epoch:  24 train_loss:  0.378 train_acc:  0.802 test_loss:  0.424 test_acc:  0.778\n",
      "min loss epoch:24\n",
      "epoch:  25 train_loss:  0.376 train_acc:  0.804 test_loss:  0.423 test_acc:  0.776\n",
      "min loss epoch:25\n",
      "epoch:  26 train_loss:  0.374 train_acc:  0.805 test_loss:  0.421 test_acc:  0.778\n",
      "min loss epoch:26\n",
      "epoch:  27 train_loss:  0.372 train_acc:  0.807 test_loss:  0.42 test_acc:  0.775\n",
      "min loss epoch:27\n",
      "epoch:  28 train_loss:  0.37 train_acc:  0.809 test_loss:  0.419 test_acc:  0.775\n",
      "min loss epoch:28\n",
      "epoch:  29 train_loss:  0.368 train_acc:  0.81 test_loss:  0.417 test_acc:  0.78\n",
      "min loss epoch:29\n",
      "max acc epoch：29        max acc：0.78\n",
      "epoch:  30 train_loss:  0.366 train_acc:  0.813 test_loss:  0.415 test_acc:  0.781\n",
      "min loss epoch:30\n",
      "max acc epoch：30        max acc：0.781\n",
      "epoch:  31 train_loss:  0.364 train_acc:  0.815 test_loss:  0.414 test_acc:  0.78\n",
      "min loss epoch:31\n",
      "epoch:  32 train_loss:  0.362 train_acc:  0.817 test_loss:  0.413 test_acc:  0.784\n",
      "min loss epoch:32\n",
      "max acc epoch：32        max acc：0.784\n",
      "epoch:  33 train_loss:  0.36 train_acc:  0.819 test_loss:  0.411 test_acc:  0.784\n",
      "min loss epoch:33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  34 train_loss:  0.358 train_acc:  0.821 test_loss:  0.41 test_acc:  0.785\n",
      "min loss epoch:34\n",
      "max acc epoch：34        max acc：0.785\n",
      "epoch:  35 train_loss:  0.356 train_acc:  0.821 test_loss:  0.408 test_acc:  0.79\n",
      "min loss epoch:35\n",
      "max acc epoch：35        max acc：0.79\n",
      "epoch:  36 train_loss:  0.355 train_acc:  0.822 test_loss:  0.407 test_acc:  0.787\n",
      "min loss epoch:36\n",
      "epoch:  37 train_loss:  0.353 train_acc:  0.823 test_loss:  0.407 test_acc:  0.79\n",
      "min loss epoch:37\n",
      "epoch:  38 train_loss:  0.351 train_acc:  0.825 test_loss:  0.405 test_acc:  0.792\n",
      "min loss epoch:38\n",
      "max acc epoch：38        max acc：0.792\n",
      "epoch:  39 train_loss:  0.35 train_acc:  0.826 test_loss:  0.404 test_acc:  0.79\n",
      "min loss epoch:39\n",
      "epoch:  40 train_loss:  0.348 train_acc:  0.826 test_loss:  0.403 test_acc:  0.79\n",
      "min loss epoch:40\n",
      "epoch:  41 train_loss:  0.347 train_acc:  0.827 test_loss:  0.403 test_acc:  0.787\n",
      "min loss epoch:41\n",
      "epoch:  42 train_loss:  0.345 train_acc:  0.828 test_loss:  0.401 test_acc:  0.791\n",
      "min loss epoch:42\n",
      "epoch:  43 train_loss:  0.343 train_acc:  0.829 test_loss:  0.4 test_acc:  0.793\n",
      "min loss epoch:43\n",
      "max acc epoch：43        max acc：0.793\n",
      "epoch:  44 train_loss:  0.342 train_acc:  0.83 test_loss:  0.398 test_acc:  0.793\n",
      "min loss epoch:44\n",
      "epoch:  45 train_loss:  0.341 train_acc:  0.829 test_loss:  0.398 test_acc:  0.796\n",
      "max acc epoch：45        max acc：0.796\n",
      "epoch:  46 train_loss:  0.339 train_acc:  0.832 test_loss:  0.397 test_acc:  0.796\n",
      "min loss epoch:46\n",
      "epoch:  47 train_loss:  0.338 train_acc:  0.829 test_loss:  0.396 test_acc:  0.798\n",
      "min loss epoch:47\n",
      "max acc epoch：47        max acc：0.798\n",
      "epoch:  48 train_loss:  0.336 train_acc:  0.832 test_loss:  0.395 test_acc:  0.796\n",
      "min loss epoch:48\n",
      "epoch:  49 train_loss:  0.335 train_acc:  0.833 test_loss:  0.395 test_acc:  0.798\n",
      "min loss epoch:49\n",
      "epoch:  50 train_loss:  0.333 train_acc:  0.835 test_loss:  0.394 test_acc:  0.796\n",
      "min loss epoch:50\n",
      "epoch:  51 train_loss:  0.332 train_acc:  0.836 test_loss:  0.392 test_acc:  0.796\n",
      "min loss epoch:51\n",
      "epoch:  52 train_loss:  0.331 train_acc:  0.835 test_loss:  0.392 test_acc:  0.797\n",
      "epoch:  53 train_loss:  0.329 train_acc:  0.836 test_loss:  0.391 test_acc:  0.796\n",
      "min loss epoch:53\n",
      "epoch:  54 train_loss:  0.328 train_acc:  0.836 test_loss:  0.39 test_acc:  0.799\n",
      "min loss epoch:54\n",
      "max acc epoch：54        max acc：0.799\n",
      "epoch:  55 train_loss:  0.327 train_acc:  0.837 test_loss:  0.389 test_acc:  0.796\n",
      "min loss epoch:55\n",
      "epoch:  56 train_loss:  0.325 train_acc:  0.839 test_loss:  0.388 test_acc:  0.799\n",
      "min loss epoch:56\n",
      "epoch:  57 train_loss:  0.324 train_acc:  0.84 test_loss:  0.387 test_acc:  0.799\n",
      "min loss epoch:57\n",
      "epoch:  58 train_loss:  0.323 train_acc:  0.839 test_loss:  0.386 test_acc:  0.8\n",
      "min loss epoch:58\n",
      "max acc epoch：58        max acc：0.8\n",
      "epoch:  59 train_loss:  0.322 train_acc:  0.841 test_loss:  0.386 test_acc:  0.801\n",
      "min loss epoch:59\n",
      "max acc epoch：59        max acc：0.801\n",
      "epoch:  60 train_loss:  0.321 train_acc:  0.841 test_loss:  0.384 test_acc:  0.799\n",
      "min loss epoch:60\n",
      "epoch:  61 train_loss:  0.32 train_acc:  0.839 test_loss:  0.384 test_acc:  0.8\n",
      "min loss epoch:61\n",
      "epoch:  62 train_loss:  0.319 train_acc:  0.842 test_loss:  0.383 test_acc:  0.799\n",
      "min loss epoch:62\n",
      "epoch:  63 train_loss:  0.318 train_acc:  0.843 test_loss:  0.381 test_acc:  0.799\n",
      "min loss epoch:63\n",
      "epoch:  64 train_loss:  0.317 train_acc:  0.844 test_loss:  0.38 test_acc:  0.801\n",
      "min loss epoch:64\n",
      "epoch:  65 train_loss:  0.315 train_acc:  0.844 test_loss:  0.379 test_acc:  0.804\n",
      "min loss epoch:65\n",
      "max acc epoch：65        max acc：0.804\n",
      "epoch:  66 train_loss:  0.314 train_acc:  0.845 test_loss:  0.377 test_acc:  0.804\n",
      "min loss epoch:66\n",
      "epoch:  67 train_loss:  0.313 train_acc:  0.844 test_loss:  0.377 test_acc:  0.803\n",
      "min loss epoch:67\n",
      "epoch:  68 train_loss:  0.312 train_acc:  0.845 test_loss:  0.376 test_acc:  0.807\n",
      "min loss epoch:68\n",
      "max acc epoch：68        max acc：0.807\n",
      "epoch:  69 train_loss:  0.31 train_acc:  0.847 test_loss:  0.376 test_acc:  0.805\n",
      "min loss epoch:69\n",
      "epoch:  70 train_loss:  0.31 train_acc:  0.847 test_loss:  0.375 test_acc:  0.807\n",
      "min loss epoch:70\n",
      "epoch:  71 train_loss:  0.308 train_acc:  0.848 test_loss:  0.374 test_acc:  0.809\n",
      "min loss epoch:71\n",
      "max acc epoch：71        max acc：0.809\n",
      "epoch:  72 train_loss:  0.308 train_acc:  0.848 test_loss:  0.373 test_acc:  0.811\n",
      "min loss epoch:72\n",
      "max acc epoch：72        max acc：0.811\n",
      "epoch:  73 train_loss:  0.306 train_acc:  0.849 test_loss:  0.371 test_acc:  0.81\n",
      "min loss epoch:73\n",
      "epoch:  74 train_loss:  0.305 train_acc:  0.851 test_loss:  0.371 test_acc:  0.81\n",
      "min loss epoch:74\n",
      "epoch:  75 train_loss:  0.304 train_acc:  0.85 test_loss:  0.37 test_acc:  0.807\n",
      "min loss epoch:75\n",
      "epoch:  76 train_loss:  0.303 train_acc:  0.851 test_loss:  0.368 test_acc:  0.811\n",
      "min loss epoch:76\n",
      "epoch:  77 train_loss:  0.302 train_acc:  0.851 test_loss:  0.368 test_acc:  0.813\n",
      "min loss epoch:77\n",
      "max acc epoch：77        max acc：0.813\n",
      "epoch:  78 train_loss:  0.301 train_acc:  0.851 test_loss:  0.367 test_acc:  0.814\n",
      "min loss epoch:78\n",
      "max acc epoch：78        max acc：0.814\n",
      "epoch:  79 train_loss:  0.299 train_acc:  0.852 test_loss:  0.365 test_acc:  0.813\n",
      "min loss epoch:79\n",
      "epoch:  80 train_loss:  0.297 train_acc:  0.853 test_loss:  0.364 test_acc:  0.813\n",
      "min loss epoch:80\n",
      "epoch:  81 train_loss:  0.296 train_acc:  0.854 test_loss:  0.363 test_acc:  0.815\n",
      "min loss epoch:81\n",
      "max acc epoch：81        max acc：0.815\n",
      "epoch:  82 train_loss:  0.295 train_acc:  0.856 test_loss:  0.361 test_acc:  0.811\n",
      "min loss epoch:82\n",
      "epoch:  83 train_loss:  0.294 train_acc:  0.855 test_loss:  0.36 test_acc:  0.816\n",
      "min loss epoch:83\n",
      "max acc epoch：83        max acc：0.816\n",
      "epoch:  84 train_loss:  0.293 train_acc:  0.856 test_loss:  0.359 test_acc:  0.816\n",
      "min loss epoch:84\n",
      "epoch:  85 train_loss:  0.292 train_acc:  0.857 test_loss:  0.358 test_acc:  0.819\n",
      "min loss epoch:85\n",
      "max acc epoch：85        max acc：0.819\n",
      "epoch:  86 train_loss:  0.29 train_acc:  0.856 test_loss:  0.357 test_acc:  0.816\n",
      "min loss epoch:86\n",
      "epoch:  87 train_loss:  0.289 train_acc:  0.857 test_loss:  0.355 test_acc:  0.816\n",
      "min loss epoch:87\n",
      "epoch:  88 train_loss:  0.288 train_acc:  0.857 test_loss:  0.354 test_acc:  0.819\n",
      "min loss epoch:88\n",
      "epoch:  89 train_loss:  0.287 train_acc:  0.857 test_loss:  0.353 test_acc:  0.817\n",
      "min loss epoch:89\n",
      "epoch:  90 train_loss:  0.285 train_acc:  0.856 test_loss:  0.351 test_acc:  0.817\n",
      "min loss epoch:90\n",
      "epoch:  91 train_loss:  0.284 train_acc:  0.856 test_loss:  0.35 test_acc:  0.818\n",
      "min loss epoch:91\n",
      "epoch:  92 train_loss:  0.283 train_acc:  0.857 test_loss:  0.348 test_acc:  0.814\n",
      "min loss epoch:92\n",
      "epoch:  93 train_loss:  0.282 train_acc:  0.857 test_loss:  0.346 test_acc:  0.819\n",
      "min loss epoch:93\n",
      "epoch:  94 train_loss:  0.281 train_acc:  0.856 test_loss:  0.345 test_acc:  0.815\n",
      "min loss epoch:94\n",
      "epoch:  95 train_loss:  0.28 train_acc:  0.857 test_loss:  0.345 test_acc:  0.816\n",
      "min loss epoch:95\n",
      "epoch:  96 train_loss:  0.279 train_acc:  0.859 test_loss:  0.342 test_acc:  0.822\n",
      "min loss epoch:96\n",
      "max acc epoch：96        max acc：0.822\n",
      "epoch:  97 train_loss:  0.279 train_acc:  0.858 test_loss:  0.342 test_acc:  0.822\n",
      "min loss epoch:97\n",
      "epoch:  98 train_loss:  0.278 train_acc:  0.857 test_loss:  0.341 test_acc:  0.822\n",
      "min loss epoch:98\n",
      "epoch:  99 train_loss:  0.277 train_acc:  0.86 test_loss:  0.34 test_acc:  0.82\n",
      "min loss epoch:99\n",
      "End max acc epoch：99        max acc：0.822\n",
      "['keypoint', 'brightness', 'contrast', 'edgeLength', 'Hue1', 'Hue2', 'Hue3', 'Hue4', 'Hue5', 'Hue7', 'Saturation', 'Value']\n",
      "6697\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([1340, 12])\n",
      "epoch:  0 train_loss:  0.469 train_acc:  0.755 test_loss:  0.485 test_acc:  0.745\n",
      "min loss epoch:0\n",
      "max acc epoch：0        max acc：0.745\n",
      "epoch:  1 train_loss:  0.452 train_acc:  0.756 test_loss:  0.473 test_acc:  0.748\n",
      "min loss epoch:1\n",
      "max acc epoch：1        max acc：0.748\n",
      "epoch:  2 train_loss:  0.444 train_acc:  0.764 test_loss:  0.467 test_acc:  0.749\n",
      "min loss epoch:2\n",
      "max acc epoch：2        max acc：0.749\n",
      "epoch:  3 train_loss:  0.437 train_acc:  0.767 test_loss:  0.462 test_acc:  0.749\n",
      "min loss epoch:3\n",
      "epoch:  4 train_loss:  0.431 train_acc:  0.769 test_loss:  0.458 test_acc:  0.752\n",
      "min loss epoch:4\n",
      "max acc epoch：4        max acc：0.752\n",
      "epoch:  5 train_loss:  0.426 train_acc:  0.775 test_loss:  0.454 test_acc:  0.752\n",
      "min loss epoch:5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  6 train_loss:  0.421 train_acc:  0.777 test_loss:  0.451 test_acc:  0.753\n",
      "min loss epoch:6\n",
      "max acc epoch：6        max acc：0.753\n",
      "epoch:  7 train_loss:  0.416 train_acc:  0.78 test_loss:  0.448 test_acc:  0.756\n",
      "min loss epoch:7\n",
      "max acc epoch：7        max acc：0.756\n",
      "epoch:  8 train_loss:  0.412 train_acc:  0.782 test_loss:  0.445 test_acc:  0.761\n",
      "min loss epoch:8\n",
      "max acc epoch：8        max acc：0.761\n",
      "epoch:  9 train_loss:  0.408 train_acc:  0.785 test_loss:  0.443 test_acc:  0.762\n",
      "min loss epoch:9\n",
      "max acc epoch：9        max acc：0.762\n",
      "epoch:  10 train_loss:  0.405 train_acc:  0.785 test_loss:  0.44 test_acc:  0.762\n",
      "min loss epoch:10\n",
      "epoch:  11 train_loss:  0.402 train_acc:  0.785 test_loss:  0.438 test_acc:  0.762\n",
      "min loss epoch:11\n",
      "epoch:  12 train_loss:  0.399 train_acc:  0.788 test_loss:  0.437 test_acc:  0.761\n",
      "min loss epoch:12\n",
      "epoch:  13 train_loss:  0.397 train_acc:  0.789 test_loss:  0.435 test_acc:  0.767\n",
      "min loss epoch:13\n",
      "max acc epoch：13        max acc：0.767\n",
      "epoch:  14 train_loss:  0.394 train_acc:  0.791 test_loss:  0.434 test_acc:  0.765\n",
      "min loss epoch:14\n",
      "epoch:  15 train_loss:  0.392 train_acc:  0.793 test_loss:  0.433 test_acc:  0.771\n",
      "min loss epoch:15\n",
      "max acc epoch：15        max acc：0.771\n",
      "epoch:  16 train_loss:  0.39 train_acc:  0.793 test_loss:  0.431 test_acc:  0.775\n",
      "min loss epoch:16\n",
      "max acc epoch：16        max acc：0.775\n",
      "epoch:  17 train_loss:  0.387 train_acc:  0.797 test_loss:  0.43 test_acc:  0.775\n",
      "min loss epoch:17\n",
      "epoch:  18 train_loss:  0.386 train_acc:  0.797 test_loss:  0.429 test_acc:  0.771\n",
      "min loss epoch:18\n",
      "epoch:  19 train_loss:  0.384 train_acc:  0.799 test_loss:  0.428 test_acc:  0.77\n",
      "min loss epoch:19\n",
      "epoch:  20 train_loss:  0.382 train_acc:  0.8 test_loss:  0.428 test_acc:  0.772\n",
      "min loss epoch:20\n",
      "epoch:  21 train_loss:  0.381 train_acc:  0.801 test_loss:  0.427 test_acc:  0.774\n",
      "min loss epoch:21\n",
      "epoch:  22 train_loss:  0.379 train_acc:  0.802 test_loss:  0.427 test_acc:  0.772\n",
      "epoch:  23 train_loss:  0.378 train_acc:  0.803 test_loss:  0.426 test_acc:  0.778\n",
      "min loss epoch:23\n",
      "max acc epoch：23        max acc：0.778\n",
      "epoch:  24 train_loss:  0.377 train_acc:  0.804 test_loss:  0.425 test_acc:  0.78\n",
      "min loss epoch:24\n",
      "max acc epoch：24        max acc：0.78\n",
      "epoch:  25 train_loss:  0.376 train_acc:  0.805 test_loss:  0.425 test_acc:  0.777\n",
      "min loss epoch:25\n",
      "epoch:  26 train_loss:  0.375 train_acc:  0.808 test_loss:  0.424 test_acc:  0.778\n",
      "min loss epoch:26\n",
      "epoch:  27 train_loss:  0.373 train_acc:  0.808 test_loss:  0.423 test_acc:  0.779\n",
      "min loss epoch:27\n",
      "epoch:  28 train_loss:  0.372 train_acc:  0.81 test_loss:  0.423 test_acc:  0.781\n",
      "min loss epoch:28\n",
      "max acc epoch：28        max acc：0.781\n",
      "epoch:  29 train_loss:  0.371 train_acc:  0.811 test_loss:  0.422 test_acc:  0.779\n",
      "min loss epoch:29\n",
      "epoch:  30 train_loss:  0.37 train_acc:  0.814 test_loss:  0.422 test_acc:  0.78\n",
      "min loss epoch:30\n",
      "epoch:  31 train_loss:  0.369 train_acc:  0.814 test_loss:  0.421 test_acc:  0.781\n",
      "min loss epoch:31\n",
      "epoch:  32 train_loss:  0.368 train_acc:  0.814 test_loss:  0.42 test_acc:  0.781\n",
      "min loss epoch:32\n",
      "epoch:  33 train_loss:  0.367 train_acc:  0.816 test_loss:  0.419 test_acc:  0.782\n",
      "min loss epoch:33\n",
      "max acc epoch：33        max acc：0.782\n",
      "epoch:  34 train_loss:  0.367 train_acc:  0.817 test_loss:  0.418 test_acc:  0.782\n",
      "min loss epoch:34\n",
      "epoch:  35 train_loss:  0.365 train_acc:  0.815 test_loss:  0.417 test_acc:  0.782\n",
      "min loss epoch:35\n",
      "epoch:  36 train_loss:  0.365 train_acc:  0.815 test_loss:  0.416 test_acc:  0.781\n",
      "min loss epoch:36\n",
      "epoch:  37 train_loss:  0.364 train_acc:  0.816 test_loss:  0.416 test_acc:  0.781\n",
      "min loss epoch:37\n",
      "epoch:  38 train_loss:  0.363 train_acc:  0.816 test_loss:  0.416 test_acc:  0.779\n",
      "min loss epoch:38\n",
      "epoch:  39 train_loss:  0.362 train_acc:  0.818 test_loss:  0.414 test_acc:  0.78\n",
      "min loss epoch:39\n",
      "epoch:  40 train_loss:  0.361 train_acc:  0.818 test_loss:  0.414 test_acc:  0.779\n",
      "min loss epoch:40\n",
      "epoch:  41 train_loss:  0.361 train_acc:  0.818 test_loss:  0.413 test_acc:  0.78\n",
      "min loss epoch:41\n",
      "epoch:  42 train_loss:  0.36 train_acc:  0.818 test_loss:  0.412 test_acc:  0.781\n",
      "min loss epoch:42\n",
      "epoch:  43 train_loss:  0.359 train_acc:  0.818 test_loss:  0.412 test_acc:  0.782\n",
      "min loss epoch:43\n",
      "epoch:  44 train_loss:  0.358 train_acc:  0.82 test_loss:  0.411 test_acc:  0.785\n",
      "min loss epoch:44\n",
      "max acc epoch：44        max acc：0.785\n",
      "epoch:  45 train_loss:  0.358 train_acc:  0.819 test_loss:  0.41 test_acc:  0.785\n",
      "min loss epoch:45\n",
      "epoch:  46 train_loss:  0.357 train_acc:  0.819 test_loss:  0.41 test_acc:  0.784\n",
      "min loss epoch:46\n",
      "epoch:  47 train_loss:  0.356 train_acc:  0.82 test_loss:  0.409 test_acc:  0.784\n",
      "min loss epoch:47\n",
      "epoch:  48 train_loss:  0.356 train_acc:  0.821 test_loss:  0.408 test_acc:  0.787\n",
      "min loss epoch:48\n",
      "max acc epoch：48        max acc：0.787\n",
      "epoch:  49 train_loss:  0.356 train_acc:  0.822 test_loss:  0.409 test_acc:  0.787\n",
      "epoch:  50 train_loss:  0.355 train_acc:  0.821 test_loss:  0.408 test_acc:  0.79\n",
      "min loss epoch:50\n",
      "max acc epoch：50        max acc：0.79\n",
      "epoch:  51 train_loss:  0.354 train_acc:  0.822 test_loss:  0.407 test_acc:  0.792\n",
      "min loss epoch:51\n",
      "max acc epoch：51        max acc：0.792\n",
      "epoch:  52 train_loss:  0.353 train_acc:  0.824 test_loss:  0.406 test_acc:  0.793\n",
      "min loss epoch:52\n",
      "max acc epoch：52        max acc：0.793\n",
      "epoch:  53 train_loss:  0.352 train_acc:  0.825 test_loss:  0.405 test_acc:  0.796\n",
      "min loss epoch:53\n",
      "max acc epoch：53        max acc：0.796\n",
      "epoch:  54 train_loss:  0.351 train_acc:  0.824 test_loss:  0.404 test_acc:  0.795\n",
      "min loss epoch:54\n",
      "epoch:  55 train_loss:  0.351 train_acc:  0.825 test_loss:  0.404 test_acc:  0.797\n",
      "max acc epoch：55        max acc：0.797\n",
      "epoch:  56 train_loss:  0.351 train_acc:  0.825 test_loss:  0.403 test_acc:  0.796\n",
      "min loss epoch:56\n",
      "epoch:  57 train_loss:  0.349 train_acc:  0.824 test_loss:  0.402 test_acc:  0.798\n",
      "min loss epoch:57\n",
      "max acc epoch：57        max acc：0.798\n",
      "epoch:  58 train_loss:  0.349 train_acc:  0.824 test_loss:  0.403 test_acc:  0.801\n",
      "max acc epoch：58        max acc：0.801\n",
      "epoch:  59 train_loss:  0.348 train_acc:  0.824 test_loss:  0.401 test_acc:  0.8\n",
      "min loss epoch:59\n",
      "epoch:  60 train_loss:  0.348 train_acc:  0.824 test_loss:  0.401 test_acc:  0.8\n",
      "epoch:  61 train_loss:  0.347 train_acc:  0.825 test_loss:  0.401 test_acc:  0.8\n",
      "min loss epoch:61\n",
      "epoch:  62 train_loss:  0.347 train_acc:  0.826 test_loss:  0.402 test_acc:  0.799\n",
      "epoch:  63 train_loss:  0.346 train_acc:  0.827 test_loss:  0.401 test_acc:  0.798\n",
      "epoch:  64 train_loss:  0.345 train_acc:  0.826 test_loss:  0.4 test_acc:  0.797\n",
      "min loss epoch:64\n",
      "epoch:  65 train_loss:  0.345 train_acc:  0.826 test_loss:  0.4 test_acc:  0.797\n",
      "min loss epoch:65\n",
      "epoch:  66 train_loss:  0.344 train_acc:  0.825 test_loss:  0.4 test_acc:  0.797\n",
      "epoch:  67 train_loss:  0.343 train_acc:  0.828 test_loss:  0.398 test_acc:  0.796\n",
      "min loss epoch:67\n",
      "epoch:  68 train_loss:  0.343 train_acc:  0.829 test_loss:  0.398 test_acc:  0.796\n",
      "epoch:  69 train_loss:  0.342 train_acc:  0.828 test_loss:  0.398 test_acc:  0.795\n",
      "min loss epoch:69\n",
      "epoch:  70 train_loss:  0.341 train_acc:  0.828 test_loss:  0.398 test_acc:  0.797\n",
      "epoch:  71 train_loss:  0.341 train_acc:  0.829 test_loss:  0.397 test_acc:  0.797\n",
      "min loss epoch:71\n",
      "epoch:  72 train_loss:  0.34 train_acc:  0.829 test_loss:  0.398 test_acc:  0.797\n",
      "epoch:  73 train_loss:  0.339 train_acc:  0.833 test_loss:  0.397 test_acc:  0.798\n",
      "min loss epoch:73\n",
      "epoch:  74 train_loss:  0.339 train_acc:  0.832 test_loss:  0.397 test_acc:  0.799\n",
      "epoch:  75 train_loss:  0.338 train_acc:  0.833 test_loss:  0.397 test_acc:  0.8\n",
      "min loss epoch:75\n",
      "epoch:  76 train_loss:  0.337 train_acc:  0.833 test_loss:  0.396 test_acc:  0.799\n",
      "min loss epoch:76\n",
      "epoch:  77 train_loss:  0.336 train_acc:  0.834 test_loss:  0.394 test_acc:  0.798\n",
      "min loss epoch:77\n",
      "epoch:  78 train_loss:  0.336 train_acc:  0.834 test_loss:  0.394 test_acc:  0.797\n",
      "min loss epoch:78\n",
      "epoch:  79 train_loss:  0.335 train_acc:  0.833 test_loss:  0.394 test_acc:  0.801\n",
      "min loss epoch:79\n",
      "epoch:  80 train_loss:  0.335 train_acc:  0.834 test_loss:  0.394 test_acc:  0.799\n",
      "min loss epoch:80\n",
      "epoch:  81 train_loss:  0.334 train_acc:  0.833 test_loss:  0.394 test_acc:  0.797\n",
      "min loss epoch:81\n",
      "epoch:  82 train_loss:  0.333 train_acc:  0.834 test_loss:  0.392 test_acc:  0.801\n",
      "min loss epoch:82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  83 train_loss:  0.333 train_acc:  0.834 test_loss:  0.393 test_acc:  0.8\n",
      "epoch:  84 train_loss:  0.333 train_acc:  0.834 test_loss:  0.392 test_acc:  0.796\n",
      "epoch:  85 train_loss:  0.333 train_acc:  0.833 test_loss:  0.392 test_acc:  0.799\n",
      "epoch:  86 train_loss:  0.331 train_acc:  0.834 test_loss:  0.391 test_acc:  0.798\n",
      "min loss epoch:86\n",
      "epoch:  87 train_loss:  0.33 train_acc:  0.834 test_loss:  0.39 test_acc:  0.799\n",
      "min loss epoch:87\n",
      "epoch:  88 train_loss:  0.329 train_acc:  0.835 test_loss:  0.39 test_acc:  0.799\n",
      "epoch:  89 train_loss:  0.328 train_acc:  0.835 test_loss:  0.389 test_acc:  0.798\n",
      "min loss epoch:89\n",
      "epoch:  90 train_loss:  0.326 train_acc:  0.839 test_loss:  0.387 test_acc:  0.804\n",
      "min loss epoch:90\n",
      "max acc epoch：90        max acc：0.804\n",
      "epoch:  91 train_loss:  0.326 train_acc:  0.838 test_loss:  0.388 test_acc:  0.803\n",
      "epoch:  92 train_loss:  0.323 train_acc:  0.84 test_loss:  0.384 test_acc:  0.807\n",
      "min loss epoch:92\n",
      "max acc epoch：92        max acc：0.807\n",
      "epoch:  93 train_loss:  0.324 train_acc:  0.84 test_loss:  0.385 test_acc:  0.806\n",
      "epoch:  94 train_loss:  0.323 train_acc:  0.842 test_loss:  0.384 test_acc:  0.806\n",
      "min loss epoch:94\n",
      "epoch:  95 train_loss:  0.321 train_acc:  0.842 test_loss:  0.383 test_acc:  0.807\n",
      "min loss epoch:95\n",
      "epoch:  96 train_loss:  0.321 train_acc:  0.843 test_loss:  0.383 test_acc:  0.806\n",
      "min loss epoch:96\n",
      "epoch:  97 train_loss:  0.321 train_acc:  0.843 test_loss:  0.384 test_acc:  0.805\n",
      "epoch:  98 train_loss:  0.32 train_acc:  0.843 test_loss:  0.382 test_acc:  0.805\n",
      "min loss epoch:98\n",
      "epoch:  99 train_loss:  0.319 train_acc:  0.844 test_loss:  0.383 test_acc:  0.802\n",
      "End max acc epoch：99        max acc：0.807\n",
      "['keypoint', 'brightness', 'contrast', 'edgeLength', 'Hue1', 'Hue2', 'Hue3', 'Hue4', 'Hue5', 'Hue6', 'Saturation', 'Value']\n",
      "6697\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([1340, 12])\n",
      "epoch:  0 train_loss:  0.464 train_acc:  0.759 test_loss:  0.481 test_acc:  0.751\n",
      "min loss epoch:0\n",
      "max acc epoch：0        max acc：0.751\n",
      "epoch:  1 train_loss:  0.449 train_acc:  0.762 test_loss:  0.471 test_acc:  0.756\n",
      "min loss epoch:1\n",
      "max acc epoch：1        max acc：0.756\n",
      "epoch:  2 train_loss:  0.441 train_acc:  0.766 test_loss:  0.465 test_acc:  0.756\n",
      "min loss epoch:2\n",
      "epoch:  3 train_loss:  0.435 train_acc:  0.77 test_loss:  0.46 test_acc:  0.758\n",
      "min loss epoch:3\n",
      "max acc epoch：3        max acc：0.758\n",
      "epoch:  4 train_loss:  0.429 train_acc:  0.774 test_loss:  0.456 test_acc:  0.757\n",
      "min loss epoch:4\n",
      "epoch:  5 train_loss:  0.424 train_acc:  0.78 test_loss:  0.452 test_acc:  0.76\n",
      "min loss epoch:5\n",
      "max acc epoch：5        max acc：0.76\n",
      "epoch:  6 train_loss:  0.419 train_acc:  0.781 test_loss:  0.448 test_acc:  0.764\n",
      "min loss epoch:6\n",
      "max acc epoch：6        max acc：0.764\n",
      "epoch:  7 train_loss:  0.414 train_acc:  0.784 test_loss:  0.444 test_acc:  0.766\n",
      "min loss epoch:7\n",
      "max acc epoch：7        max acc：0.766\n",
      "epoch:  8 train_loss:  0.41 train_acc:  0.787 test_loss:  0.441 test_acc:  0.768\n",
      "min loss epoch:8\n",
      "max acc epoch：8        max acc：0.768\n",
      "epoch:  9 train_loss:  0.406 train_acc:  0.789 test_loss:  0.438 test_acc:  0.768\n",
      "min loss epoch:9\n",
      "epoch:  10 train_loss:  0.402 train_acc:  0.789 test_loss:  0.436 test_acc:  0.772\n",
      "min loss epoch:10\n",
      "max acc epoch：10        max acc：0.772\n",
      "epoch:  11 train_loss:  0.4 train_acc:  0.791 test_loss:  0.433 test_acc:  0.773\n",
      "min loss epoch:11\n",
      "max acc epoch：11        max acc：0.773\n",
      "epoch:  12 train_loss:  0.396 train_acc:  0.792 test_loss:  0.431 test_acc:  0.773\n",
      "min loss epoch:12\n",
      "epoch:  13 train_loss:  0.394 train_acc:  0.796 test_loss:  0.429 test_acc:  0.773\n",
      "min loss epoch:13\n",
      "epoch:  14 train_loss:  0.391 train_acc:  0.797 test_loss:  0.427 test_acc:  0.775\n",
      "min loss epoch:14\n",
      "max acc epoch：14        max acc：0.775\n",
      "epoch:  15 train_loss:  0.388 train_acc:  0.797 test_loss:  0.425 test_acc:  0.772\n",
      "min loss epoch:15\n",
      "epoch:  16 train_loss:  0.386 train_acc:  0.797 test_loss:  0.423 test_acc:  0.774\n",
      "min loss epoch:16\n",
      "epoch:  17 train_loss:  0.383 train_acc:  0.799 test_loss:  0.421 test_acc:  0.775\n",
      "min loss epoch:17\n",
      "epoch:  18 train_loss:  0.381 train_acc:  0.8 test_loss:  0.42 test_acc:  0.775\n",
      "min loss epoch:18\n",
      "epoch:  19 train_loss:  0.379 train_acc:  0.8 test_loss:  0.419 test_acc:  0.775\n",
      "min loss epoch:19\n",
      "epoch:  20 train_loss:  0.376 train_acc:  0.8 test_loss:  0.417 test_acc:  0.778\n",
      "min loss epoch:20\n",
      "max acc epoch：20        max acc：0.778\n",
      "epoch:  21 train_loss:  0.374 train_acc:  0.803 test_loss:  0.416 test_acc:  0.773\n",
      "min loss epoch:21\n",
      "epoch:  22 train_loss:  0.372 train_acc:  0.805 test_loss:  0.414 test_acc:  0.777\n",
      "min loss epoch:22\n",
      "epoch:  23 train_loss:  0.37 train_acc:  0.807 test_loss:  0.413 test_acc:  0.778\n",
      "min loss epoch:23\n",
      "epoch:  24 train_loss:  0.368 train_acc:  0.809 test_loss:  0.413 test_acc:  0.781\n",
      "min loss epoch:24\n",
      "max acc epoch：24        max acc：0.781\n",
      "epoch:  25 train_loss:  0.366 train_acc:  0.811 test_loss:  0.412 test_acc:  0.783\n",
      "min loss epoch:25\n",
      "max acc epoch：25        max acc：0.783\n",
      "epoch:  26 train_loss:  0.364 train_acc:  0.813 test_loss:  0.411 test_acc:  0.784\n",
      "min loss epoch:26\n",
      "max acc epoch：26        max acc：0.784\n",
      "epoch:  27 train_loss:  0.362 train_acc:  0.814 test_loss:  0.41 test_acc:  0.789\n",
      "min loss epoch:27\n",
      "max acc epoch：27        max acc：0.789\n",
      "epoch:  28 train_loss:  0.36 train_acc:  0.816 test_loss:  0.409 test_acc:  0.79\n",
      "min loss epoch:28\n",
      "max acc epoch：28        max acc：0.79\n",
      "epoch:  29 train_loss:  0.358 train_acc:  0.815 test_loss:  0.408 test_acc:  0.789\n",
      "min loss epoch:29\n",
      "epoch:  30 train_loss:  0.356 train_acc:  0.819 test_loss:  0.407 test_acc:  0.79\n",
      "min loss epoch:30\n",
      "epoch:  31 train_loss:  0.354 train_acc:  0.82 test_loss:  0.406 test_acc:  0.791\n",
      "min loss epoch:31\n",
      "max acc epoch：31        max acc：0.791\n",
      "epoch:  32 train_loss:  0.353 train_acc:  0.822 test_loss:  0.406 test_acc:  0.789\n",
      "epoch:  33 train_loss:  0.351 train_acc:  0.823 test_loss:  0.406 test_acc:  0.794\n",
      "min loss epoch:33\n",
      "max acc epoch：33        max acc：0.794\n",
      "epoch:  34 train_loss:  0.349 train_acc:  0.825 test_loss:  0.404 test_acc:  0.793\n",
      "min loss epoch:34\n",
      "epoch:  35 train_loss:  0.348 train_acc:  0.825 test_loss:  0.404 test_acc:  0.792\n",
      "min loss epoch:35\n",
      "epoch:  36 train_loss:  0.346 train_acc:  0.829 test_loss:  0.404 test_acc:  0.791\n",
      "min loss epoch:36\n",
      "epoch:  37 train_loss:  0.344 train_acc:  0.829 test_loss:  0.402 test_acc:  0.79\n",
      "min loss epoch:37\n",
      "epoch:  38 train_loss:  0.342 train_acc:  0.831 test_loss:  0.4 test_acc:  0.79\n",
      "min loss epoch:38\n",
      "epoch:  39 train_loss:  0.341 train_acc:  0.832 test_loss:  0.399 test_acc:  0.791\n",
      "min loss epoch:39\n",
      "epoch:  40 train_loss:  0.339 train_acc:  0.832 test_loss:  0.398 test_acc:  0.793\n",
      "min loss epoch:40\n",
      "epoch:  41 train_loss:  0.337 train_acc:  0.834 test_loss:  0.397 test_acc:  0.793\n",
      "min loss epoch:41\n",
      "epoch:  42 train_loss:  0.337 train_acc:  0.835 test_loss:  0.398 test_acc:  0.791\n",
      "epoch:  43 train_loss:  0.335 train_acc:  0.836 test_loss:  0.396 test_acc:  0.792\n",
      "min loss epoch:43\n",
      "epoch:  44 train_loss:  0.334 train_acc:  0.837 test_loss:  0.396 test_acc:  0.796\n",
      "min loss epoch:44\n",
      "max acc epoch：44        max acc：0.796\n",
      "epoch:  45 train_loss:  0.332 train_acc:  0.838 test_loss:  0.395 test_acc:  0.798\n",
      "min loss epoch:45\n",
      "max acc epoch：45        max acc：0.798\n",
      "epoch:  46 train_loss:  0.331 train_acc:  0.837 test_loss:  0.395 test_acc:  0.801\n",
      "min loss epoch:46\n",
      "max acc epoch：46        max acc：0.801\n",
      "epoch:  47 train_loss:  0.329 train_acc:  0.839 test_loss:  0.394 test_acc:  0.801\n",
      "min loss epoch:47\n",
      "epoch:  48 train_loss:  0.328 train_acc:  0.839 test_loss:  0.394 test_acc:  0.798\n",
      "min loss epoch:48\n",
      "epoch:  49 train_loss:  0.327 train_acc:  0.839 test_loss:  0.393 test_acc:  0.802\n",
      "min loss epoch:49\n",
      "max acc epoch：49        max acc：0.802\n",
      "epoch:  50 train_loss:  0.326 train_acc:  0.842 test_loss:  0.392 test_acc:  0.8\n",
      "min loss epoch:50\n",
      "epoch:  51 train_loss:  0.325 train_acc:  0.841 test_loss:  0.392 test_acc:  0.8\n",
      "epoch:  52 train_loss:  0.323 train_acc:  0.843 test_loss:  0.39 test_acc:  0.805\n",
      "min loss epoch:52\n",
      "max acc epoch：52        max acc：0.805\n",
      "epoch:  53 train_loss:  0.322 train_acc:  0.843 test_loss:  0.39 test_acc:  0.804\n",
      "min loss epoch:53\n",
      "epoch:  54 train_loss:  0.321 train_acc:  0.844 test_loss:  0.389 test_acc:  0.804\n",
      "min loss epoch:54\n",
      "epoch:  55 train_loss:  0.32 train_acc:  0.844 test_loss:  0.388 test_acc:  0.805\n",
      "min loss epoch:55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  56 train_loss:  0.318 train_acc:  0.844 test_loss:  0.387 test_acc:  0.804\n",
      "min loss epoch:56\n",
      "epoch:  57 train_loss:  0.317 train_acc:  0.846 test_loss:  0.387 test_acc:  0.808\n",
      "min loss epoch:57\n",
      "max acc epoch：57        max acc：0.808\n",
      "epoch:  58 train_loss:  0.316 train_acc:  0.845 test_loss:  0.385 test_acc:  0.81\n",
      "min loss epoch:58\n",
      "max acc epoch：58        max acc：0.81\n",
      "epoch:  59 train_loss:  0.315 train_acc:  0.845 test_loss:  0.385 test_acc:  0.808\n",
      "min loss epoch:59\n",
      "epoch:  60 train_loss:  0.315 train_acc:  0.842 test_loss:  0.385 test_acc:  0.806\n",
      "min loss epoch:60\n",
      "epoch:  61 train_loss:  0.313 train_acc:  0.846 test_loss:  0.383 test_acc:  0.81\n",
      "min loss epoch:61\n",
      "epoch:  62 train_loss:  0.313 train_acc:  0.845 test_loss:  0.381 test_acc:  0.808\n",
      "min loss epoch:62\n",
      "epoch:  63 train_loss:  0.311 train_acc:  0.846 test_loss:  0.38 test_acc:  0.805\n",
      "min loss epoch:63\n",
      "epoch:  64 train_loss:  0.31 train_acc:  0.845 test_loss:  0.379 test_acc:  0.81\n",
      "min loss epoch:64\n",
      "epoch:  65 train_loss:  0.309 train_acc:  0.847 test_loss:  0.379 test_acc:  0.81\n",
      "min loss epoch:65\n",
      "epoch:  66 train_loss:  0.307 train_acc:  0.847 test_loss:  0.378 test_acc:  0.813\n",
      "min loss epoch:66\n",
      "max acc epoch：66        max acc：0.813\n",
      "epoch:  67 train_loss:  0.305 train_acc:  0.847 test_loss:  0.378 test_acc:  0.81\n",
      "min loss epoch:67\n",
      "epoch:  68 train_loss:  0.305 train_acc:  0.848 test_loss:  0.377 test_acc:  0.811\n",
      "min loss epoch:68\n",
      "epoch:  69 train_loss:  0.303 train_acc:  0.847 test_loss:  0.376 test_acc:  0.81\n",
      "min loss epoch:69\n",
      "epoch:  70 train_loss:  0.303 train_acc:  0.849 test_loss:  0.376 test_acc:  0.812\n",
      "epoch:  71 train_loss:  0.3 train_acc:  0.851 test_loss:  0.374 test_acc:  0.811\n",
      "min loss epoch:71\n",
      "epoch:  72 train_loss:  0.3 train_acc:  0.852 test_loss:  0.375 test_acc:  0.811\n",
      "epoch:  73 train_loss:  0.3 train_acc:  0.852 test_loss:  0.375 test_acc:  0.813\n",
      "epoch:  74 train_loss:  0.299 train_acc:  0.854 test_loss:  0.375 test_acc:  0.812\n",
      "epoch:  75 train_loss:  0.296 train_acc:  0.855 test_loss:  0.372 test_acc:  0.813\n",
      "min loss epoch:75\n",
      "epoch:  76 train_loss:  0.296 train_acc:  0.853 test_loss:  0.372 test_acc:  0.811\n",
      "epoch:  77 train_loss:  0.294 train_acc:  0.854 test_loss:  0.371 test_acc:  0.812\n",
      "min loss epoch:77\n",
      "epoch:  78 train_loss:  0.291 train_acc:  0.855 test_loss:  0.37 test_acc:  0.81\n",
      "min loss epoch:78\n",
      "epoch:  79 train_loss:  0.292 train_acc:  0.856 test_loss:  0.371 test_acc:  0.807\n",
      "epoch:  80 train_loss:  0.29 train_acc:  0.857 test_loss:  0.371 test_acc:  0.81\n",
      "epoch:  81 train_loss:  0.289 train_acc:  0.859 test_loss:  0.368 test_acc:  0.812\n",
      "min loss epoch:81\n",
      "epoch:  82 train_loss:  0.284 train_acc:  0.86 test_loss:  0.364 test_acc:  0.816\n",
      "min loss epoch:82\n",
      "max acc epoch：82        max acc：0.816\n",
      "epoch:  83 train_loss:  0.284 train_acc:  0.86 test_loss:  0.365 test_acc:  0.815\n",
      "epoch:  84 train_loss:  0.281 train_acc:  0.863 test_loss:  0.361 test_acc:  0.816\n",
      "min loss epoch:84\n",
      "epoch:  85 train_loss:  0.279 train_acc:  0.865 test_loss:  0.361 test_acc:  0.818\n",
      "min loss epoch:85\n",
      "max acc epoch：85        max acc：0.818\n",
      "epoch:  86 train_loss:  0.278 train_acc:  0.866 test_loss:  0.36 test_acc:  0.819\n",
      "min loss epoch:86\n",
      "max acc epoch：86        max acc：0.819\n",
      "epoch:  87 train_loss:  0.277 train_acc:  0.866 test_loss:  0.36 test_acc:  0.817\n",
      "epoch:  88 train_loss:  0.275 train_acc:  0.867 test_loss:  0.359 test_acc:  0.819\n",
      "min loss epoch:88\n",
      "epoch:  89 train_loss:  0.275 train_acc:  0.868 test_loss:  0.358 test_acc:  0.822\n",
      "min loss epoch:89\n",
      "max acc epoch：89        max acc：0.822\n",
      "epoch:  90 train_loss:  0.274 train_acc:  0.869 test_loss:  0.358 test_acc:  0.82\n",
      "epoch:  91 train_loss:  0.273 train_acc:  0.869 test_loss:  0.357 test_acc:  0.824\n",
      "min loss epoch:91\n",
      "max acc epoch：91        max acc：0.824\n",
      "epoch:  92 train_loss:  0.272 train_acc:  0.87 test_loss:  0.356 test_acc:  0.825\n",
      "min loss epoch:92\n",
      "max acc epoch：92        max acc：0.825\n",
      "epoch:  93 train_loss:  0.271 train_acc:  0.87 test_loss:  0.356 test_acc:  0.825\n",
      "epoch:  94 train_loss:  0.27 train_acc:  0.871 test_loss:  0.356 test_acc:  0.826\n",
      "max acc epoch：94        max acc：0.826\n",
      "epoch:  95 train_loss:  0.27 train_acc:  0.87 test_loss:  0.355 test_acc:  0.826\n",
      "min loss epoch:95\n",
      "epoch:  96 train_loss:  0.268 train_acc:  0.872 test_loss:  0.354 test_acc:  0.827\n",
      "min loss epoch:96\n",
      "max acc epoch：96        max acc：0.827\n",
      "epoch:  97 train_loss:  0.267 train_acc:  0.872 test_loss:  0.354 test_acc:  0.829\n",
      "max acc epoch：97        max acc：0.829\n",
      "epoch:  98 train_loss:  0.266 train_acc:  0.874 test_loss:  0.353 test_acc:  0.827\n",
      "min loss epoch:98\n",
      "epoch:  99 train_loss:  0.265 train_acc:  0.874 test_loss:  0.353 test_acc:  0.832\n",
      "max acc epoch：99        max acc：0.832\n",
      "End max acc epoch：99        max acc：0.832\n",
      "['keypoint', 'brightness', 'contrast', 'edgeLength', 'Hue1', 'Hue2', 'Hue3', 'Hue4', 'Hue5', 'Hue6', 'Hue7', 'Value']\n",
      "6697\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([1340, 12])\n",
      "epoch:  0 train_loss:  0.468 train_acc:  0.752 test_loss:  0.488 test_acc:  0.741\n",
      "min loss epoch:0\n",
      "max acc epoch：0        max acc：0.741\n",
      "epoch:  1 train_loss:  0.454 train_acc:  0.758 test_loss:  0.477 test_acc:  0.743\n",
      "min loss epoch:1\n",
      "max acc epoch：1        max acc：0.743\n",
      "epoch:  2 train_loss:  0.447 train_acc:  0.76 test_loss:  0.471 test_acc:  0.747\n",
      "min loss epoch:2\n",
      "max acc epoch：2        max acc：0.747\n",
      "epoch:  3 train_loss:  0.441 train_acc:  0.763 test_loss:  0.466 test_acc:  0.752\n",
      "min loss epoch:3\n",
      "max acc epoch：3        max acc：0.752\n",
      "epoch:  4 train_loss:  0.436 train_acc:  0.764 test_loss:  0.462 test_acc:  0.75\n",
      "min loss epoch:4\n",
      "epoch:  5 train_loss:  0.432 train_acc:  0.769 test_loss:  0.46 test_acc:  0.749\n",
      "min loss epoch:5\n",
      "epoch:  6 train_loss:  0.428 train_acc:  0.772 test_loss:  0.457 test_acc:  0.75\n",
      "min loss epoch:6\n",
      "epoch:  7 train_loss:  0.425 train_acc:  0.775 test_loss:  0.455 test_acc:  0.752\n",
      "min loss epoch:7\n",
      "epoch:  8 train_loss:  0.422 train_acc:  0.779 test_loss:  0.454 test_acc:  0.754\n",
      "min loss epoch:8\n",
      "max acc epoch：8        max acc：0.754\n",
      "epoch:  9 train_loss:  0.419 train_acc:  0.78 test_loss:  0.452 test_acc:  0.76\n",
      "min loss epoch:9\n",
      "max acc epoch：9        max acc：0.76\n",
      "epoch:  10 train_loss:  0.416 train_acc:  0.78 test_loss:  0.45 test_acc:  0.76\n",
      "min loss epoch:10\n",
      "epoch:  11 train_loss:  0.413 train_acc:  0.783 test_loss:  0.448 test_acc:  0.76\n",
      "min loss epoch:11\n",
      "epoch:  12 train_loss:  0.41 train_acc:  0.783 test_loss:  0.447 test_acc:  0.763\n",
      "min loss epoch:12\n",
      "max acc epoch：12        max acc：0.763\n",
      "epoch:  13 train_loss:  0.408 train_acc:  0.786 test_loss:  0.446 test_acc:  0.763\n",
      "min loss epoch:13\n",
      "epoch:  14 train_loss:  0.405 train_acc:  0.786 test_loss:  0.444 test_acc:  0.764\n",
      "min loss epoch:14\n",
      "max acc epoch：14        max acc：0.764\n",
      "epoch:  15 train_loss:  0.403 train_acc:  0.788 test_loss:  0.443 test_acc:  0.763\n",
      "min loss epoch:15\n",
      "epoch:  16 train_loss:  0.4 train_acc:  0.789 test_loss:  0.441 test_acc:  0.762\n",
      "min loss epoch:16\n",
      "epoch:  17 train_loss:  0.398 train_acc:  0.787 test_loss:  0.44 test_acc:  0.765\n",
      "min loss epoch:17\n",
      "max acc epoch：17        max acc：0.765\n",
      "epoch:  18 train_loss:  0.395 train_acc:  0.787 test_loss:  0.439 test_acc:  0.767\n",
      "min loss epoch:18\n",
      "max acc epoch：18        max acc：0.767\n",
      "epoch:  19 train_loss:  0.393 train_acc:  0.788 test_loss:  0.438 test_acc:  0.764\n",
      "min loss epoch:19\n",
      "epoch:  20 train_loss:  0.391 train_acc:  0.788 test_loss:  0.437 test_acc:  0.761\n",
      "min loss epoch:20\n",
      "epoch:  21 train_loss:  0.389 train_acc:  0.79 test_loss:  0.436 test_acc:  0.764\n",
      "min loss epoch:21\n",
      "epoch:  22 train_loss:  0.386 train_acc:  0.792 test_loss:  0.435 test_acc:  0.763\n",
      "min loss epoch:22\n",
      "epoch:  23 train_loss:  0.384 train_acc:  0.794 test_loss:  0.434 test_acc:  0.766\n",
      "min loss epoch:23\n",
      "epoch:  24 train_loss:  0.382 train_acc:  0.796 test_loss:  0.433 test_acc:  0.769\n",
      "min loss epoch:24\n",
      "max acc epoch：24        max acc：0.769\n",
      "epoch:  25 train_loss:  0.379 train_acc:  0.797 test_loss:  0.431 test_acc:  0.772\n",
      "min loss epoch:25\n",
      "max acc epoch：25        max acc：0.772\n",
      "epoch:  26 train_loss:  0.378 train_acc:  0.798 test_loss:  0.431 test_acc:  0.772\n",
      "min loss epoch:26\n",
      "epoch:  27 train_loss:  0.375 train_acc:  0.8 test_loss:  0.429 test_acc:  0.775\n",
      "min loss epoch:27\n",
      "max acc epoch：27        max acc：0.775\n",
      "epoch:  28 train_loss:  0.373 train_acc:  0.804 test_loss:  0.427 test_acc:  0.781\n",
      "min loss epoch:28\n",
      "max acc epoch：28        max acc：0.781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  29 train_loss:  0.371 train_acc:  0.805 test_loss:  0.425 test_acc:  0.779\n",
      "min loss epoch:29\n",
      "epoch:  30 train_loss:  0.369 train_acc:  0.807 test_loss:  0.424 test_acc:  0.781\n",
      "min loss epoch:30\n",
      "epoch:  31 train_loss:  0.368 train_acc:  0.807 test_loss:  0.424 test_acc:  0.778\n",
      "min loss epoch:31\n",
      "epoch:  32 train_loss:  0.366 train_acc:  0.807 test_loss:  0.424 test_acc:  0.776\n",
      "min loss epoch:32\n",
      "epoch:  33 train_loss:  0.364 train_acc:  0.809 test_loss:  0.422 test_acc:  0.778\n",
      "min loss epoch:33\n",
      "epoch:  34 train_loss:  0.363 train_acc:  0.811 test_loss:  0.422 test_acc:  0.775\n",
      "min loss epoch:34\n",
      "epoch:  35 train_loss:  0.361 train_acc:  0.811 test_loss:  0.421 test_acc:  0.775\n",
      "min loss epoch:35\n",
      "epoch:  36 train_loss:  0.36 train_acc:  0.814 test_loss:  0.421 test_acc:  0.777\n",
      "min loss epoch:36\n",
      "epoch:  37 train_loss:  0.358 train_acc:  0.813 test_loss:  0.42 test_acc:  0.779\n",
      "min loss epoch:37\n",
      "epoch:  38 train_loss:  0.357 train_acc:  0.814 test_loss:  0.419 test_acc:  0.781\n",
      "min loss epoch:38\n",
      "epoch:  39 train_loss:  0.356 train_acc:  0.815 test_loss:  0.418 test_acc:  0.779\n",
      "min loss epoch:39\n",
      "epoch:  40 train_loss:  0.354 train_acc:  0.815 test_loss:  0.418 test_acc:  0.779\n",
      "min loss epoch:40\n",
      "epoch:  41 train_loss:  0.353 train_acc:  0.815 test_loss:  0.418 test_acc:  0.779\n",
      "min loss epoch:41\n",
      "epoch:  42 train_loss:  0.352 train_acc:  0.816 test_loss:  0.417 test_acc:  0.778\n",
      "min loss epoch:42\n",
      "epoch:  43 train_loss:  0.351 train_acc:  0.817 test_loss:  0.417 test_acc:  0.783\n",
      "min loss epoch:43\n",
      "max acc epoch：43        max acc：0.783\n",
      "epoch:  44 train_loss:  0.35 train_acc:  0.818 test_loss:  0.416 test_acc:  0.781\n",
      "min loss epoch:44\n",
      "epoch:  45 train_loss:  0.349 train_acc:  0.818 test_loss:  0.416 test_acc:  0.787\n",
      "min loss epoch:45\n",
      "max acc epoch：45        max acc：0.787\n",
      "epoch:  46 train_loss:  0.347 train_acc:  0.82 test_loss:  0.414 test_acc:  0.785\n",
      "min loss epoch:46\n",
      "epoch:  47 train_loss:  0.346 train_acc:  0.82 test_loss:  0.415 test_acc:  0.783\n",
      "epoch:  48 train_loss:  0.345 train_acc:  0.821 test_loss:  0.414 test_acc:  0.787\n",
      "min loss epoch:48\n",
      "epoch:  49 train_loss:  0.344 train_acc:  0.823 test_loss:  0.414 test_acc:  0.787\n",
      "min loss epoch:49\n",
      "epoch:  50 train_loss:  0.343 train_acc:  0.824 test_loss:  0.414 test_acc:  0.785\n",
      "epoch:  51 train_loss:  0.342 train_acc:  0.825 test_loss:  0.414 test_acc:  0.787\n",
      "epoch:  52 train_loss:  0.341 train_acc:  0.825 test_loss:  0.413 test_acc:  0.787\n",
      "min loss epoch:52\n",
      "epoch:  53 train_loss:  0.34 train_acc:  0.827 test_loss:  0.412 test_acc:  0.787\n",
      "min loss epoch:53\n",
      "epoch:  54 train_loss:  0.338 train_acc:  0.827 test_loss:  0.41 test_acc:  0.789\n",
      "min loss epoch:54\n",
      "max acc epoch：54        max acc：0.789\n",
      "epoch:  55 train_loss:  0.336 train_acc:  0.829 test_loss:  0.409 test_acc:  0.787\n",
      "min loss epoch:55\n",
      "epoch:  56 train_loss:  0.335 train_acc:  0.83 test_loss:  0.407 test_acc:  0.787\n",
      "min loss epoch:56\n",
      "epoch:  57 train_loss:  0.337 train_acc:  0.827 test_loss:  0.41 test_acc:  0.785\n",
      "epoch:  58 train_loss:  0.336 train_acc:  0.828 test_loss:  0.41 test_acc:  0.781\n",
      "epoch:  59 train_loss:  0.334 train_acc:  0.831 test_loss:  0.407 test_acc:  0.785\n",
      "epoch:  60 train_loss:  0.329 train_acc:  0.835 test_loss:  0.404 test_acc:  0.788\n",
      "min loss epoch:60\n",
      "epoch:  61 train_loss:  0.329 train_acc:  0.835 test_loss:  0.403 test_acc:  0.787\n",
      "min loss epoch:61\n",
      "epoch:  62 train_loss:  0.329 train_acc:  0.832 test_loss:  0.403 test_acc:  0.787\n",
      "epoch:  63 train_loss:  0.328 train_acc:  0.835 test_loss:  0.403 test_acc:  0.787\n",
      "epoch:  64 train_loss:  0.325 train_acc:  0.836 test_loss:  0.402 test_acc:  0.79\n",
      "min loss epoch:64\n",
      "max acc epoch：64        max acc：0.79\n",
      "epoch:  65 train_loss:  0.323 train_acc:  0.836 test_loss:  0.403 test_acc:  0.79\n",
      "epoch:  66 train_loss:  0.322 train_acc:  0.838 test_loss:  0.402 test_acc:  0.79\n",
      "min loss epoch:66\n",
      "epoch:  67 train_loss:  0.321 train_acc:  0.838 test_loss:  0.402 test_acc:  0.791\n",
      "min loss epoch:67\n",
      "max acc epoch：67        max acc：0.791\n",
      "epoch:  68 train_loss:  0.32 train_acc:  0.837 test_loss:  0.401 test_acc:  0.792\n",
      "min loss epoch:68\n",
      "max acc epoch：68        max acc：0.792\n",
      "epoch:  69 train_loss:  0.317 train_acc:  0.84 test_loss:  0.397 test_acc:  0.792\n",
      "min loss epoch:69\n",
      "epoch:  70 train_loss:  0.317 train_acc:  0.839 test_loss:  0.399 test_acc:  0.793\n",
      "max acc epoch：70        max acc：0.793\n",
      "epoch:  71 train_loss:  0.315 train_acc:  0.843 test_loss:  0.395 test_acc:  0.794\n",
      "min loss epoch:71\n",
      "max acc epoch：71        max acc：0.794\n",
      "epoch:  72 train_loss:  0.318 train_acc:  0.84 test_loss:  0.399 test_acc:  0.795\n",
      "max acc epoch：72        max acc：0.795\n",
      "epoch:  73 train_loss:  0.312 train_acc:  0.844 test_loss:  0.393 test_acc:  0.795\n",
      "min loss epoch:73\n",
      "epoch:  74 train_loss:  0.31 train_acc:  0.844 test_loss:  0.391 test_acc:  0.796\n",
      "min loss epoch:74\n",
      "max acc epoch：74        max acc：0.796\n",
      "epoch:  75 train_loss:  0.308 train_acc:  0.845 test_loss:  0.388 test_acc:  0.794\n",
      "min loss epoch:75\n",
      "epoch:  76 train_loss:  0.308 train_acc:  0.845 test_loss:  0.389 test_acc:  0.792\n",
      "epoch:  77 train_loss:  0.306 train_acc:  0.845 test_loss:  0.386 test_acc:  0.796\n",
      "min loss epoch:77\n",
      "epoch:  78 train_loss:  0.305 train_acc:  0.845 test_loss:  0.387 test_acc:  0.793\n",
      "epoch:  79 train_loss:  0.305 train_acc:  0.847 test_loss:  0.388 test_acc:  0.794\n",
      "epoch:  80 train_loss:  0.303 train_acc:  0.848 test_loss:  0.385 test_acc:  0.796\n",
      "min loss epoch:80\n",
      "epoch:  81 train_loss:  0.3 train_acc:  0.849 test_loss:  0.38 test_acc:  0.804\n",
      "min loss epoch:81\n",
      "max acc epoch：81        max acc：0.804\n",
      "epoch:  82 train_loss:  0.299 train_acc:  0.85 test_loss:  0.379 test_acc:  0.802\n",
      "min loss epoch:82\n",
      "epoch:  83 train_loss:  0.298 train_acc:  0.852 test_loss:  0.378 test_acc:  0.803\n",
      "min loss epoch:83\n",
      "epoch:  84 train_loss:  0.297 train_acc:  0.852 test_loss:  0.378 test_acc:  0.804\n",
      "epoch:  85 train_loss:  0.294 train_acc:  0.855 test_loss:  0.373 test_acc:  0.807\n",
      "min loss epoch:85\n",
      "max acc epoch：85        max acc：0.807\n",
      "epoch:  86 train_loss:  0.293 train_acc:  0.857 test_loss:  0.371 test_acc:  0.808\n",
      "min loss epoch:86\n",
      "max acc epoch：86        max acc：0.808\n",
      "epoch:  87 train_loss:  0.291 train_acc:  0.859 test_loss:  0.37 test_acc:  0.809\n",
      "min loss epoch:87\n",
      "max acc epoch：87        max acc：0.809\n",
      "epoch:  88 train_loss:  0.29 train_acc:  0.859 test_loss:  0.371 test_acc:  0.81\n",
      "max acc epoch：88        max acc：0.81\n",
      "epoch:  89 train_loss:  0.29 train_acc:  0.859 test_loss:  0.371 test_acc:  0.809\n",
      "epoch:  90 train_loss:  0.29 train_acc:  0.86 test_loss:  0.372 test_acc:  0.811\n",
      "max acc epoch：90        max acc：0.811\n",
      "epoch:  91 train_loss:  0.29 train_acc:  0.859 test_loss:  0.372 test_acc:  0.812\n",
      "max acc epoch：91        max acc：0.812\n",
      "epoch:  92 train_loss:  0.285 train_acc:  0.859 test_loss:  0.367 test_acc:  0.818\n",
      "min loss epoch:92\n",
      "max acc epoch：92        max acc：0.818\n",
      "epoch:  93 train_loss:  0.285 train_acc:  0.86 test_loss:  0.367 test_acc:  0.819\n",
      "max acc epoch：93        max acc：0.819\n",
      "epoch:  94 train_loss:  0.284 train_acc:  0.86 test_loss:  0.366 test_acc:  0.82\n",
      "min loss epoch:94\n",
      "max acc epoch：94        max acc：0.82\n",
      "epoch:  95 train_loss:  0.282 train_acc:  0.861 test_loss:  0.365 test_acc:  0.822\n",
      "min loss epoch:95\n",
      "max acc epoch：95        max acc：0.822\n",
      "epoch:  96 train_loss:  0.281 train_acc:  0.861 test_loss:  0.362 test_acc:  0.825\n",
      "min loss epoch:96\n",
      "max acc epoch：96        max acc：0.825\n",
      "epoch:  97 train_loss:  0.28 train_acc:  0.863 test_loss:  0.361 test_acc:  0.825\n",
      "min loss epoch:97\n",
      "epoch:  98 train_loss:  0.278 train_acc:  0.864 test_loss:  0.36 test_acc:  0.825\n",
      "min loss epoch:98\n",
      "epoch:  99 train_loss:  0.277 train_acc:  0.863 test_loss:  0.361 test_acc:  0.824\n",
      "End max acc epoch：99        max acc：0.825\n",
      "['keypoint', 'brightness', 'contrast', 'edgeLength', 'Hue1', 'Hue2', 'Hue3', 'Hue4', 'Hue5', 'Hue6', 'Hue7', 'Saturation']\n",
      "6697\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([1340, 12])\n",
      "epoch:  0 train_loss:  0.465 train_acc:  0.754 test_loss:  0.485 test_acc:  0.744\n",
      "min loss epoch:0\n",
      "max acc epoch：0        max acc：0.744\n",
      "epoch:  1 train_loss:  0.451 train_acc:  0.759 test_loss:  0.476 test_acc:  0.74\n",
      "min loss epoch:1\n",
      "epoch:  2 train_loss:  0.443 train_acc:  0.765 test_loss:  0.47 test_acc:  0.743\n",
      "min loss epoch:2\n",
      "epoch:  3 train_loss:  0.436 train_acc:  0.771 test_loss:  0.466 test_acc:  0.742\n",
      "min loss epoch:3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  4 train_loss:  0.43 train_acc:  0.775 test_loss:  0.461 test_acc:  0.749\n",
      "min loss epoch:4\n",
      "max acc epoch：4        max acc：0.749\n",
      "epoch:  5 train_loss:  0.424 train_acc:  0.777 test_loss:  0.456 test_acc:  0.751\n",
      "min loss epoch:5\n",
      "max acc epoch：5        max acc：0.751\n",
      "epoch:  6 train_loss:  0.419 train_acc:  0.782 test_loss:  0.453 test_acc:  0.754\n",
      "min loss epoch:6\n",
      "max acc epoch：6        max acc：0.754\n",
      "epoch:  7 train_loss:  0.415 train_acc:  0.786 test_loss:  0.45 test_acc:  0.757\n",
      "min loss epoch:7\n",
      "max acc epoch：7        max acc：0.757\n",
      "epoch:  8 train_loss:  0.412 train_acc:  0.789 test_loss:  0.447 test_acc:  0.763\n",
      "min loss epoch:8\n",
      "max acc epoch：8        max acc：0.763\n",
      "epoch:  9 train_loss:  0.408 train_acc:  0.79 test_loss:  0.444 test_acc:  0.768\n",
      "min loss epoch:9\n",
      "max acc epoch：9        max acc：0.768\n",
      "epoch:  10 train_loss:  0.405 train_acc:  0.793 test_loss:  0.441 test_acc:  0.769\n",
      "min loss epoch:10\n",
      "max acc epoch：10        max acc：0.769\n",
      "epoch:  11 train_loss:  0.402 train_acc:  0.793 test_loss:  0.439 test_acc:  0.772\n",
      "min loss epoch:11\n",
      "max acc epoch：11        max acc：0.772\n",
      "epoch:  12 train_loss:  0.399 train_acc:  0.796 test_loss:  0.437 test_acc:  0.774\n",
      "min loss epoch:12\n",
      "max acc epoch：12        max acc：0.774\n",
      "epoch:  13 train_loss:  0.396 train_acc:  0.795 test_loss:  0.435 test_acc:  0.777\n",
      "min loss epoch:13\n",
      "max acc epoch：13        max acc：0.777\n",
      "epoch:  14 train_loss:  0.393 train_acc:  0.795 test_loss:  0.433 test_acc:  0.778\n",
      "min loss epoch:14\n",
      "max acc epoch：14        max acc：0.778\n",
      "epoch:  15 train_loss:  0.39 train_acc:  0.797 test_loss:  0.431 test_acc:  0.78\n",
      "min loss epoch:15\n",
      "max acc epoch：15        max acc：0.78\n",
      "epoch:  16 train_loss:  0.388 train_acc:  0.796 test_loss:  0.429 test_acc:  0.784\n",
      "min loss epoch:16\n",
      "max acc epoch：16        max acc：0.784\n",
      "epoch:  17 train_loss:  0.385 train_acc:  0.799 test_loss:  0.427 test_acc:  0.782\n",
      "min loss epoch:17\n",
      "epoch:  18 train_loss:  0.382 train_acc:  0.8 test_loss:  0.425 test_acc:  0.782\n",
      "min loss epoch:18\n",
      "epoch:  19 train_loss:  0.38 train_acc:  0.802 test_loss:  0.423 test_acc:  0.784\n",
      "min loss epoch:19\n",
      "epoch:  20 train_loss:  0.377 train_acc:  0.804 test_loss:  0.422 test_acc:  0.784\n",
      "min loss epoch:20\n",
      "epoch:  21 train_loss:  0.375 train_acc:  0.805 test_loss:  0.42 test_acc:  0.786\n",
      "min loss epoch:21\n",
      "max acc epoch：21        max acc：0.786\n",
      "epoch:  22 train_loss:  0.373 train_acc:  0.805 test_loss:  0.418 test_acc:  0.787\n",
      "min loss epoch:22\n",
      "max acc epoch：22        max acc：0.787\n",
      "epoch:  23 train_loss:  0.371 train_acc:  0.807 test_loss:  0.417 test_acc:  0.789\n",
      "min loss epoch:23\n",
      "max acc epoch：23        max acc：0.789\n",
      "epoch:  24 train_loss:  0.369 train_acc:  0.808 test_loss:  0.415 test_acc:  0.79\n",
      "min loss epoch:24\n",
      "max acc epoch：24        max acc：0.79\n",
      "epoch:  25 train_loss:  0.367 train_acc:  0.807 test_loss:  0.413 test_acc:  0.793\n",
      "min loss epoch:25\n",
      "max acc epoch：25        max acc：0.793\n",
      "epoch:  26 train_loss:  0.365 train_acc:  0.809 test_loss:  0.411 test_acc:  0.799\n",
      "min loss epoch:26\n",
      "max acc epoch：26        max acc：0.799\n",
      "epoch:  27 train_loss:  0.363 train_acc:  0.812 test_loss:  0.41 test_acc:  0.798\n",
      "min loss epoch:27\n",
      "epoch:  28 train_loss:  0.361 train_acc:  0.814 test_loss:  0.409 test_acc:  0.798\n",
      "min loss epoch:28\n",
      "epoch:  29 train_loss:  0.359 train_acc:  0.815 test_loss:  0.407 test_acc:  0.796\n",
      "min loss epoch:29\n",
      "epoch:  30 train_loss:  0.357 train_acc:  0.814 test_loss:  0.406 test_acc:  0.796\n",
      "min loss epoch:30\n",
      "epoch:  31 train_loss:  0.355 train_acc:  0.816 test_loss:  0.405 test_acc:  0.798\n",
      "min loss epoch:31\n",
      "epoch:  32 train_loss:  0.353 train_acc:  0.816 test_loss:  0.403 test_acc:  0.796\n",
      "min loss epoch:32\n",
      "epoch:  33 train_loss:  0.351 train_acc:  0.818 test_loss:  0.402 test_acc:  0.799\n",
      "min loss epoch:33\n",
      "epoch:  34 train_loss:  0.35 train_acc:  0.819 test_loss:  0.401 test_acc:  0.799\n",
      "min loss epoch:34\n",
      "epoch:  35 train_loss:  0.348 train_acc:  0.821 test_loss:  0.4 test_acc:  0.8\n",
      "min loss epoch:35\n",
      "max acc epoch：35        max acc：0.8\n",
      "epoch:  36 train_loss:  0.346 train_acc:  0.824 test_loss:  0.399 test_acc:  0.8\n",
      "min loss epoch:36\n",
      "epoch:  37 train_loss:  0.345 train_acc:  0.824 test_loss:  0.397 test_acc:  0.804\n",
      "min loss epoch:37\n",
      "max acc epoch：37        max acc：0.804\n",
      "epoch:  38 train_loss:  0.343 train_acc:  0.826 test_loss:  0.396 test_acc:  0.802\n",
      "min loss epoch:38\n",
      "epoch:  39 train_loss:  0.342 train_acc:  0.827 test_loss:  0.395 test_acc:  0.804\n",
      "min loss epoch:39\n",
      "epoch:  40 train_loss:  0.34 train_acc:  0.829 test_loss:  0.393 test_acc:  0.802\n",
      "min loss epoch:40\n",
      "epoch:  41 train_loss:  0.338 train_acc:  0.829 test_loss:  0.392 test_acc:  0.809\n",
      "min loss epoch:41\n",
      "max acc epoch：41        max acc：0.809\n",
      "epoch:  42 train_loss:  0.338 train_acc:  0.83 test_loss:  0.393 test_acc:  0.803\n",
      "epoch:  43 train_loss:  0.336 train_acc:  0.831 test_loss:  0.392 test_acc:  0.806\n",
      "min loss epoch:43\n",
      "epoch:  44 train_loss:  0.334 train_acc:  0.831 test_loss:  0.39 test_acc:  0.806\n",
      "min loss epoch:44\n",
      "epoch:  45 train_loss:  0.333 train_acc:  0.833 test_loss:  0.39 test_acc:  0.81\n",
      "min loss epoch:45\n",
      "max acc epoch：45        max acc：0.81\n",
      "epoch:  46 train_loss:  0.331 train_acc:  0.834 test_loss:  0.389 test_acc:  0.809\n",
      "min loss epoch:46\n",
      "epoch:  47 train_loss:  0.33 train_acc:  0.835 test_loss:  0.388 test_acc:  0.811\n",
      "min loss epoch:47\n",
      "max acc epoch：47        max acc：0.811\n",
      "epoch:  48 train_loss:  0.329 train_acc:  0.834 test_loss:  0.388 test_acc:  0.81\n",
      "min loss epoch:48\n",
      "epoch:  49 train_loss:  0.328 train_acc:  0.835 test_loss:  0.388 test_acc:  0.81\n",
      "min loss epoch:49\n",
      "epoch:  50 train_loss:  0.327 train_acc:  0.834 test_loss:  0.387 test_acc:  0.809\n",
      "min loss epoch:50\n",
      "epoch:  51 train_loss:  0.325 train_acc:  0.836 test_loss:  0.385 test_acc:  0.809\n",
      "min loss epoch:51\n",
      "epoch:  52 train_loss:  0.322 train_acc:  0.837 test_loss:  0.383 test_acc:  0.81\n",
      "min loss epoch:52\n",
      "epoch:  53 train_loss:  0.322 train_acc:  0.838 test_loss:  0.384 test_acc:  0.806\n",
      "epoch:  54 train_loss:  0.321 train_acc:  0.839 test_loss:  0.384 test_acc:  0.808\n",
      "epoch:  55 train_loss:  0.32 train_acc:  0.84 test_loss:  0.382 test_acc:  0.806\n",
      "min loss epoch:55\n",
      "epoch:  56 train_loss:  0.319 train_acc:  0.84 test_loss:  0.383 test_acc:  0.806\n",
      "epoch:  57 train_loss:  0.318 train_acc:  0.84 test_loss:  0.383 test_acc:  0.804\n",
      "epoch:  58 train_loss:  0.317 train_acc:  0.841 test_loss:  0.382 test_acc:  0.807\n",
      "min loss epoch:58\n",
      "epoch:  59 train_loss:  0.316 train_acc:  0.843 test_loss:  0.382 test_acc:  0.807\n",
      "min loss epoch:59\n",
      "epoch:  60 train_loss:  0.315 train_acc:  0.843 test_loss:  0.381 test_acc:  0.804\n",
      "min loss epoch:60\n",
      "epoch:  61 train_loss:  0.313 train_acc:  0.844 test_loss:  0.379 test_acc:  0.804\n",
      "min loss epoch:61\n",
      "epoch:  62 train_loss:  0.313 train_acc:  0.845 test_loss:  0.381 test_acc:  0.804\n",
      "epoch:  63 train_loss:  0.312 train_acc:  0.846 test_loss:  0.38 test_acc:  0.804\n",
      "epoch:  64 train_loss:  0.31 train_acc:  0.848 test_loss:  0.378 test_acc:  0.805\n",
      "min loss epoch:64\n",
      "epoch:  65 train_loss:  0.31 train_acc:  0.847 test_loss:  0.378 test_acc:  0.805\n",
      "epoch:  66 train_loss:  0.31 train_acc:  0.847 test_loss:  0.38 test_acc:  0.806\n",
      "epoch:  67 train_loss:  0.308 train_acc:  0.847 test_loss:  0.377 test_acc:  0.807\n",
      "min loss epoch:67\n",
      "epoch:  68 train_loss:  0.307 train_acc:  0.848 test_loss:  0.376 test_acc:  0.807\n",
      "min loss epoch:68\n",
      "epoch:  69 train_loss:  0.306 train_acc:  0.848 test_loss:  0.376 test_acc:  0.806\n",
      "min loss epoch:69\n",
      "epoch:  70 train_loss:  0.305 train_acc:  0.846 test_loss:  0.375 test_acc:  0.807\n",
      "min loss epoch:70\n",
      "epoch:  71 train_loss:  0.304 train_acc:  0.848 test_loss:  0.374 test_acc:  0.807\n",
      "min loss epoch:71\n",
      "epoch:  72 train_loss:  0.303 train_acc:  0.847 test_loss:  0.374 test_acc:  0.809\n",
      "epoch:  73 train_loss:  0.303 train_acc:  0.848 test_loss:  0.373 test_acc:  0.807\n",
      "min loss epoch:73\n",
      "epoch:  74 train_loss:  0.302 train_acc:  0.849 test_loss:  0.371 test_acc:  0.811\n",
      "min loss epoch:74\n",
      "epoch:  75 train_loss:  0.302 train_acc:  0.85 test_loss:  0.372 test_acc:  0.81\n",
      "epoch:  76 train_loss:  0.3 train_acc:  0.85 test_loss:  0.371 test_acc:  0.813\n",
      "min loss epoch:76\n",
      "max acc epoch：76        max acc：0.813\n",
      "epoch:  77 train_loss:  0.299 train_acc:  0.85 test_loss:  0.37 test_acc:  0.813\n",
      "min loss epoch:77\n",
      "epoch:  78 train_loss:  0.298 train_acc:  0.849 test_loss:  0.368 test_acc:  0.815\n",
      "min loss epoch:78\n",
      "max acc epoch：78        max acc：0.815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  79 train_loss:  0.297 train_acc:  0.851 test_loss:  0.368 test_acc:  0.813\n",
      "epoch:  80 train_loss:  0.297 train_acc:  0.85 test_loss:  0.368 test_acc:  0.813\n",
      "epoch:  81 train_loss:  0.297 train_acc:  0.851 test_loss:  0.367 test_acc:  0.814\n",
      "min loss epoch:81\n",
      "epoch:  82 train_loss:  0.295 train_acc:  0.852 test_loss:  0.366 test_acc:  0.816\n",
      "min loss epoch:82\n",
      "max acc epoch：82        max acc：0.816\n",
      "epoch:  83 train_loss:  0.295 train_acc:  0.852 test_loss:  0.366 test_acc:  0.813\n",
      "epoch:  84 train_loss:  0.295 train_acc:  0.852 test_loss:  0.367 test_acc:  0.814\n",
      "epoch:  85 train_loss:  0.293 train_acc:  0.854 test_loss:  0.365 test_acc:  0.816\n",
      "min loss epoch:85\n",
      "epoch:  86 train_loss:  0.292 train_acc:  0.852 test_loss:  0.364 test_acc:  0.816\n",
      "min loss epoch:86\n",
      "epoch:  87 train_loss:  0.291 train_acc:  0.853 test_loss:  0.364 test_acc:  0.815\n",
      "min loss epoch:87\n",
      "epoch:  88 train_loss:  0.29 train_acc:  0.854 test_loss:  0.362 test_acc:  0.816\n",
      "min loss epoch:88\n",
      "epoch:  89 train_loss:  0.289 train_acc:  0.855 test_loss:  0.362 test_acc:  0.815\n",
      "epoch:  90 train_loss:  0.288 train_acc:  0.854 test_loss:  0.361 test_acc:  0.816\n",
      "min loss epoch:90\n",
      "epoch:  91 train_loss:  0.288 train_acc:  0.856 test_loss:  0.36 test_acc:  0.819\n",
      "min loss epoch:91\n",
      "max acc epoch：91        max acc：0.819\n",
      "epoch:  92 train_loss:  0.287 train_acc:  0.855 test_loss:  0.359 test_acc:  0.821\n",
      "min loss epoch:92\n",
      "max acc epoch：92        max acc：0.821\n",
      "epoch:  93 train_loss:  0.286 train_acc:  0.856 test_loss:  0.36 test_acc:  0.817\n",
      "epoch:  94 train_loss:  0.285 train_acc:  0.857 test_loss:  0.359 test_acc:  0.821\n",
      "epoch:  95 train_loss:  0.285 train_acc:  0.857 test_loss:  0.359 test_acc:  0.819\n",
      "epoch:  96 train_loss:  0.284 train_acc:  0.857 test_loss:  0.359 test_acc:  0.816\n",
      "epoch:  97 train_loss:  0.285 train_acc:  0.857 test_loss:  0.36 test_acc:  0.818\n",
      "epoch:  98 train_loss:  0.283 train_acc:  0.858 test_loss:  0.36 test_acc:  0.819\n",
      "epoch:  99 train_loss:  0.283 train_acc:  0.857 test_loss:  0.359 test_acc:  0.818\n",
      "End max acc epoch：99        max acc：0.821\n",
      "[0.819, 0.809, 0.834, 0.806, 0.819, 0.817, 0.813, 0.8, 0.822, 0.807, 0.832, 0.825, 0.821]\n"
     ]
    }
   ],
   "source": [
    "#批量处理patch预测的特征重要性\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt   \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import torch.nn.functional as Fun\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import preprocessing\n",
    "\n",
    "ACC=[]\n",
    "for i in range(13):\n",
    "    N=['keypoint','brightness','contrast','edgeLength',\n",
    "       'Hue1','Hue2','Hue3','Hue4',\t'Hue5',\t'Hue6',\t'Hue7',\t\n",
    "       'Saturation','Value']\n",
    "    del N[i]\n",
    "    print(N)\n",
    "    data = pd.read_csv(\"H:/edge_dection/feature_test.csv\")########## \n",
    "    X = data[N].values\n",
    "    X = preprocessing.QuantileTransformer(output_distribution=\"normal\").fit_transform(X)\n",
    "    print(len(X))\n",
    "    Y = data.label.values.reshape(-1, 1)#######3\n",
    "\n",
    "    train_x,test_x,train_y,test_y=train_test_split(X,Y,train_size=0.8,random_state=1)  #shuffle=False 就是按照顺序划分的测试集和验证集,默认为true才行\n",
    "    print(type(train_x))\n",
    "\n",
    "    #将数据转换成Tensor LongTensor等价于int64\n",
    "    train_x = torch.from_numpy(train_x).type(torch.FloatTensor)\n",
    "    train_y = torch.from_numpy(train_y).type(torch.FloatTensor)\n",
    "    test_x  =torch.from_numpy(test_x).type(torch.FloatTensor)\n",
    "    test_y  = torch.from_numpy(test_y).type(torch.FloatTensor)\n",
    "    print(test_x.shape)\n",
    "    points = int(test_x.shape[1])\n",
    "\n",
    "    batch = 8#32\n",
    "    #no_of_batches = len(data)//batch\n",
    "    epochs = 100#3000\n",
    "\n",
    "    #TensorDataset()可以对tensor进行打包即合并\n",
    "    train_ds = TensorDataset(train_x,train_y)\n",
    "    #希望模型不关注训练集数据顺序故用乱序\n",
    "    train_dl = DataLoader(train_ds,batch_size=batch)#,shuffle=True\n",
    "    test_ds = TensorDataset(test_x,test_y)\n",
    "    #对测试集不需要用乱序避免工作量增加\n",
    "    test_dl = DataLoader(test_ds,batch_size=batch)\n",
    "\n",
    "    # 建立神经网络\n",
    "    class Net(torch.nn.Module):     # 继承 torch 的 Module\n",
    "        def __init__(self, n_feature):\n",
    "            super(Net, self).__init__()     # 继承 __init__ 功能\n",
    "            self.hidden1 = torch.nn.Linear(n_feature, int(n_feature*2))   # 隐藏层线性输出\n",
    "            self.hidden4 = torch.nn.Linear(int(n_feature*2), int(n_feature))   # 隐藏层线性输出\n",
    "            self.out = torch.nn.Linear(int(n_feature),1)       # 输出层线性输出\n",
    "            self.relu = nn.ReLU()  # 模块的激活函数\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # 正向传播输入值, 神经网络分析出输出值\n",
    "            x = self.relu(self.hidden1(x))   # 激励函数(隐藏层的线性值)\n",
    "            x = self.relu(self.hidden4(x))\n",
    "            x = torch.sigmoid(self.out(x)) \n",
    "            return x\n",
    "\n",
    "    #便于随着训练的进行观察数值的变化\n",
    "    train_loss=[]\n",
    "    test_loss=[]\n",
    "    min_loss=1\n",
    "    max_acc=0.1\n",
    "\n",
    "    def trans(pred):\n",
    "        P=[]\n",
    "        for i  in pred:\n",
    "            if i>=0.5:\n",
    "                i=[1]\n",
    "            else:\n",
    "                i=[0]\n",
    "            P.append(i)\n",
    "        pred = torch.Tensor(P)\n",
    "        return pred\n",
    "\n",
    "    def accury(pred,y):\n",
    "        pred = trans(pred)\n",
    "        return sum(pred == y)/len(y)\n",
    "\n",
    "    #获得这个模型\n",
    "    model = Net(n_feature=points)\n",
    "    #优化函数 优化的是模型所有变量即model.parameters()\n",
    "    optim = torch.optim.Adam(model.parameters(),lr=0.001)  #lr=0.0001  #选择使用哪种优化器 761个数据集用这个优化器\n",
    "\n",
    "    # 训练网络\n",
    "    # 算误差的时候, 注意真实值!不是! one-hot 形式的, 而是1D Tensor, (batch,)\n",
    "    loss_fn =nn.BCELoss()#nn.CrossEntropyLoss()##nn.BCEWithLogitsLoss()# #\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for x,y in train_dl:\n",
    "            y_pred = model(x)\n",
    "            #print(y_pred,y)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            #print(loss)\n",
    "            # 梯度置为0\n",
    "            optim.zero_grad()\n",
    "            # 反向传播求解梯度\n",
    "            loss.backward()\n",
    "            # 优化\n",
    "            optim.step()\n",
    "        \n",
    "        # 不需要进行梯度计算\n",
    "        with torch.no_grad():\n",
    "            epoch_loss = loss_fn(model(train_x), train_y).data\n",
    "            epoch_acc = accury(model(train_x), train_y).numpy()\n",
    "            #print(epoch_acc)\n",
    "            epoch_test_loss = loss_fn(model(test_x), test_y).data\n",
    "            epoch_test_acc = accury(model(test_x), test_y).numpy()\n",
    "            #print(epoch_acc,sum(epoch_acc),len(epoch_acc),sum(epoch_acc)/len(epoch_acc))\n",
    "            print('epoch: ',epoch,'train_loss: ',round(epoch_loss.item(),3),'train_acc: ', round(float(sum(epoch_acc)/len(epoch_acc)),3),\n",
    "                  'test_loss: ',round(epoch_test_loss.item(),3),'test_acc: ',round(float(sum(epoch_test_acc)/len(epoch_test_acc)),3))\n",
    "            train_loss.append(epoch_loss)\n",
    "            test_loss.append(epoch_test_loss)\n",
    "            if epoch_test_loss < min_loss:\n",
    "                min_loss = epoch_test_loss\n",
    "                print(\"min loss epoch:\"+str(epoch))\n",
    "                #torch.save(model, 'H:/vamf_model/model_vmaf.pth')\n",
    "            if round(float(sum(epoch_test_acc)/len(epoch_test_acc)),3) > max_acc:\n",
    "                max_acc = round(float(sum(epoch_test_acc)/len(epoch_test_acc)),3)\n",
    "                print(\"max acc epoch：\"+str(epoch)+\"        max acc：\"+str(max_acc))\n",
    "                \n",
    "    print(\"End max acc epoch：\"+str(epoch)+\"        max acc：\"+str(max_acc))   \n",
    "    ACC.append(max_acc)\n",
    "    \n",
    "print(ACC)\n",
    "# list转dataframe\n",
    "df = pd.DataFrame(ACC, columns=['acc'])\n",
    "# 保存到本地excel\n",
    "df.to_excel(\"F:/video_cut/patch_acc.xlsx\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b874e575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['keypoint',\n",
       " 'brightness',\n",
       " 'contrast',\n",
       " 'edgeLength',\n",
       " 'Hue1',\n",
       " 'Hue2',\n",
       " 'Hue3',\n",
       " 'Hue4',\n",
       " 'Hue5',\n",
       " 'Hue6',\n",
       " 'Hue7',\n",
       " 'Saturation',\n",
       " 'Value',\n",
       " 'TI',\n",
       " 'SI',\n",
       " 'Size']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " N=['keypoint','brightness','contrast','edgeLength',\n",
    "       'Hue1','Hue2','Hue3','Hue4',\t'Hue5',\t'Hue6',\t'Hue7',\t\n",
    "       'Saturation','Value','TI','SI','Size']\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91c941fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[685  12]\n",
      " [ 16 627]]\n",
      "accuracy:0.9791044776119403\n",
      "precision:0.9791044776119403\n",
      "recall:0.9791044776119403\n",
      "f1-score:0.9791044776119403\n",
      "97.91044776119404\n",
      "['brightness', 'contrast', 'edgeLength', 'Hue1', 'Hue2', 'Hue3', 'Hue4', 'Hue5', 'Hue6', 'Hue7', 'Saturation', 'Value', 'TI', 'SI', 'Size']\n",
      "[0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[683  14]\n",
      " [ 23 620]]\n",
      "accuracy:0.9723880597014926\n",
      "precision:0.9723880597014926\n",
      "recall:0.9723880597014926\n",
      "f1-score:0.9723880597014926\n",
      "97.23880597014926\n",
      "['keypoint', 'contrast', 'edgeLength', 'Hue1', 'Hue2', 'Hue3', 'Hue4', 'Hue5', 'Hue6', 'Hue7', 'Saturation', 'Value', 'TI', 'SI', 'Size']\n",
      "[1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[683  14]\n",
      " [ 25 618]]\n",
      "accuracy:0.9708955223880597\n",
      "precision:0.9708955223880597\n",
      "recall:0.9708955223880597\n",
      "f1-score:0.9708955223880597\n",
      "97.08955223880596\n",
      "['keypoint', 'brightness', 'edgeLength', 'Hue1', 'Hue2', 'Hue3', 'Hue4', 'Hue5', 'Hue6', 'Hue7', 'Saturation', 'Value', 'TI', 'SI', 'Size']\n",
      "[0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[689   8]\n",
      " [ 13 630]]\n",
      "accuracy:0.9843283582089553\n",
      "precision:0.9843283582089553\n",
      "recall:0.9843283582089553\n",
      "f1-score:0.9843283582089553\n",
      "98.43283582089552\n",
      "['keypoint', 'brightness', 'contrast', 'Hue1', 'Hue2', 'Hue3', 'Hue4', 'Hue5', 'Hue6', 'Hue7', 'Saturation', 'Value', 'TI', 'SI', 'Size']\n",
      "[0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[682  15]\n",
      " [ 19 624]]\n",
      "accuracy:0.9746268656716418\n",
      "precision:0.9746268656716418\n",
      "recall:0.9746268656716418\n",
      "f1-score:0.9746268656716418\n",
      "97.46268656716418\n",
      "['keypoint', 'brightness', 'contrast', 'edgeLength', 'Hue2', 'Hue3', 'Hue4', 'Hue5', 'Hue6', 'Hue7', 'Saturation', 'Value', 'TI', 'SI', 'Size']\n",
      "[0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[685  12]\n",
      " [ 18 625]]\n",
      "accuracy:0.9776119402985075\n",
      "precision:0.9776119402985075\n",
      "recall:0.9776119402985075\n",
      "f1-score:0.9776119402985075\n",
      "97.76119402985076\n",
      "['keypoint', 'brightness', 'contrast', 'edgeLength', 'Hue1', 'Hue3', 'Hue4', 'Hue5', 'Hue6', 'Hue7', 'Saturation', 'Value', 'TI', 'SI', 'Size']\n",
      "[0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[684  13]\n",
      " [ 15 628]]\n",
      "accuracy:0.9791044776119403\n",
      "precision:0.9791044776119403\n",
      "recall:0.9791044776119403\n",
      "f1-score:0.9791044776119403\n",
      "97.91044776119404\n",
      "['keypoint', 'brightness', 'contrast', 'edgeLength', 'Hue1', 'Hue2', 'Hue4', 'Hue5', 'Hue6', 'Hue7', 'Saturation', 'Value', 'TI', 'SI', 'Size']\n",
      "[0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[687  10]\n",
      " [ 14 629]]\n",
      "accuracy:0.982089552238806\n",
      "precision:0.982089552238806\n",
      "recall:0.982089552238806\n",
      "f1-score:0.982089552238806\n",
      "98.2089552238806\n",
      "['keypoint', 'brightness', 'contrast', 'edgeLength', 'Hue1', 'Hue2', 'Hue3', 'Hue5', 'Hue6', 'Hue7', 'Saturation', 'Value', 'TI', 'SI', 'Size']\n",
      "[0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[685  12]\n",
      " [ 18 625]]\n",
      "accuracy:0.9776119402985075\n",
      "precision:0.9776119402985075\n",
      "recall:0.9776119402985075\n",
      "f1-score:0.9776119402985075\n",
      "97.76119402985076\n",
      "['keypoint', 'brightness', 'contrast', 'edgeLength', 'Hue1', 'Hue2', 'Hue3', 'Hue4', 'Hue6', 'Hue7', 'Saturation', 'Value', 'TI', 'SI', 'Size']\n",
      "[0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[687  10]\n",
      " [ 14 629]]\n",
      "accuracy:0.982089552238806\n",
      "precision:0.982089552238806\n",
      "recall:0.982089552238806\n",
      "f1-score:0.982089552238806\n",
      "98.2089552238806\n",
      "['keypoint', 'brightness', 'contrast', 'edgeLength', 'Hue1', 'Hue2', 'Hue3', 'Hue4', 'Hue5', 'Hue7', 'Saturation', 'Value', 'TI', 'SI', 'Size']\n",
      "[0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[687  10]\n",
      " [ 17 626]]\n",
      "accuracy:0.9798507462686568\n",
      "precision:0.9798507462686568\n",
      "recall:0.9798507462686568\n",
      "f1-score:0.9798507462686568\n",
      "97.98507462686568\n",
      "['keypoint', 'brightness', 'contrast', 'edgeLength', 'Hue1', 'Hue2', 'Hue3', 'Hue4', 'Hue5', 'Hue6', 'Saturation', 'Value', 'TI', 'SI', 'Size']\n",
      "[0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[686  11]\n",
      " [ 16 627]]\n",
      "accuracy:0.9798507462686568\n",
      "precision:0.9798507462686568\n",
      "recall:0.9798507462686568\n",
      "f1-score:0.9798507462686568\n",
      "97.98507462686568\n",
      "['keypoint', 'brightness', 'contrast', 'edgeLength', 'Hue1', 'Hue2', 'Hue3', 'Hue4', 'Hue5', 'Hue6', 'Hue7', 'Value', 'TI', 'SI', 'Size']\n",
      "[0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[685  12]\n",
      " [ 16 627]]\n",
      "accuracy:0.9791044776119403\n",
      "precision:0.9791044776119403\n",
      "recall:0.9791044776119403\n",
      "f1-score:0.9791044776119403\n",
      "97.91044776119404\n",
      "['keypoint', 'brightness', 'contrast', 'edgeLength', 'Hue1', 'Hue2', 'Hue3', 'Hue4', 'Hue5', 'Hue6', 'Hue7', 'Saturation', 'TI', 'SI', 'Size']\n",
      "[0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[688   9]\n",
      " [ 14 629]]\n",
      "accuracy:0.9828358208955223\n",
      "precision:0.9828358208955223\n",
      "recall:0.9828358208955223\n",
      "f1-score:0.9828358208955223\n",
      "98.28358208955224\n",
      "['keypoint', 'brightness', 'contrast', 'edgeLength', 'Hue1', 'Hue2', 'Hue3', 'Hue4', 'Hue5', 'Hue6', 'Hue7', 'Saturation', 'Value', 'SI', 'Size']\n",
      "[0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[683  14]\n",
      " [ 15 628]]\n",
      "accuracy:0.9783582089552239\n",
      "precision:0.9783582089552239\n",
      "recall:0.9783582089552239\n",
      "f1-score:0.9783582089552239\n",
      "97.83582089552239\n",
      "['keypoint', 'brightness', 'contrast', 'edgeLength', 'Hue1', 'Hue2', 'Hue3', 'Hue4', 'Hue5', 'Hue6', 'Hue7', 'Saturation', 'Value', 'TI', 'Size']\n",
      "[0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[672  25]\n",
      " [ 40 603]]\n",
      "accuracy:0.9514925373134329\n",
      "precision:0.9514925373134329\n",
      "recall:0.9514925373134329\n",
      "f1-score:0.9514925373134329\n",
      "95.1492537313433\n",
      "['keypoint', 'brightness', 'contrast', 'edgeLength', 'Hue1', 'Hue2', 'Hue3', 'Hue4', 'Hue5', 'Hue6', 'Hue7', 'Saturation', 'Value', 'TI', 'SI']\n",
      "[0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1] [0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "confusion_matrix:\n",
      " [[683  14]\n",
      " [ 18 625]]\n",
      "accuracy:0.9761194029850746\n",
      "precision:0.9761194029850746\n",
      "recall:0.9761194029850746\n",
      "f1-score:0.9761194029850746\n",
      "97.61194029850746\n",
      "[97.91044776119404, 97.23880597014926, 97.08955223880596, 98.43283582089552, 97.46268656716418, 97.76119402985076, 97.91044776119404, 98.2089552238806, 97.76119402985076, 98.2089552238806, 97.98507462686568, 97.98507462686568, 97.91044776119404, 98.28358208955224, 97.83582089552239, 95.1492537313433, 97.61194029850746]\n"
     ]
    }
   ],
   "source": [
    "#批量处理patch预测的特征重要性\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt   \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import torch.nn.functional as Fun\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "\n",
    "# 训练决策树\n",
    "def trainDT(x_train, y_train):\n",
    "    # DT生成和训练\n",
    "    clf = tree.DecisionTreeClassifier(criterion='entropy')#ID3分类树，信息增益特征选择\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf\n",
    "\n",
    "ACC=[]\n",
    "for i in range(-1,16):\n",
    "    N=['keypoint','brightness','contrast','edgeLength',\n",
    "       'Hue1','Hue2','Hue3','Hue4',\t'Hue5',\t'Hue6',\t'Hue7',\t\n",
    "       'Saturation','Value','TI','SI','Size']\n",
    "    if i != -1:\n",
    "        del N[i]\n",
    "        print(N)\n",
    "    data = pd.read_excel(\"H:/edge_dection/feature_test.xlsx\")########## \n",
    "    X = data[N].values\n",
    "    X = preprocessing.QuantileTransformer(output_distribution=\"normal\").fit_transform(X)\n",
    "    #print(len(X))\n",
    "    Y = data.label.values.reshape(-1, 1)#######3\n",
    "    \n",
    "    train_x,test_x,train_y,test_y=train_test_split(X,Y,train_size=0.8,random_state=10)  #shuffle=False 就是按照顺序划分的测试集和验证集,默认为true才行\n",
    "\n",
    "    model = trainDT(train_x, train_y.ravel())\n",
    "    test(model,test_x, test_y.ravel())\n",
    "    acc = accury(model,test_x,test_y.ravel())\n",
    "    ACC.append(acc)\n",
    "print(ACC)\n",
    "# list转dataframe\n",
    "df = pd.DataFrame(ACC, columns=['acc'])\n",
    "# 保存到本地excel\n",
    "df.to_excel(\"F:/video_cut/patch_tree_acc.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b63769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91c0498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_envs]",
   "language": "python",
   "name": "conda-env-pytorch_envs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
