{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "448c32b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_frame</th>\n",
       "      <th>TI</th>\n",
       "      <th>SI</th>\n",
       "      <th>keypoint</th>\n",
       "      <th>brightness</th>\n",
       "      <th>contrast</th>\n",
       "      <th>edgeLength</th>\n",
       "      <th>Hue1</th>\n",
       "      <th>Hue2</th>\n",
       "      <th>Hue3</th>\n",
       "      <th>Hue4</th>\n",
       "      <th>Hue5</th>\n",
       "      <th>Hue6</th>\n",
       "      <th>Hue7</th>\n",
       "      <th>Saturation</th>\n",
       "      <th>Value</th>\n",
       "      <th>video4</th>\n",
       "      <th>video6</th>\n",
       "      <th>video8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15563.023440</td>\n",
       "      <td>19.995989</td>\n",
       "      <td>99.299255</td>\n",
       "      <td>96335.700000</td>\n",
       "      <td>131.093257</td>\n",
       "      <td>50.075259</td>\n",
       "      <td>1.639670e+06</td>\n",
       "      <td>48.885553</td>\n",
       "      <td>38.815013</td>\n",
       "      <td>26.810291</td>\n",
       "      <td>60.863760</td>\n",
       "      <td>43.180126</td>\n",
       "      <td>46.297540</td>\n",
       "      <td>146.223288</td>\n",
       "      <td>106.916600</td>\n",
       "      <td>142.922966</td>\n",
       "      <td>514.630859</td>\n",
       "      <td>219.875000</td>\n",
       "      <td>117.837891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16130.993160</td>\n",
       "      <td>19.377771</td>\n",
       "      <td>98.798615</td>\n",
       "      <td>92503.700000</td>\n",
       "      <td>133.617967</td>\n",
       "      <td>50.388930</td>\n",
       "      <td>1.613227e+06</td>\n",
       "      <td>49.089428</td>\n",
       "      <td>38.933680</td>\n",
       "      <td>26.928219</td>\n",
       "      <td>60.957078</td>\n",
       "      <td>43.079494</td>\n",
       "      <td>46.735909</td>\n",
       "      <td>145.862168</td>\n",
       "      <td>105.498921</td>\n",
       "      <td>145.540919</td>\n",
       "      <td>490.655273</td>\n",
       "      <td>217.898438</td>\n",
       "      <td>117.535156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16350.106450</td>\n",
       "      <td>18.683294</td>\n",
       "      <td>100.835434</td>\n",
       "      <td>93543.133330</td>\n",
       "      <td>134.556517</td>\n",
       "      <td>50.716521</td>\n",
       "      <td>1.642275e+06</td>\n",
       "      <td>49.262484</td>\n",
       "      <td>39.039213</td>\n",
       "      <td>26.972903</td>\n",
       "      <td>61.036619</td>\n",
       "      <td>42.962030</td>\n",
       "      <td>46.577120</td>\n",
       "      <td>145.752035</td>\n",
       "      <td>105.739884</td>\n",
       "      <td>146.580133</td>\n",
       "      <td>496.205078</td>\n",
       "      <td>214.589844</td>\n",
       "      <td>119.447266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16235.444340</td>\n",
       "      <td>17.698383</td>\n",
       "      <td>99.843338</td>\n",
       "      <td>93216.166670</td>\n",
       "      <td>134.781613</td>\n",
       "      <td>50.695966</td>\n",
       "      <td>1.640041e+06</td>\n",
       "      <td>48.996321</td>\n",
       "      <td>38.919626</td>\n",
       "      <td>26.879124</td>\n",
       "      <td>60.873019</td>\n",
       "      <td>43.184083</td>\n",
       "      <td>47.113937</td>\n",
       "      <td>145.922089</td>\n",
       "      <td>105.424799</td>\n",
       "      <td>146.794137</td>\n",
       "      <td>478.143555</td>\n",
       "      <td>207.558594</td>\n",
       "      <td>112.416992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16325.968750</td>\n",
       "      <td>16.584223</td>\n",
       "      <td>101.031555</td>\n",
       "      <td>93978.700000</td>\n",
       "      <td>134.750404</td>\n",
       "      <td>50.730587</td>\n",
       "      <td>1.645418e+06</td>\n",
       "      <td>49.144942</td>\n",
       "      <td>39.006811</td>\n",
       "      <td>26.959807</td>\n",
       "      <td>60.987993</td>\n",
       "      <td>43.106776</td>\n",
       "      <td>47.020205</td>\n",
       "      <td>145.813482</td>\n",
       "      <td>105.484641</td>\n",
       "      <td>146.764501</td>\n",
       "      <td>496.059570</td>\n",
       "      <td>214.871094</td>\n",
       "      <td>118.107422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>8096.879883</td>\n",
       "      <td>5.340825</td>\n",
       "      <td>12.266477</td>\n",
       "      <td>1497.200000</td>\n",
       "      <td>149.970529</td>\n",
       "      <td>58.319979</td>\n",
       "      <td>3.536846e+03</td>\n",
       "      <td>49.121758</td>\n",
       "      <td>83.634994</td>\n",
       "      <td>74.440953</td>\n",
       "      <td>45.339770</td>\n",
       "      <td>126.207614</td>\n",
       "      <td>119.436579</td>\n",
       "      <td>91.507964</td>\n",
       "      <td>57.965439</td>\n",
       "      <td>168.945742</td>\n",
       "      <td>155.458984</td>\n",
       "      <td>89.447266</td>\n",
       "      <td>60.113281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>7892.939453</td>\n",
       "      <td>5.826466</td>\n",
       "      <td>12.201683</td>\n",
       "      <td>1499.233333</td>\n",
       "      <td>144.369581</td>\n",
       "      <td>58.301420</td>\n",
       "      <td>1.453691e+03</td>\n",
       "      <td>59.429101</td>\n",
       "      <td>91.880945</td>\n",
       "      <td>79.875389</td>\n",
       "      <td>50.228180</td>\n",
       "      <td>128.664562</td>\n",
       "      <td>112.233985</td>\n",
       "      <td>83.960095</td>\n",
       "      <td>59.494577</td>\n",
       "      <td>164.329315</td>\n",
       "      <td>168.297852</td>\n",
       "      <td>97.144531</td>\n",
       "      <td>65.839844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>8608.088867</td>\n",
       "      <td>5.509458</td>\n",
       "      <td>12.686021</td>\n",
       "      <td>1447.033333</td>\n",
       "      <td>144.804492</td>\n",
       "      <td>57.764878</td>\n",
       "      <td>1.485674e+03</td>\n",
       "      <td>60.006962</td>\n",
       "      <td>92.120943</td>\n",
       "      <td>80.231870</td>\n",
       "      <td>50.551533</td>\n",
       "      <td>128.065475</td>\n",
       "      <td>111.920640</td>\n",
       "      <td>83.528597</td>\n",
       "      <td>59.350706</td>\n",
       "      <td>164.900697</td>\n",
       "      <td>151.416016</td>\n",
       "      <td>88.652344</td>\n",
       "      <td>60.095703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>7857.112305</td>\n",
       "      <td>4.280614</td>\n",
       "      <td>10.780957</td>\n",
       "      <td>1291.200000</td>\n",
       "      <td>154.923462</td>\n",
       "      <td>57.141601</td>\n",
       "      <td>1.068472e+03</td>\n",
       "      <td>50.197428</td>\n",
       "      <td>85.221489</td>\n",
       "      <td>75.486053</td>\n",
       "      <td>45.846897</td>\n",
       "      <td>128.626873</td>\n",
       "      <td>118.785155</td>\n",
       "      <td>90.293822</td>\n",
       "      <td>53.928743</td>\n",
       "      <td>173.768869</td>\n",
       "      <td>147.665039</td>\n",
       "      <td>85.440430</td>\n",
       "      <td>57.964844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>7844.910156</td>\n",
       "      <td>4.201571</td>\n",
       "      <td>10.456973</td>\n",
       "      <td>1364.766667</td>\n",
       "      <td>151.442296</td>\n",
       "      <td>57.155360</td>\n",
       "      <td>3.837154e+02</td>\n",
       "      <td>60.320793</td>\n",
       "      <td>91.934628</td>\n",
       "      <td>80.226692</td>\n",
       "      <td>50.589360</td>\n",
       "      <td>127.032311</td>\n",
       "      <td>111.065611</td>\n",
       "      <td>83.727646</td>\n",
       "      <td>53.917129</td>\n",
       "      <td>170.698985</td>\n",
       "      <td>146.054688</td>\n",
       "      <td>85.718750</td>\n",
       "      <td>58.074219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>325 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      first_frame         TI          SI      keypoint  brightness   contrast  \\\n",
       "0    15563.023440  19.995989   99.299255  96335.700000  131.093257  50.075259   \n",
       "1    16130.993160  19.377771   98.798615  92503.700000  133.617967  50.388930   \n",
       "2    16350.106450  18.683294  100.835434  93543.133330  134.556517  50.716521   \n",
       "3    16235.444340  17.698383   99.843338  93216.166670  134.781613  50.695966   \n",
       "4    16325.968750  16.584223  101.031555  93978.700000  134.750404  50.730587   \n",
       "..            ...        ...         ...           ...         ...        ...   \n",
       "320   8096.879883   5.340825   12.266477   1497.200000  149.970529  58.319979   \n",
       "321   7892.939453   5.826466   12.201683   1499.233333  144.369581  58.301420   \n",
       "322   8608.088867   5.509458   12.686021   1447.033333  144.804492  57.764878   \n",
       "323   7857.112305   4.280614   10.780957   1291.200000  154.923462  57.141601   \n",
       "324   7844.910156   4.201571   10.456973   1364.766667  151.442296  57.155360   \n",
       "\n",
       "       edgeLength       Hue1       Hue2       Hue3       Hue4        Hue5  \\\n",
       "0    1.639670e+06  48.885553  38.815013  26.810291  60.863760   43.180126   \n",
       "1    1.613227e+06  49.089428  38.933680  26.928219  60.957078   43.079494   \n",
       "2    1.642275e+06  49.262484  39.039213  26.972903  61.036619   42.962030   \n",
       "3    1.640041e+06  48.996321  38.919626  26.879124  60.873019   43.184083   \n",
       "4    1.645418e+06  49.144942  39.006811  26.959807  60.987993   43.106776   \n",
       "..            ...        ...        ...        ...        ...         ...   \n",
       "320  3.536846e+03  49.121758  83.634994  74.440953  45.339770  126.207614   \n",
       "321  1.453691e+03  59.429101  91.880945  79.875389  50.228180  128.664562   \n",
       "322  1.485674e+03  60.006962  92.120943  80.231870  50.551533  128.065475   \n",
       "323  1.068472e+03  50.197428  85.221489  75.486053  45.846897  128.626873   \n",
       "324  3.837154e+02  60.320793  91.934628  80.226692  50.589360  127.032311   \n",
       "\n",
       "           Hue6        Hue7  Saturation       Value      video4      video6  \\\n",
       "0     46.297540  146.223288  106.916600  142.922966  514.630859  219.875000   \n",
       "1     46.735909  145.862168  105.498921  145.540919  490.655273  217.898438   \n",
       "2     46.577120  145.752035  105.739884  146.580133  496.205078  214.589844   \n",
       "3     47.113937  145.922089  105.424799  146.794137  478.143555  207.558594   \n",
       "4     47.020205  145.813482  105.484641  146.764501  496.059570  214.871094   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "320  119.436579   91.507964   57.965439  168.945742  155.458984   89.447266   \n",
       "321  112.233985   83.960095   59.494577  164.329315  168.297852   97.144531   \n",
       "322  111.920640   83.528597   59.350706  164.900697  151.416016   88.652344   \n",
       "323  118.785155   90.293822   53.928743  173.768869  147.665039   85.440430   \n",
       "324  111.065611   83.727646   53.917129  170.698985  146.054688   85.718750   \n",
       "\n",
       "         video8  \n",
       "0    117.837891  \n",
       "1    117.535156  \n",
       "2    119.447266  \n",
       "3    112.416992  \n",
       "4    118.107422  \n",
       "..          ...  \n",
       "320   60.113281  \n",
       "321   65.839844  \n",
       "322   60.095703  \n",
       "323   57.964844  \n",
       "324   58.074219  \n",
       "\n",
       "[325 rows x 19 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#读数据集\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "data = pd.read_csv(\"F:/video_cut/Bear_size.csv\") \n",
    "#print(data)\n",
    "data = data[[\"first_frame\",\"TI\",\"SI\",'keypoint',\n",
    "       'brightness','contrast','edgeLength','Hue1', 'Hue2',\n",
    "       'Hue3','Hue4',\t'Hue5',\t'Hue6',\t'Hue7',\t'Saturation',\n",
    "       'Value','video4','video6','video8']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4dcfa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9335941.    9377481.    9164215.    8620233.    8122605.    9421068.\n",
      " 8925290.    9834931.    2930352.2   3010856.8   2922989.5   3045820.8\n",
      " 2904855.2   3326840.5   3121942.5   2411348.    2406362.    2138169.5\n",
      " 2740974.8   2946303.2   2026401.8   1162577.9    801905.25   906744.1\n",
      "  715134.8   1043538.5   1220584.6   1496356.8    728679.56   863785.\n",
      " 1803624.1   2007486.9   1064160.    1533059.2   1049379.6   1193198.\n",
      "  472805.25   865206.75  1020521.6   2032240.5   5624481.    5891227.5\n",
      " 5729039.5   5056335.    4571635.    3832187.8   3327794.8   2780558.2\n",
      " 3019581.8   4628785.5   5961446.5   7864073.    7472520.5   7191437.5\n",
      " 7223426.5   7941360.    8493136.    7294112.    5556531.5   1364080.5\n",
      " 1763869.9   1636578.5   1697095.9   1715684.    1795045.1   1443989.4\n",
      " 1006675.1    946006.3    836763.44   822410.3   1101017.9   1071407.6\n",
      "  950207.4    843219.25   761540.25   862004.5    798965.3   1583022.6\n",
      " 1626729.2   1001876.2    972613.4   1872387.5   1778442.9   1773924.5\n",
      " 1512030.    1353185.1   1298581.1   1788789.9   1811014.8   1446165.9\n",
      " 1442186.2   2055541.2   1710223.    1602961.8   1585989.8   1655045.8\n",
      " 1844100.8   2024160.2   1818889.5   1818331.6   1791254.9   1851985.1\n",
      " 1830303.8   2064948.1   2125797.8   2159680.5   1995909.    1838913.8\n",
      " 1582804.2   1435215.1   2862465.5   2614879.    3325022.2   3359454.2\n",
      " 2756407.    3354787.8   3357370.    3196636.8   2003614.5   1996416.4\n",
      " 2261125.5   2148536.    2158823.    2299754.2   1569781.2   1781372.6\n",
      " 1725972.4    814385.06   347646.3    274597.12   643666.25  1111860.5\n",
      " 1373942.5   1130435.8   1329814.9   1659508.8   9663234.    7660958.\n",
      " 7400268.5   7007596.    5079199.5   3622942.5   3887577.2   3553095.2\n",
      " 2701018.5   1134937.8    674766.3    807630.8    672423.2    616426.44\n",
      "  585798.1    608778.7    622914.3    718293.9    701559.06   651018.1\n",
      "  651221.6    570654.1    525405.44   531342.6    504654.47   520955.2\n",
      "  551255.75   573277.4    667796.06  5172153.5   5758841.5   5366874.\n",
      " 4914554.    5119585.5   4950187.    5677034.5   5916492.    5103050.5\n",
      " 5324174.5   6292402.5   5910782.    6023581.    6707695.5    435186.94\n",
      "  330701.3    401169.16   388238.75   412270.47   421599.12   293613.06\n",
      "  413969.03   530721.06   383153.2    379884.3    203673.86   206272.73\n",
      "  214175.05   223849.22   230922.3    240208.6    250042.88   268810.66\n",
      "  278248.16   313733.4    335749.9    350530.28   374838.62   392221.97\n",
      "  410946.47   429767.2    462860.3    536376.75    54023.85   686240.2\n",
      "  886401.8    894772.75   821883.1    789472.44   821484.9    962289.5\n",
      " 1243490.6   1083538.9   1022424.1    937492.9    961721.8   1001396.94\n",
      "  938885.4   1203404.     625482.94   842374.     774597.     844622.75\n",
      "  864307.75   831918.25  1040191.4    906827.44   262707.47   760137.6\n",
      "  741486.4    341559.3    219143.02   357311.66   359024.2    328842.25\n",
      "  238452.67   218679.14   172603.34   257928.81   179288.19   157497.88\n",
      "  272816.9    361230.84   295494.97   178310.2    133066.7    130454.914\n",
      "  148634.78   165268.34   166287.98   171107.75   108008.164  101751.61\n",
      "   87987.14    94069.875  268886.1    316080.53   179875.56   150751.94\n",
      "  136412.02   148124.47   109402.38   163863.14   209298.56   267137.3\n",
      "  577415.56   569093.06   430811.94   367699.4    378390.75   396840.12\n",
      "  385368.     308951.44   199137.9    261055.25   320262.94   278952.6\n",
      "  230818.53   327269.1    219424.25   136669.67   109949.32   112618.35\n",
      "  133277.77   112866.51  1583471.4   1617912.8   1345086.5    973034.3\n",
      "  571422.5   1372448.4   2248164.2   2008203.2    352225.22   367027.38\n",
      "  366476.28   351632.53   399803.03   380745.22   396076.     374740.62\n",
      "  454488.     406735.62   261337.53   470352.06   303167.2    208561.64\n",
      "  208420.17   555096.9    564861.6    467684.94   352643.4    450267.75\n",
      " 1360511.4   2039873.9   1297320.5   1379638.2   1422777.1   1008997.94\n",
      "  988828.44 ] [9335941.    9377481.    9164215.    8620233.    8122605.    9421068.\n",
      " 8925290.    9834931.    2930352.2   3010856.8   2922989.5   3045820.8\n",
      " 2904855.2   3326840.5   3121942.5   2411348.    2406362.    2138169.5\n",
      " 2740974.8   2946303.2   2026401.8   1162577.9    801905.25   906744.1\n",
      "  715134.8   1043538.5   1220584.6   1496356.8    728679.56   863785.\n",
      " 1803624.1   2007486.9   1064160.    1533059.2   1049379.6   1193198.\n",
      "  472805.25   865206.75  1020521.6   2032240.5   5624481.    5891227.5\n",
      " 5729039.5   5056335.    4571635.    3832187.8   3327794.8   2780558.2\n",
      " 3019581.8   4628785.5   5961446.5   7864073.    7472520.5   7191437.5\n",
      " 7223426.5   7941360.    8493136.    7294112.    5556531.5   1364080.5\n",
      " 1763869.9   1636578.5   1697095.9   1715684.    1795045.1   1443989.4\n",
      " 1006675.1    946006.3    836763.44   822410.3   1101017.9   1071407.6\n",
      "  950207.4    843219.25   761540.25   862004.5    798965.3   1583022.6\n",
      " 1626729.2   1001876.2    972613.4   1872387.5   1778442.9   1773924.5\n",
      " 1512030.    1353185.1   1298581.1   1788789.9   1811014.8   1446165.9\n",
      " 1442186.2   2055541.2   1710223.    1602961.8   1585989.8   1655045.8\n",
      " 1844100.8   2024160.2   1818889.5   1818331.6   1791254.9   1851985.1\n",
      " 1830303.8   2064948.1   2125797.8   2159680.5   1995909.    1838913.8\n",
      " 1582804.2   1435215.1   2862465.5   2614879.    3325022.2   3359454.2\n",
      " 2756407.    3354787.8   3357370.    3196636.8   2003614.5   1996416.4\n",
      " 2261125.5   2148536.    2158823.    2299754.2   1569781.2   1781372.6\n",
      " 1725972.4    814385.06   347646.3    274597.12   643666.25  1111860.5\n",
      " 1373942.5   1130435.8   1329814.9   1659508.8   9663234.    7660958.\n",
      " 7400268.5   7007596.    5079199.5   3622942.5   3887577.2   3553095.2\n",
      " 2701018.5   1134937.8    674766.3    807630.8    672423.2    616426.44\n",
      "  585798.1    608778.7    622914.3    718293.9    701559.06   651018.1\n",
      "  651221.6    570654.1    525405.44   531342.6    504654.47   520955.2\n",
      "  551255.75   573277.4    667796.06  5172153.5   5758841.5   5366874.\n",
      " 4914554.    5119585.5   4950187.    5677034.5   5916492.    5103050.5\n",
      " 5324174.5   6292402.5   5910782.    6023581.    6707695.5    435186.94\n",
      "  330701.3    401169.16   388238.75   412270.47   421599.12   293613.06\n",
      "  413969.03   530721.06   383153.2    379884.3    203673.86   206272.73\n",
      "  214175.05   223849.22   230922.3    240208.6    250042.88   268810.66\n",
      "  278248.16   313733.4    335749.9    350530.28   374838.62   392221.97\n",
      "  410946.47   429767.2    462860.3    536376.75    54023.85   686240.2\n",
      "  886401.8    894772.75   821883.1    789472.44   821484.9    962289.5\n",
      " 1243490.6   1083538.9   1022424.1    937492.9    961721.8   1001396.94\n",
      "  938885.4   1203404.     625482.94   842374.     774597.     844622.75\n",
      "  864307.75   831918.25  1040191.4    906827.44   262707.47   760137.6\n",
      "  741486.4    341559.3    219143.02   357311.66   359024.2    328842.25\n",
      "  238452.67   218679.14   172603.34   257928.81   179288.19   157497.88\n",
      "  272816.9    361230.84   295494.97   178310.2    133066.7    130454.914\n",
      "  148634.78   165268.34   166287.98   171107.75   108008.164  101751.61\n",
      "   87987.14    94069.875  268886.1    316080.53   179875.56   150751.94\n",
      "  136412.02   148124.47   109402.38   163863.14   209298.56   267137.3\n",
      "  577415.56   569093.06   430811.94   367699.4    378390.75   396840.12\n",
      "  385368.     308951.44   199137.9    261055.25   320262.94   278952.6\n",
      "  230818.53   327269.1    219424.25   136669.67   109949.32   112618.35\n",
      "  133277.77   112866.51  1583471.4   1617912.8   1345086.5    973034.3\n",
      "  571422.5   1372448.4   2248164.2   2008203.2    352225.22   367027.38\n",
      "  366476.28   351632.53   399803.03   380745.22   396076.     374740.62\n",
      "  454488.     406735.62   261337.53   470352.06   303167.2    208561.64\n",
      "  208420.17   555096.9    564861.6    467684.94   352643.4    450267.75\n",
      " 1360511.4   2039873.9   1297320.5   1379638.2   1422777.1   1008997.94\n",
      "  988828.44 ]\n"
     ]
    }
   ],
   "source": [
    "#vs = framesize + a * framesize * TI * n\n",
    "from sklearn.model_selection import train_test_split\n",
    "#数据拆分\n",
    "train = data\n",
    "test = data\n",
    "#train,test = train_test_split(data, train_size=0.8)\n",
    "\n",
    "#归一化\n",
    "max_min_scaler = lambda x:  (x-x.mean())/ x.std()#(x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "# 读取数据集的0~439行、16列的数据，训练集:440*16\n",
    "#n = train[['n']].values.astype(np.float32).ravel()\n",
    "\n",
    "train_fs = np.array(train[['first_frame']].apply(max_min_scaler).values.astype(np.float32)).ravel()#.apply(max_min_scaler)\n",
    "test_fs = np.array(test[['first_frame']].apply(max_min_scaler).values.astype(np.float32)).ravel()\n",
    "\n",
    "#train_TI = np.array(train[['frame_TI']].apply(max_min_scaler).values.astype(np.float32)).ravel()\n",
    "#test_TI = np.array(test[['frame_TI']].apply(max_min_scaler).values.astype(np.float32)).ravel()\n",
    "\n",
    "train_x = train[['first_frame']]*np.array(train[['TI']])* 30\n",
    "train_x = np.array(train_x.values.astype(np.float32)).ravel()#train_fs*train_TI*n\n",
    "test_x = test[['first_frame']]*np.array(test[['TI']])* 30#test_fs*test_TI*n\n",
    "test_x = np.array(test_x.values.astype(np.float32)).ravel()#train_fs*train_TI*n\n",
    "\n",
    "train_y4 = np.array(train.video4.values.reshape(-1, 1).astype(np.float32)).ravel()\n",
    "test_y4 = np.array(test.video4.values.reshape(-1, 1).astype(np.float32)).ravel()\n",
    "train_y6 = np.array(train.video6.values.reshape(-1, 1).astype(np.float32)).ravel()\n",
    "test_y6 = np.array(test.video6.values.reshape(-1, 1).astype(np.float32)).ravel()\n",
    "train_y8 = np.array(train.video8.values.reshape(-1, 1).astype(np.float32)).ravel()\n",
    "test_y8 = np.array(test.video8.values.reshape(-1, 1).astype(np.float32)).ravel()\n",
    "\n",
    "\n",
    "def mean_norm(df_input):\n",
    "    return df_input.apply(lambda x: (x-x.mean())/ x.std(), axis=0)\n",
    "\n",
    "#df_mean_norm = mean_norm(data)\n",
    "print(train_x,test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "20139e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拟合参数: [2.93307871e-05]\n"
     ]
    }
   ],
   "source": [
    "#vs = framesize + a * framesize * TI * n\n",
    "import numpy as np\n",
    "from scipy.optimize import leastsq\n",
    "\n",
    "def accury(pred,y):\n",
    "    return 100-(abs(pred - y)/y*100)\n",
    "\n",
    "def func(a,x):\n",
    "    return a * x + train_fs\n",
    "\n",
    "def residuals(a,x,y):\n",
    "    return y-func(a,x)\n",
    "\n",
    "a=1\n",
    "#print(residuals(a,train_y4, train_fs,train_TI,n))\n",
    "#print(func( train_fs,train_TI,n,a ))\n",
    "#print(train_y4)\n",
    "\n",
    "p = leastsq(residuals, a, args=(train_x,train_y6))\n",
    "print(\"拟合参数:\",p[0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "300d17ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.004753194326334\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (90,) (325,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-bfb1dc365d87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'first_frame'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TI'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#train_fs*train_TI*n\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mpre\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-93-39702ad7d7d0>\u001b[0m in \u001b[0;36mfunc\u001b[1;34m(a, x)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtrain_fs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mresiduals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (90,) (325,) "
     ]
    }
   ],
   "source": [
    "test_pre=func(p[0],test_x)\n",
    "#print(test_pre,test_y6)\n",
    "acc=accury(test_pre,test_y6)\n",
    "print(sum(acc)/len(acc))\n",
    "\n",
    "df = pd.read_excel('E:/研学/实验数据/Bee.xlsx')\n",
    "x = df[['first_frame']]*np.array(df[['TI']])* 30\n",
    "x = np.array(x.values.astype(np.float32)).ravel()#train_fs*train_TI*n\n",
    "pre=func(p[0],x,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "74a4863c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#-lavfi psnr=stats_file=%name%_00%%i_psnr.txt -f null -\n",
    "###绘图，看拟合效果###\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(test_x,test_y6,color=\"red\",label=\"Sample Point\",linewidth=3) #画样本点\n",
    "plt.scatter(test_x,test_pre,color=\"orange\",label=\"Fitting Line\",linewidth=2) #画拟合直线\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6bb9694c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拟合参数:\n",
      " [-8.25528808e-13  6.52754640e-05]\n"
     ]
    }
   ],
   "source": [
    "#vs = \n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.optimize import leastsq\n",
    "\n",
    "\n",
    "def func(p,x):\n",
    "    a,b = p\n",
    "    return a * x**2 + b* x +  train_fs\n",
    "\n",
    "def residuals(p,x,y):\n",
    "    return y-func(p,x)\n",
    "\n",
    "p=[1,1]\n",
    "#print(residuals(a,train_y4, train_fs,train_TI,n))\n",
    "#print(func( train_fs,train_TI,n,a ))\n",
    "#print(train_y4)\n",
    "\n",
    "p = leastsq(residuals, p, args=(train_x,train_y4))\n",
    "print(\"拟合参数:\\n\",p[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "857c15ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[539.18445  541.3773   530.7682   503.2212   477.636    543.562\n",
      " 518.6406   563.9568   184.16486  188.92506  183.55896  190.95137\n",
      " 182.3021   207.66551  195.40138  152.1473   151.71837  135.07173\n",
      " 172.15837  184.51443  128.06703   73.827835  50.672565  57.4724\n",
      "  45.12512   66.17692   77.476135  94.90474   46.886562  55.601826\n",
      " 114.80425  127.498405  68.331276  97.97615   67.4042    76.48743\n",
      "  30.491405  55.693615  65.536446] [514.63086  490.65527  496.20508  478.14355  496.05957  608.38574\n",
      " 550.4971   602.876    126.60156  134.26758  132.12402  143.43945\n",
      " 147.02148  156.28418  156.60156  154.95996  150.49121  173.45898\n",
      " 222.4209   186.20996  171.7959   132.41211  113.06152  120.45215\n",
      " 110.62207  120.674805 118.243164 133.19531   87.02246   92.64844\n",
      " 121.0293    97.18164   90.18652   86.856445  88.87891   81.84766\n",
      "  78.30957   93.70117   92.86426 ]\n",
      "80.33572223247626\n"
     ]
    }
   ],
   "source": [
    "test_pre=func(p[0],test_x)\n",
    "print(test_pre,test_y4)#x3\n",
    "acc=accury(test_pre,test_y6)\n",
    "print(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4e10447a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拟合参数:\n",
      " [ 1.27771183e-18 -1.58988757e-11  9.57468183e-05]\n"
     ]
    }
   ],
   "source": [
    "#vs = \n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.optimize import leastsq\n",
    "\n",
    "\n",
    "def func(p,x):\n",
    "    a,b,c = p\n",
    "    return a * x**3 + b* x**2 + c * x +  train_fs\n",
    "\n",
    "def residuals(p,x,y):\n",
    "    return y-func(p,x)\n",
    "\n",
    "p=[1,1,1]\n",
    "#print(residuals(a,train_y4, train_fs,train_TI,n))\n",
    "#print(func( train_fs,train_TI,n,a ))\n",
    "#print(train_y4)\n",
    "\n",
    "p = leastsq(residuals, p, args=(train_x,train_y4))\n",
    "print(\"拟合参数:\\n\",p[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cd55de36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[549.5714   555.253    527.4859   464.26056  415.3823   561.1756\n",
      " 498.2991   621.1354   176.17325  178.9003   175.75085  180.02908\n",
      " 174.94418  189.25572  182.49678  155.8937   155.56404  143.80272\n",
      " 168.74606  176.1238   138.55037   90.88792   66.07363   73.661606\n",
      "  59.674774  83.012474  94.5358   111.03122   61.581226  71.499405\n",
      " 128.22536  138.26054   85.22784  113.867836  84.25798   93.5561\n",
      "  41.6639    71.60155   82.29277 ] [514.63086  490.65527  496.20508  478.14355  496.05957  608.38574\n",
      " 550.4971   602.876    126.60156  134.26758  132.12402  143.43945\n",
      " 147.02148  156.28418  156.60156  154.95996  150.49121  173.45898\n",
      " 222.4209   186.20996  171.7959   132.41211  113.06152  120.45215\n",
      " 110.62207  120.674805 118.243164 133.19531   87.02246   92.64844\n",
      " 121.0293    97.18164   90.18652   86.856445  88.87891   81.84766\n",
      "  78.30957   93.70117   92.86426 ]\n",
      "88.00300453259395\n"
     ]
    }
   ],
   "source": [
    "test_pre=func(p[0],test_x)\n",
    "print(test_pre,test_y4)\n",
    "acc=accury(test_pre,test_y6)\n",
    "print(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "12e9a02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeYAAAFyCAYAAAAgZHmaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAqp0lEQVR4nO3de5iU5Z3n//cXkMOCRgUkaqONWRNFReIAmpgYmIwxOkbFwRVNZpU4o5mc5jfXOuthkhXdJOMmjnFnJ167TBJkf7/Ew89IguboAY/xQGMYDxAjEZBeCCDGydCA2nDvH081NnR1V3V3dddTVe/XdfVVXc+h6ubRiw/P/dz3946UEpIkKR+GVLsBkiTpHQazJEk5YjBLkpQjBrMkSTliMEuSlCMGsyRJOTKs2g0AGDduXGpubq52MyRJGjTLly9/LaU0ft/tuQjm5uZmWlpaqt0MSZIGTUSsK7bdrmxJknLEYJYkKUcMZkmSciQXz5iLefvtt2ltbWXnzp3VboqAkSNH0tTUxH777VftpkhSXcttMLe2trL//vvT3NxMRFS7OQ0tpcTWrVtpbW1l0qRJ1W6OJNW13HZl79y5k7FjxxrKORARjB071t4LSRoEuQ1mwFDOEf9bSNLgyHUwV9tXv/pVjjvuOKZMmcLUqVN5+umnB/T7Zs6c2av53JdeeimTJk1i6tSpnHTSSTz55JM9Hv/BD36w5GfecsstbN++vew2SJIqqz6Cua0Nrr8empth6NDs9frrs+199OSTT3Lffffx7LPP8txzz/HAAw8wceLEijW5Ur7xjW+wYsUKbrzxRq644ooej/3lL39Z8vMMZknqpH07rPkevPAVWPt9aN8x4F9Z+8Hc1gazZsH8+bBuHezenb3On59t72M4b9y4kXHjxjFixAgAxo0bx2GHHQbADTfcwPTp0zn++OO5/PLLSSkB2R3v3/zN33Daaadx7LHHsmzZMs4//3yOPvpovvSlLwGwdu1ajjnmGC655BKmTJnCnDlzigbhL37xCz7wgQ9w0kknccEFF7Bt27Ye23vaaaexevVqAG6++WaOP/54jj/+eG655ZY9x4wZMwaAhx9+mJkzZzJnzhyOOeYYPvnJT5JS4h//8R/ZsGEDs2bNYtasWX26bpJUN7YugyVHwZOfgue+DL/8JCyZlG0fQLUfzDfdBMu6uUjLlmX7++BjH/sY69ev573vfS+f/exneeSRR/bs+/znP8+yZct44YUX2LFjB/fdd9+efcOHD+fRRx/lM5/5DOeeey7f+ta3eOGFF7jtttvYunUrAC+99BKXX345zz33HAcccAC33nrrXt/92muv8ZWvfIUHHniAZ599lmnTpnHzzTf32N57772XE044geXLl7Nw4UKefvppnnrqKf75n/+ZX/3qV12O/9WvfsUtt9zCypUreeWVV3jiiSf44he/yGGHHcbSpUtZunRpn66bJNWF9h3wyCdg56a9t+/clG0fwDvn2g/mhQv7t78bY8aMYfny5SxYsIDx48dz4YUXcttttwGwdOlSTj75ZE444QQeeughXnzxxT3nnXPOOQCccMIJHHfccRx66KGMGDGCo446ivXr1wMwceJETj31VAA+9alP8fjjj+/13U899RQrV67k1FNPZerUqSxatIh164qWVOVv//ZvmTp1KgsWLOA73/kOjz/+OLNnz2b06NGMGTOG888/n8cee6zLeTNmzKCpqYkhQ4YwdepU1q5d26frJEl1qXVx11DusHNTtn+A5HYec9kKYdfn/T0YOnQoM2fOZObMmZxwwgksWrSIuXPn8tnPfpaWlhYmTpzI/Pnz95pG1NH1PWTIkD2/d7xvb28Huo5w3vd9SonTTz+d22+/vWQbv/GNbzBnzpw97x944IGy/myd2zZ06NA9bZMkAdte6d/+fqj9O+ZSA7L6OGDrpZde4uWXX97zfsWKFRx55JF7QnjcuHFs27aNu+++u9ef/eqrr+4ZQX377bfzoQ99aK/9p5xyCk888cSeZ8bbt2/nN7/5TVmffdppp/HDH/6Q7du309bWxuLFi/nwhz9cdtv2339//u3f/q3s4yWpLo05qn/7+6H2g3nevP7t78a2bdu45JJLmDx5MlOmTGHlypXMnz+fAw88kL/8y7/khBNO4LzzzmP69Om9/uxjjz2WRYsWMWXKFF5//XX+6q/+aq/948eP57bbbuOiiy5iypQpnHLKKfz6178u67NPOukkLr30UmbMmMHJJ5/MX/zFX/D+97+/7LZdfvnlnHnmmQ7+ktTYmmbDyAnF942ckO0fINExoriapk2blvadv7tq1SqOPfbY0id3jMouNgBsxgx46CEYPbpCLe2/tWvXcvbZZ/PCCy9Uuym9VvZ/E0mqB1uXdR0ANnICfOReGNv7m7J9RcTylNK0fbfX/jPm0aNh6dJs9PXChdkz5YkTszvlK6/MVShLkmrI2OlwzppsoNe2V7Lu66bZMGzUgH5t7QczZOF73XXZT841NzfX5N2yJDWkYaOg+eJB/craf8YsSVIdMZglScqRsoI5Ig6MiLsj4tcRsSoiPhARB0fE/RHxcuH1oE7HXxMRqyPipYg4Y+CaL0lSfSn3jvm/Az9LKR0DnAisAq4GHkwpHQ08WHhPREwG5gLHAR8Hbo2IoZVuuCRJ9ahkMEfEAcBpwHcAUkpvpZTeAM4FFhUOWwScV/j9XOCOlNKbKaU1wGpgRmWbPTiGDh3K1KlT9/ysXbt2z9KJa9eu5fvf//6eY1esWMFPfvKTPe+XLFnCjTfeWJF2XHrppV0KmWzYsGGvil+SpPpQzqjso4AtwMKIOBFYDvw1MCGltBEgpbQxIg4pHH848FSn81sL2/YSEZcDlwMcccQRff4D7NG+HdYvhrY1FRvSPmrUKFasWLHXto6lEzuC+eKLs9F6K1asoKWlhbPOOgvIamZ31M0eCIcddlifqo5JkvKtnGAeBpwEfCGl9HRE/HcK3dbdiCLbulQxSSktABZAVmCkjHZ0b4AngXc2ZswYtm3bxtVXX82qVauYOnUqF110Ed/61rfYsWMHjz/+ONdccw07duygpaWFf/qnf+LSSy/lgAMOoKWlhd/97nd8/etfZ86cOezevZvPf/7zPPLII0yaNIndu3fz6U9/uqw74c6FSm677TaWLFnC9u3b+e1vf8vs2bP5+te/DmTLR1533XW8+eabvOc972HhwoV7ln+UJOVPOc+YW4HWlNLThfd3kwX1pog4FKDwurnT8Z0LVDcBGyrT3CIGcGmuHTt27OnGnj177/JrN954Ix/+8IdZsWIFV111FTfccAMXXnghK1as4MILL+zyWRs3buTxxx/nvvvu4+qrs3/X3HPPPaxdu5bnn3+eb3/723vqZ/fFihUruPPOO3n++ee58847Wb9+fZ+Wj5QkVVfJO+aU0u8iYn1EvC+l9BLwUWBl4ecS4MbC648KpywBvh8RNwOHAUcDzwxE44Hylubq4+TwYl3ZfXXeeecxZMgQJk+ezKZNWXsff/xxLrjgAoYMGcK73/3uftWn/uhHP8q73vUuACZPnsy6det444039iwfCfDWW2/xgQ98oP9/GEnSgCm38tcXgO9FxHDgFWAe2d32XRFxGfAqcAFASunFiLiLLLjbgc+llHZVvOUdqrg0V290Xmaxoz55JeuUF1vGsTfLR0qS8qGs6VIppRUppWkppSkppfNSSr9PKW1NKX00pXR04fX1Tsd/NaX0npTS+1JKPx245lO1pbn2XR6xL8slfuhDH+IHP/gBu3fvZtOmTTz88MMVbWN/lo+UJFVH7Vf+qtLSXFOmTGHYsGGceOKJfPOb32TWrFmsXLmSqVOncuedd5b1GX/2Z39GU1MTxx9/PFdccQUnn3zynu7ofV1xxRU0NTXR1NRUdnd0f5aPlCRVR+0v+wiDOiq70rZt28aYMWPYunUrM2bM4IknnuDd7353tZtVlMs+SlLl1O+yj1C1pbkq4eyzz+aNN97grbfe4stf/nJuQ1mSNDjqI5ihKktzVUKlnytLkmpb7T9jliSpjuQ6mPPw/FsZ/1tI0uDIbTCPHDmSrVu3Ggg5kFJi69atjBw5stpNkaS6l9tnzE1NTbS2trJly5ZqN0Vk/1BqamqqdjMkqe7lNpj3228/Jk2aVO1mSJI0qHLblS1JUiPK7R2zJEl91r4d1i+GtjU1VdsCDGZJUr2p4WqQYFe2JKmetO/oGsqQvX/kE9n+nDOYJUn1o3Vx11DusHNTtj/nDGZJUv3Y/GjP+7e9Mjjt6AeDWZJUH9p3wLrbez5mzFGD05Z+MJglSfVh3R3w9h+637/fu7LR2TlnMEuSat/WZbD8Cz0fc+TcmpgyZTBLkmpbx0js9raejzvktMFpTz8ZzJKk2tbTSOwOIyfURDc2GMySpFpXaqT1sDFZcZEa6MYGg1mSVOtKjbT+o/9RExW/OhjMkqTa1jQ766ouZuQEOPLCwW1PPxnMkqTaNmxU1lW9bzh31MeukS7sDi5iIUmqfWOnwzlrsoFg216puRWlOjOYJUn1YdgoaL642q3oN7uyJUnKEe+YJUm1oX07rF8MbWtququ6FINZkpR/W5d1XWe5Y3BXDU2FKodd2ZKkfOsoublvda+dmwqlOHdUp10DxGCWJOVbTyU3d27K9tcRg1mSlG+lSm6W2l9jDGZJUr6VKrlZan+NMZglSflWquRmjawaVS6DWZKUb3VWcrMUp0tJkvKvjkpulmIwS5JqQ52U3CzFrmxJknLEYJYkKUcMZkmScsRgliQpRwxmSZJypKxgjoi1EfF8RKyIiJbCtoMj4v6IeLnwelCn46+JiNUR8VJEnDFQjZck1YH27bDme/DCV2Dt9+tuUYre6s10qVkppdc6vb8aeDCldGNEXF14f1VETAbmAscBhwEPRMR7U0q7KtZqSVJ9aKDlHMvVn67sc4FFhd8XAed12n5HSunNlNIaYDUwox/fI0mqRw22nGO5yg3mBPwiIpZHxOWFbRNSShsBCq+HFLYfDqzvdG5rYdteIuLyiGiJiJYtW7b0rfWSpNrVYMs5lqvcruxTU0obIuIQ4P6I+HUPx0aRbanLhpQWAAsApk2b1mW/JKnONdhyjuUq6445pbSh8LoZWEzWNb0pIg4FKLxuLhzeCkzsdHoTsKFSDZYk1YkGW86xXCWDOSJGR8T+Hb8DHwNeAJYAlxQOuwT4UeH3JcDciBgREZOAo4FnKt1wSVKNa7DlHMtVTlf2BGBxRHQc//2U0s8iYhlwV0RcBrwKXACQUnoxIu4CVgLtwOcckS1J6qJjOcfuRmXX4cpR5YiUqv94d9q0aamlpaXazZAkVUP7joZYznFfEbE8pTRt3+0u+yhJqq4GWc6xXJbklCQpRwxmSZJyxGCWJClHDGZJknLEYJYkKUcMZkmScsRgliQpRwxmSZJyxAIjkqTKad8O6xdD25qGquJVSQazJKkyti7rvu712OnVa1eNsStbktR/7Tu6hjJk7x/5RLZfZTGYJUn917q4ayh32Lkp26+yGMySpP7b9kr/9msPg1mS1H9jjurffu1hMEuS+q9pdjbQq5iRE7L9KovBLEnqv2GjstHX+4Zzx6hsp0yVzelSkqTKGDsdzlmTDfTa9orzmPvIYJYkVc6wUdB8cbVbUdPsypYkKUcMZkmScsRgliQpRwxmSZJyxGCWJClHDGZJknLEYJYkKUcMZkmScsRgliQpRwxmSZJyxGCWJClHDGZJknLEYJYkKUcMZkmScsRgliQpR1yPWZIaUft2WL8Y2tbAmKOgaXa2lrKqzmCWpEazdRk88gnYuemdbSMnwEfuhbHTq9cuAXZlS1Jjad/RNZQhe//IJ7L9qiqDWZIaSevirqHcYeembL+qymCWpEay7ZX+7deAM5glqZGMOap/+zXgDGZJaiRNs7OBXsWMnJDtV1UZzJLUSIaNykZf7xvOHaOynTJVdWVPl4qIoUAL8H9SSmdHxMHAnUAzsBb4Dyml3xeOvQa4DNgFfDGl9PMKt1uS1Fdjp8M5a7KBXttecR5zzvRmHvNfA6uAAwrvrwYeTCndGBFXF95fFRGTgbnAccBhwAMR8d6U0q4KtluS1B/DRkHzxdVuhYooqys7IpqAPwW+3WnzucCiwu+LgPM6bb8jpfRmSmkNsBqYUZHWSpJU58p9xnwL8J+B3Z22TUgpbQQovB5S2H44sL7Tca2FbXuJiMsjoiUiWrZs2dLbdkuSVJdKBnNEnA1sTiktL/Mzo8i21GVDSgtSStNSStPGjx9f5kdLklTfynnGfCpwTkScBYwEDoiI/w/YFBGHppQ2RsShwObC8a3AxE7nNwEbKtloSZLqVck75pTSNSmlppRSM9mgrodSSp8ClgCXFA67BPhR4fclwNyIGBERk4CjgWcq3nJJkupQf1aXuhG4KyIuA14FLgBIKb0YEXcBK4F24HOOyJYkqTyRUpfHv4Nu2rRpqaWlpdrNkCRp0ETE8pTStH23W/lLkqQcMZglScoRg1mSpBwxmCVJyhGDWZKkHDGYJUnKEYNZkqQcMZglScoRg1mSpBwxmCVJyhGDWZKkHDGYJUnKEYNZkqQcMZglScoRg1mSpBwxmCVJyhGDWZKkHDGYJUnKEYNZkqQcMZglScoRg1mSpBwxmCVJyhGDWZKkHDGYJUnKEYNZkqQcMZglScoRg1mSpBwxmCVJyhGDWZKkHDGYJUnKEYNZkqQcMZglScoRg1mSpBwxmCVJyhGDWZKkHDGYJUnKEYNZkqQcMZglScoRg1mSpBwxmCVJyhGDWZKkHDGYJUnKkZLBHBEjI+KZiPiXiHgxIq4vbD84Iu6PiJcLrwd1OueaiFgdES9FxBkD+QeQJKmelHPH/CbwxymlE4GpwMcj4hTgauDBlNLRwIOF90TEZGAucBzwceDWiBg6AG2XJKnulAzmlNlWeLtf4ScB5wKLCtsXAecVfj8XuCOl9GZKaQ2wGphRyUZLklSvynrGHBFDI2IFsBm4P6X0NDAhpbQRoPB6SOHww4H1nU5vLWyTJEkllBXMKaVdKaWpQBMwIyKO7+HwKPYRXQ6KuDwiWiKiZcuWLWU1VpKketerUdkppTeAh8meHW+KiEMBCq+bC4e1AhM7ndYEbCjyWQtSStNSStPGjx/f+5ZLklSHyhmVPT4iDiz8Pgr4E+DXwBLgksJhlwA/Kvy+BJgbESMiYhJwNPBMhdstSVJdGlbGMYcCiwojq4cAd6WU7ouIJ4G7IuIy4FXgAoCU0osRcRewEmgHPpdS2jUwzZckqb5ESl0e/w66adOmpZaWlmo3Q5KkQRMRy1NK0/bdbuUvSZJyxGCWJClHDGZJknLEYJYkKUcMZkmScsRgliQpRwxmSZJyxGCWJClHDGZJknLEYJYkKUcMZkmScsRgliQpRwxmSZJyxGCWJNWXtja4/npoboahQ7PX66/Ptlfjc3rJZR8lSfWjrQ1mzYJly7rumz4dli6F0aMH73N64LKPkqT6d9NNxcMUsu033TS4n9MHBrMkqX4sXNi//ZX+nD4wmCVJ9WP9+v7tr/Tn9IHBLEmqHxMn9m9/pT+nDwxmSVL9mDevf/sr/Tl9YDBLkurHlVdmo6aLmTEj2z+Yn9MHBrMkqX6MHp1NZZo/H448EoYMyV7nz4eHHip/ilOlPqcPnMcsSVIVOI9ZkpQ/VaqulWfDqt0ASVKDKlZda926rLv4xz+uSHWtWuQdsySpOqpYXSvPDGZJUnVUsbpWnhnMkqTqqGJ1rTwzmCVJ1VHF6lp5ZjBLkqqjitW18sxgliRVRxWra+WZwSxJqo4qVtfKM4NZklQ9o0fDddfB2rWwa1f2et11vQ/lOipUYjBLkmpbR6GS+fOzAiW7d79TqOTgg+Hv/q6mAtpglqRGU4t3lz21uadCJW+9BV/7Whbcef7zdeIiFpLUSIqVwewwfXo+y2CWavPvflfenOf587Nu8pxwEQtJUm2WwSzV5tbW8j6nRiqJGcyS1EhqsQxmqTYNKTPKaqSSmMEsSY1kIMtgDtSz61Jt2r27vM+pkUpiBrMkNZKBKoPZ08jo/g68KqfN3RUq6axGKokZzJLUSAaqDOZAPrsu1aZPfzobtHbttTB8ePFjaqiSmKOyJamR9DTCecaMvlfcam7O7pC7c+SRWfGQvuhNmzumTy1cmHWBT5yYBfuVV+ZutHmfR2VHxMSIWBoRqyLixYj468L2gyPi/oh4ufB6UKdzromI1RHxUkScUdk/iiSpzwaqDOZAPrvuTZsrVUmsikreMUfEocChKaVnI2J/YDlwHnAp8HpK6caIuBo4KKV0VURMBm4HZgCHAQ8A700p7eruO7xjlqQaN5B3zHWqz3fMKaWNKaVnC7//G7AKOBw4F1hUOGwRWVhT2H5HSunNlNIaYDVZSEuS6pVLOFZMrwZ/RUQz8H7gaWBCSmkjZOENHFI47HCgc59Fa2GbJKleuYRjxZQdzBExBvgB8P+klP7Q06FFtnXpL4+IyyOiJSJatmzZUm4zJEl55BKOFTOsnIMiYj+yUP5eSumewuZNEXFoSmlj4Tn05sL2VqDzpLMmYMO+n5lSWgAsgOwZcx/br0bWvh3WL4a2NTDmKGiaDcNGVbtVUuPqGHiVo3rUtahkMEdEAN8BVqWUbu60awlwCXBj4fVHnbZ/PyJuJhv8dTTwTCUbLbF1GTzyCdi56Z1tIyfAR+6FsWUUGpCknCqnK/tU4M+BP46IFYWfs8gC+fSIeBk4vfCelNKLwF3ASuBnwOd6GpEt9Vr7jq6hDNn7Rz6R7ZekGlXyjjml9DjFnxsDfLSbc74KfLUf7ZK617q4ayh32Lkp29988eC2SZIqpKxnzNKg6+n58bZXej631H5JyjGDWfnQvh3W3gEbfwpvvg6/b4G3Ow3+7/z8eMxRPX9Wqf2SlGMGs6pv6zJY+nF46/Xuj+l4fnzOmuzueeSE4t3ZIydk+yWpRrm6lKpj52vw1F/CfZPhFx/sOZT3nFN4fjxsVHb3PHLC3vs77qqdMiWphnnHrMH38gJYdkXfzu14fjx2enb33Lo42+Y8Zkl1wmDW4NrwQN9DGfZ+fjxslKOvJdUdu7I1eNp3wGPn9P18nx9LagDeMWtgFJvu1LoYdvWx+IfPjyU1CINZlddducym83r3OUNHw6RPwSGn+fxYUsMwmFVZPZXLXHdH6fOHjIBDz4Kmc+DICw1jSQ3HYFZl9VQu8+1/hf0O2LtwSGfDDoCPPuAiFJIamoO/VFmlymEeeVHX+ccxDI69Bs7/naEsqeF5x6zKKlUO85DT4KRvOv9YkrphMKt77duz58Ibfpq9P+xPSz/3LadcpvOPJalbBrOKK1a/ev3d8Kv/BLN+1n2Xc0e5zGKjsp3uJEklGczqqn0HPHx28frVb72e7Tt3bfcha7lMSeozg1ldtS6GNzd3v//NzdkxPXVH210tSX3iqGx1VWpkdbnHSJJ6zWBWV6VGVpd7jCSp1+zKbjTFaljv++y3aTaMOKT77uwRh7iYhCQNEIO5kXRXw/oj9+49ynrYKJh5X9dR2QDDD872OZBLkgaEwdwoeqph/cgnslHUncN27HQ4rxXW3QkbfpxtK2cesySpXwzmerVvcZDhB3dfw3rnpuKjrIeNgvdcmv1IkgaFwVyPihUHKcVR1pKUC47Krjc9FQfpiaOsJSkXDOZ6U6o4SDEdNawlSVVnMNeb3nZJW8NaknLFZ8y1qKe5yOV0SR/9WRh16MDVsG5rg5tugoULYf16mDgR5s2DK6+E0aMr+12SVGcipVTtNjBt2rTU0tJS7WbUhlJzkdt3wI+aey4O0tMCFP3V1gazZsGyZV33TZ8OS5cazpIERMTylNK0fbfblV1LSs1Fbt/xTnGQ4Qd3PX8wioPcdFPxUIZs+003Ddx3S1IdMJhrSevi0nOR4Z3iICcvhIlzsp+TF2bbultHuVIWLuzffklqcD5jriWlBnZ13l+t4iDr1/dvvyQ1OO+Ya0mpgV15mIs8cWL/9ktSgzOYa0nT7GygVzF5mYs8b17/9ktSgzOYa8mwUdno633DOU9zka+8Mht9XcyMGdl+SVK3fMacF+WskwzZ4K1z1mQDvba9MnBzkftq9OhsSpTzmCWpT5zHXG3t22HVTbDqG9C+7Z3txdZJliTVje7mMXvHXE1bl2ULThQrBtLdOsmSpLrmM+Zq6SgW0tOCE53nJkuSGoLBXC09FQvpzHWSJamhGMzVUm7g5mFusiRp0BjM1VJO4FZ6bnJbG1x/PTQ3w9Ch2ev112fbB1ue2iJJOVJyVHZEfBc4G9icUjq+sO1g4E6gGVgL/IeU0u8L+64BLgN2AV9MKf28VCMaclR2+w5YMqn77uwRh2QLTlRqVHaeVn3KU1skqUr6s7rUbcDH99l2NfBgSulo4MHCeyJiMjAXOK5wzq0RMbQf7a5f3RULGTYaTrghW5qxklOl8rTqU57aIkk5U9Y85ohoBu7rdMf8EjAzpbQxIg4FHk4pva9wt0xK6e8Lx/0cmJ9SerKnz2/IO+YO7TsGp1hIczOsW9f9/iOPhLVrK/+9eW+LJFVJpecxT0gpbQQohPMhhe2HA091Oq61sK0xtG+HdXfAhp9m7w/7Uzjywp6DdtgoaL544NuWp1Wf8tQWScqZSg/+iiLbit6SR8TlEdESES1btmypcDOqYPNjcM8h8PRlsP7u7OfpefDDpqyQSLX1ZdWngRqgVaotTU2lv9fBY5LqVF+DeVOhC5vCa0eVjFag89+6TcCGYh+QUlqQUpqWUpo2fvz4PjYjJzY/Bg98BNqLhMJbr2fVvdp3DH67Ouvtqk8dA7Tmz8+6nXfvzl7nz8+29ycAS7Wlvb3n7x3ItklSlfU1mJcAlxR+vwT4UaftcyNiRERMAo4GnulfE3OufQc8cjbddAxk3txc/QpevV316WtfG7gBWj215fDDYUPRf8u9870OHpNUx0oGc0TcDjwJvC8iWiPiMuBG4PSIeBk4vfCelNKLwF3ASuBnwOdSSrsGqvFV074d1nwPXvgK/OpKePsPpc8Z7Ape+3b1HnccnH46XHttNrhqyJDsdf58eOihvacntbWVDreFC/veto4VqObP79qWISX+l1y4sPR396dtklRlri7VW5sfK3RNlxHGnX3we4MzyAv6P0/4+uuzkOzJkCGwqw//5uoI/e6WhBw6NOua7ul7ofQxfWmbJA0iV5eqhI33w9KP9f68EYdUtoJXKeV09V53Xffnl3PHWWoAVzHF/sHQ8Wz43nvhjDMgio0fLPK9PU236kvbJCknLMlZrs2PwdIzen/efgdlFbwGc+nG/nb1ljNdqdQArmJ6+gfD8uXZc+1Sd7rz5vV+IJsk1RCDuRwdSzT2NMBrX0NGwHFfhtn/p/sKXgM15ae/84RL3XEOH951sFg5+vvst2OQWm8HsklSDTGYy9G6GN7+19LHjZ0BU/5r9jx5zu/hxBu6v1MeyCk/fZmz3FmpO86O58G91dfCIUOH7j1IrafBY/sOZJOkGmMwd6fzyOv1ZU51+vd/Bcd/KRvkVarreiCn/PS3q7fUHem11/atXX199ptS9ky8c+COHp1tW7s26/5eu7brMZJUgwzmYjbeDz8YB09+Cp77clbFq5RdI+GM/1J+l/RATvnpb1fvQN2R9vXZr4O5JDUQp0vt6+UFsOyK3p2zYwh8bTfsO1W5p6lJ5UwL6s+Un1LTkqqhrQ0OPhjeeqt3582f3/MockmqQd1NlzKYO9v5OtwzjrIHeQ0ZAZtPhisfhbe7OWbmTLjvvq5h2KgrLJWaDrWv4cPh9dftopZUd/qzHnNjaN8OT1xIyVCeOGfvAV43r+s+lAEefrj4YK5GnfIztJfLc7/9tqEsqaEYzJCt/rTkKNj0QOljl+2Gs78N7/lz+PfHwquvlnFOkcFcjTrlp6fu+2KOOGJg2iFJOdWYwdwxf/joI2BmwJKTYeem0ue9FXD1PXtPbyr3UcC+g7kadcpPU1Pvjq/XngNJ6kbjPWPumD/8h2VwJfDvyjwvAV8jW56jL6zfnCmnDneHGTPq+x8pkhqaz5g73HQT/Osy+BK9C+Vv0/dQhspN+RmoamGDpacu/OHDG6fnQJK60XiLWPy/34GrKP+fJM8B/wPY3s/vrUSXbE+LQPz4x6VXjcqDji78vE3lkqScaLw75ne3Qrl/978B3Ex5oTxx4sAP5hrIamGDyapdktStxgvmow8o77g3gH+g56lQnV122cAP5hrIamGSpFxovMFfN/8ZvPue7ve/BXwXeIryQ3mwBikNdLUwSdKgcfBXh7/4X9DWzaP13cDfA4/RcyWvak1vKnfVqFofICZJDazxBn8dMA7OfBAeOguGdQqqN4fCdw+Gl1+D4fsVr+c8Y0bx8pqDZd68nqcazZtXHwPEJKmBNd4dM0DTaXDeWvjd+XD/u+DWgK8eBn/yOfjtb+Hkk/eu6TxiBHzxi/AnfwKTJ2d3y8OGZa9HHDF4d6PlVAurlwFiktSgGu8ZMxS/q+wwZEjx57jdbe9w0knw6KMDfzdaatWoRl0cQ5JqTGM8Y25rgxv+Ds4dB+cPgXPHww1f6no329NdZXfhW6rG87PPwpln9u25bm+eCZeaarR+fc/fVWq/JKmq6ueOua0N5k6HM1fBgZ22vwH89Fi4Y9k74VXqrrKSelqTGXq+ey91bjHeMUtSTaj/O+Z/+PuuoQzZ+zNXZfs7DOZdY6nnupV+Jtyoy0lKUp2on2Be/r+6hnKHAwv7O1SqbnW5eir8UemiIY26nKQk1Yn6CeahW8vfP9h3jT3doVf6mXCjLicpSXWifoJ519jy9/d0Vzmkm0vS3fZy9HSHXm7RkN6wFrUk1az6CeY/uiIb6FXMG4X9HXq6q1y9OqvuNXRoduzQodn71avh2muzpQl7q6c7dJ8JS5I6aYxR2T+bDLc/U5k7xmLziD/5Sfj5z2H58q7Hl6qj3dOo7MGqwS1JGnTdjcqun2CGLOT+4e+zgV5Dt2bd1390Bfyna6pf+GOgzpUk1aTGCGZJkmpE/c9jliSpDhjMkiTliMEsSVKOGMySJOWIwSxJUo4YzJIk5YjBLElSjhjMkiTliMEsSVKO5KLyV0RsAdZVux11ahzwWrUbUae8tgPD6zpwvLYDpy/X9siU0vh9N+YimDVwIqKlWMk39Z/XdmB4XQeO13bgVPLa2pUtSVKOGMySJOWIwVz/FlS7AXXMazswvK4Dx2s7cCp2bX3GLElSjnjHLElSjhjMdSAiPh4RL0XE6oi4usj+T0bEc4WfX0bEidVoZy0qdW07HTc9InZFxJzBbF8tK+faRsTMiFgRES9GxCOD3cZaVcbfCe+KiHsj4l8K13ZeNdpZayLiuxGxOSJe6GZ/RMQ/Fq77cxFxUp++KKXkTw3/AEOB3wJHAcOBfwEm73PMB4GDCr+fCTxd7XbXwk8517bTcQ8BPwHmVLvdtfBT5v+3BwIrgSMK7w+pdrtr4afMa3st8N8Kv48HXgeGV7vtef8BTgNOAl7oZv9ZwE+BAE7p69+13jHXvhnA6pTSKymlt4A7gHM7H5BS+mVK6feFt08BTYPcxlpV8toWfAH4AbB5MBtX48q5thcD96SUXgVIKXl9y1POtU3A/hERwBiyYG4f3GbWnpTSo2TXqjvnAv87ZZ4CDoyIQ3v7PQZz7TscWN/pfWthW3cuI/sXnUoreW0j4nBgNvA/B7Fd9aCc/2/fCxwUEQ9HxPKI+I+D1rraVs61/SfgWGAD8Dzw1yml3YPTvLrW27+PixpWseaoWqLItqJD7SNiFlkwf2hAW1Q/yrm2twBXpZR2ZTcfKlM513YY8EfAR4FRwJMR8VRK6TcD3bgaV861PQNYAfwx8B7g/oh4LKX0hwFuW70r++/jnhjMta8VmNjpfRPZv4L3EhFTgG8DZ6aUtg5S22pdOdd2GnBHIZTHAWdFRHtK6YeD0sLaVc61bQVeSym1AW0R8ShwImAw96ycazsPuDFlD0ZXR8Qa4BjgmcFpYt0q6+/jUuzKrn3LgKMjYlJEDAfmAks6HxARRwD3AH/u3UavlLy2KaVJKaXmlFIzcDfwWUO5LCWvLfAj4MMRMSwi/h1wMrBqkNtZi8q5tq+S9UQQEROA9wGvDGor69MS4D8WRmefAvxrSmljbz/EO+Yal1Jqj4jPAz8nG4353ZTSixHxmcL+/wn8F2AscGvhzq49Wci+pDKvrfqgnGubUloVET8DngN2A99OKRWdpqJ3lPn/7X8FbouI58m6X69KKbnqVAkRcTswExgXEa3AdcB+sOe6/oRsZPZqYDtZz0Tvv6cwxFuSJOWAXdmSJOWIwSxJUo4YzJIk5YjBLElSjhjMkiT1QqnFLPY59puFhVhWRMRvIuKNkuc4KluSpPJFxGnANrK62Mf34rwvAO9PKX26p+O8Y5YkqReKLWYREe+JiJ8V6ro/FhHHFDn1IuD2Up9vgRFJkvpvAfCZlNLLEXEycCtZLXIAIuJIYBLZErE9MpglSeqHiBhDtu79/99pMZsR+xw2F7g7pbSr1OcZzJIk9c8Q4I2U0tQejpkLfK7cD5MkSX1UWC5zTURcAFBYxOLEjv0R8T7gIODJcj7PYJYkqRcKi1k8CbwvIloj4jLgk8BlEfEvwIvAuZ1OuQi4I5U5DcrpUpIk5Yh3zJIk5YjBLElSjhjMkiTliMEsSVKOGMySJOWIwSxJUo4YzJIk5YjBLElSjvxfdIva9uMN9kcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#-lavfi psnr=stats_file=%name%_00%%i_psnr.txt -f null -\n",
    "###绘图，看拟合效果###\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(test_x,test_y6,color=\"red\",label=\"Sample Point\",linewidth=3) #画样本点\n",
    "plt.scatter(test_x,test_pre,color=\"orange\",label=\"Fitting Line\",linewidth=2) #画拟合直线\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89c663ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt   \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46e253c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "torch.Size([65, 7]) torch.Size([324, 7]) (325, 7)\n"
     ]
    }
   ],
   "source": [
    "#读数据集\n",
    "data = pd.read_csv(\"F:/video_cut/Bear_size.csv\") \n",
    "#print(data)\n",
    "#scale放里面误差极大，需对3个x训练不同的模型################\n",
    "#X = data[[\"first_frame\", \"frame_TI\",\"frame_SI\",'keypoint','brightness','contrast','edgeLength','Hue1', 'Hue2','Hue3','Hue4',\t'Hue5',\t'Hue6',\t'Hue7',\t'Saturation','Value']].values\n",
    "N4 = [\"4\",\"TI\", 'Hue2','Hue4','Hue5', 'contrast',\t'Saturation']\n",
    "'''[\"4\",\"first_frame\", \"TI\",\"SI\",'keypoint','brightness','edgeLength',\n",
    "      'Hue1', 'Hue2','Hue3','Hue4',\t'Hue5',\t'Hue6',\t'Hue7',\t'Saturation','Value']'''\n",
    "#[\"4\",\"first_frame\",\"TI\",\"SI\",'keypoint','brightness','contrast','edgeLength','Hue1', 'Hue2', 'Hue3','Hue4',\t'Hue5',\t'Hue6',\t'Hue7',\t'Saturation','Value']\n",
    "\n",
    "\n",
    "N6 = [\"6\",\"first_frame\", \"TI\",'keypoint','brightness','edgeLength',\n",
    "      'Hue1', 'Hue2','Hue3','Hue4',\t'Hue5',\t'Hue6',\t'Hue7',\t 'Saturation','Value']\n",
    "#[\"6\",\"first_frame\",\"TI\",\"SI\",'keypoint', 'brightness','contrast','edgeLength','Hue1', 'Hue2', 'Hue4',\t'Hue5',\t'Hue6',\t'Hue7',\t'Saturation','Value']\n",
    "N8 = [\"8\",\"first_frame\", \"TI\",\"SI\",'keypoint','brightness', 'edgeLength',\n",
    "      'Hue1', 'Hue2','Hue3','Hue4', 'Hue5',\t'Hue6',\t'Hue7',\t 'Saturation']\n",
    "#[\"8\",\"first_frame\",\"TI\",\"SI\",'keypoint', 'brightness','contrast','edgeLength','Hue1', 'Hue2','Hue3','Hue4',\t'Hue5',\t'Hue6',\t'Hue7','Saturation','Value']\n",
    "\n",
    "X = data[N4].values##########\n",
    "#现在加入了value\n",
    "Y = data.video4.values.reshape(-1, 1)############\n",
    "\n",
    "train_x,test_x,train_y,test_y=train_test_split(X,Y,train_size=0.999,random_state=1)  #shuffle=False 就是按照顺序划分的测试集和验证集,默认为true才行\n",
    "train_x2,test_x,train_y2,test_y=train_test_split(X,Y,train_size=0.8,random_state=1)  #shuffle=False 就是按照顺序划分的测试集和验证集,默认为true才行\n",
    "\n",
    "print(type(train_x))\n",
    "\n",
    "#将数据转换成Tensor LongTensor等价于int64\n",
    "train_x = torch.from_numpy(train_x).type(torch.FloatTensor)\n",
    "train_y = torch.from_numpy(train_y).type(torch.FloatTensor)\n",
    "test_x = torch.from_numpy(test_x).type(torch.FloatTensor)\n",
    "test_y = torch.from_numpy(test_y).type(torch.FloatTensor)\n",
    "print(test_x.shape,train_x.shape,X.shape)\n",
    "points = int(test_x.shape[1])\n",
    "\n",
    "#数据只有3805行故batchsize设置8较优\n",
    "\n",
    "batch = 8#32 \n",
    "no_of_batches = len(data)//batch\n",
    "epochs = 3000#3000\n",
    "\n",
    "#TensorDataset()可以对tensor进行打包即合并\n",
    "train_ds = TensorDataset(train_x,train_y)\n",
    "#希望模型不关注训练集数据顺序故用乱序\n",
    "train_dl = DataLoader(train_ds,batch_size=batch,shuffle=True)\n",
    "test_ds = TensorDataset(test_x,test_y)\n",
    "#对测试集不需要用乱序避免工作量增加\n",
    "test_dl = DataLoader(test_ds,batch_size=batch)\n",
    "#print(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c79f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建模型\n",
    "#继承nn.Module这个类并自定义模型\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = nn.Linear(points,points*2)  # 输入是一个点对（x1，x2）,所以我们输入的神经节点是两个\n",
    "        self.linear2 = nn.Linear(points*2,points*2, bias=True)\n",
    "        self.linear3 = nn.Linear(points*2,points*2, bias=True)\n",
    "        self.linear4 = nn.Linear(points*2,points*2, bias=True)\n",
    "        self.linear5 = nn.Linear(points*2,points, bias=True)\n",
    "        self.linear6 = nn.Linear(points,1)  # 输出层由于是二分类，所以输出节点是2\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = F.relu(self.linear4(x))\n",
    "        x = F.relu(self.linear5(x))\n",
    "        c =self.linear6(x)\n",
    "        #c = F.softmax(self.linear6(x),dim=1)\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5a06bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 train_loss:  37252.012 train_acc:  0.047 test_loss:  37812.738 test_acc:  0.041\n",
      "epoch:  1 train_loss:  37043.816 train_acc:  0.51 test_loss:  37601.594 test_acc:  0.461\n",
      "epoch:  2 train_loss:  35843.66 train_acc:  3.234 test_loss:  36379.438 test_acc:  2.921\n",
      "epoch:  3 train_loss:  28675.111 train_acc:  20.842 test_loss:  29010.605 test_acc:  18.933\n",
      "epoch:  4 train_loss:  10023.601 train_acc:  62.215 test_loss:  9438.784 test_acc:  68.84\n",
      "epoch:  5 train_loss:  8167.285 train_acc:  54.225 test_loss:  7337.469 test_acc:  64.226\n",
      "epoch:  6 train_loss:  7851.6 train_acc:  56.465 test_loss:  7154.896 test_acc:  65.65\n",
      "epoch:  7 train_loss:  7575.493 train_acc:  55.076 test_loss:  6923.402 test_acc:  64.092\n",
      "epoch:  8 train_loss:  7342.659 train_acc:  56.437 test_loss:  6783.984 test_acc:  64.77\n",
      "epoch:  9 train_loss:  7163.216 train_acc:  56.533 test_loss:  6652.827 test_acc:  64.536\n",
      "epoch:  10 train_loss:  7005.836 train_acc:  58.055 test_loss:  6549.308 test_acc:  65.59\n",
      "epoch:  11 train_loss:  6860.57 train_acc:  58.957 test_loss:  6441.525 test_acc:  66.147\n",
      "epoch:  12 train_loss:  6730.04 train_acc:  57.311 test_loss:  6312.522 test_acc:  64.529\n",
      "epoch:  13 train_loss:  6603.395 train_acc:  60.569 test_loss:  6247.375 test_acc:  67.129\n",
      "epoch:  14 train_loss:  6484.117 train_acc:  60.582 test_loss:  6140.192 test_acc:  66.995\n",
      "epoch:  15 train_loss:  6390.004 train_acc:  61.987 test_loss:  6064.539 test_acc:  68.175\n",
      "epoch:  16 train_loss:  6268.737 train_acc:  61.482 test_loss:  5947.673 test_acc:  67.596\n",
      "epoch:  17 train_loss:  6177.448 train_acc:  60.913 test_loss:  5858.591 test_acc:  67.035\n",
      "epoch:  18 train_loss:  6083.417 train_acc:  61.583 test_loss:  5775.227 test_acc:  67.56\n",
      "epoch:  19 train_loss:  6012.376 train_acc:  63.644 test_loss:  5731.858 test_acc:  69.204\n",
      "epoch:  20 train_loss:  5933.347 train_acc:  63.709 test_loss:  5642.009 test_acc:  69.419\n",
      "epoch:  21 train_loss:  5887.696 train_acc:  65.643 test_loss:  5617.556 test_acc:  71.038\n",
      "epoch:  22 train_loss:  5825.299 train_acc:  65.879 test_loss:  5558.572 test_acc:  71.206\n",
      "epoch:  23 train_loss:  5734.205 train_acc:  63.838 test_loss:  5441.64 test_acc:  69.486\n",
      "epoch:  24 train_loss:  5673.872 train_acc:  65.061 test_loss:  5393.368 test_acc:  70.481\n",
      "epoch:  25 train_loss:  5641.929 train_acc:  66.134 test_loss:  5371.304 test_acc:  71.267\n",
      "epoch:  26 train_loss:  5587.41 train_acc:  66.473 test_loss:  5317.09 test_acc:  71.459\n",
      "epoch:  27 train_loss:  5532.037 train_acc:  65.392 test_loss:  5245.303 test_acc:  70.689\n",
      "epoch:  28 train_loss:  5488.063 train_acc:  65.38 test_loss:  5200.811 test_acc:  70.604\n",
      "epoch:  29 train_loss:  5441.366 train_acc:  66.369 test_loss:  5163.388 test_acc:  71.351\n",
      "epoch:  30 train_loss:  5392.868 train_acc:  66.978 test_loss:  5127.156 test_acc:  71.704\n",
      "epoch:  31 train_loss:  5351.68 train_acc:  66.006 test_loss:  5074.218 test_acc:  70.949\n",
      "epoch:  32 train_loss:  5306.637 train_acc:  67.999 test_loss:  5058.544 test_acc:  72.476\n",
      "epoch:  33 train_loss:  5249.96 train_acc:  67.318 test_loss:  5005.733 test_acc:  71.883\n",
      "epoch:  34 train_loss:  5198.321 train_acc:  67.738 test_loss:  4962.962 test_acc:  72.129\n",
      "epoch:  35 train_loss:  5151.12 train_acc:  68.088 test_loss:  4930.169 test_acc:  72.282\n",
      "epoch:  36 train_loss:  5122.341 train_acc:  70.24 test_loss:  4926.432 test_acc:  74.016\n",
      "epoch:  37 train_loss:  5050.579 train_acc:  67.085 test_loss:  4836.394 test_acc:  71.214\n",
      "epoch:  38 train_loss:  4979.712 train_acc:  69.184 test_loss:  4787.449 test_acc:  72.887\n",
      "epoch:  39 train_loss:  4926.042 train_acc:  68.954 test_loss:  4733.684 test_acc:  72.676\n",
      "epoch:  40 train_loss:  4880.682 train_acc:  70.411 test_loss:  4709.061 test_acc:  73.828\n",
      "epoch:  41 train_loss:  4840.516 train_acc:  68.715 test_loss:  4668.044 test_acc:  72.261\n",
      "epoch:  42 train_loss:  4789.717 train_acc:  71.642 test_loss:  4624.45 test_acc:  74.798\n",
      "epoch:  43 train_loss:  4730.72 train_acc:  70.527 test_loss:  4569.608 test_acc:  73.744\n",
      "epoch:  44 train_loss:  4723.173 train_acc:  73.349 test_loss:  4576.853 test_acc:  76.15\n",
      "epoch:  45 train_loss:  4657.891 train_acc:  69.108 test_loss:  4506.451 test_acc:  72.386\n",
      "epoch:  46 train_loss:  4587.189 train_acc:  70.699 test_loss:  4446.596 test_acc:  73.678\n",
      "epoch:  47 train_loss:  4547.631 train_acc:  70.243 test_loss:  4408.466 test_acc:  73.289\n",
      "epoch:  48 train_loss:  4487.282 train_acc:  72.795 test_loss:  4358.915 test_acc:  75.327\n",
      "epoch:  49 train_loss:  4445.498 train_acc:  70.706 test_loss:  4317.542 test_acc:  73.511\n",
      "epoch:  50 train_loss:  4392.062 train_acc:  73.433 test_loss:  4272.14 test_acc:  75.791\n",
      "epoch:  51 train_loss:  4358.087 train_acc:  70.951 test_loss:  4238.066 test_acc:  73.632\n",
      "epoch:  52 train_loss:  4315.675 train_acc:  70.776 test_loss:  4204.4 test_acc:  73.459\n",
      "epoch:  53 train_loss:  4304.703 train_acc:  70.097 test_loss:  4196.94 test_acc:  72.851\n",
      "epoch:  54 train_loss:  4190.184 train_acc:  74.272 test_loss:  4087.407 test_acc:  76.287\n",
      "epoch:  55 train_loss:  4229.203 train_acc:  77.53 test_loss:  4130.167 test_acc:  79.07\n",
      "epoch:  56 train_loss:  4117.35 train_acc:  76.363 test_loss:  4015.159 test_acc:  77.958\n",
      "epoch:  57 train_loss:  4059.819 train_acc:  73.344 test_loss:  3970.601 test_acc:  75.43\n",
      "epoch:  58 train_loss:  4018.503 train_acc:  76.105 test_loss:  3926.754 test_acc:  77.714\n",
      "epoch:  59 train_loss:  3977.585 train_acc:  73.382 test_loss:  3893.654 test_acc:  75.385\n",
      "epoch:  60 train_loss:  3930.801 train_acc:  74.658 test_loss:  3852.3 test_acc:  76.515\n",
      "epoch:  61 train_loss:  3925.441 train_acc:  72.543 test_loss:  3844.992 test_acc:  74.652\n",
      "epoch:  62 train_loss:  3886.263 train_acc:  78.449 test_loss:  3799.362 test_acc:  79.504\n",
      "epoch:  63 train_loss:  3803.701 train_acc:  76.363 test_loss:  3730.689 test_acc:  77.821\n",
      "epoch:  64 train_loss:  3753.44 train_acc:  77.289 test_loss:  3673.597 test_acc:  78.432\n",
      "epoch:  65 train_loss:  3708.388 train_acc:  77.091 test_loss:  3633.906 test_acc:  78.246\n",
      "epoch:  66 train_loss:  3676.904 train_acc:  78.203 test_loss:  3603.423 test_acc:  79.063\n",
      "epoch:  67 train_loss:  3625.549 train_acc:  76.822 test_loss:  3560.675 test_acc:  77.831\n",
      "epoch:  68 train_loss:  3603.398 train_acc:  78.669 test_loss:  3537.24 test_acc:  79.389\n",
      "epoch:  69 train_loss:  3547.285 train_acc:  77.999 test_loss:  3484.238 test_acc:  78.72\n",
      "epoch:  70 train_loss:  3505.031 train_acc:  78.28 test_loss:  3449.667 test_acc:  78.925\n",
      "epoch:  71 train_loss:  3480.52 train_acc:  79.371 test_loss:  3420.522 test_acc:  79.912\n",
      "epoch:  72 train_loss:  3430.409 train_acc:  76.787 test_loss:  3390.231 test_acc:  77.609\n",
      "epoch:  73 train_loss:  3398.19 train_acc:  79.572 test_loss:  3357.39 test_acc:  80.052\n",
      "epoch:  74 train_loss:  3353.804 train_acc:  79.746 test_loss:  3321.524 test_acc:  80.157\n",
      "epoch:  75 train_loss:  3288.081 train_acc:  78.386 test_loss:  3261.536 test_acc:  78.842\n",
      "epoch:  76 train_loss:  3240.207 train_acc:  78.146 test_loss:  3234.487 test_acc:  78.634\n",
      "epoch:  77 train_loss:  3191.947 train_acc:  77.666 test_loss:  3184.831 test_acc:  78.091\n",
      "epoch:  78 train_loss:  3141.455 train_acc:  80.426 test_loss:  3117.93 test_acc:  80.625\n",
      "epoch:  79 train_loss:  3102.647 train_acc:  80.945 test_loss:  3081.323 test_acc:  81.095\n",
      "epoch:  80 train_loss:  3069.258 train_acc:  81.45 test_loss:  3041.379 test_acc:  81.623\n",
      "epoch:  81 train_loss:  3058.074 train_acc:  77.006 test_loss:  3084.03 test_acc:  77.138\n",
      "epoch:  82 train_loss:  3038.963 train_acc:  82.039 test_loss:  3013.601 test_acc:  82.488\n",
      "epoch:  83 train_loss:  2915.971 train_acc:  80.064 test_loss:  2919.888 test_acc:  79.895\n",
      "epoch:  84 train_loss:  2883.284 train_acc:  79.27 test_loss:  2898.006 test_acc:  79.017\n",
      "epoch:  85 train_loss:  2856.943 train_acc:  79.272 test_loss:  2876.329 test_acc:  78.986\n",
      "epoch:  86 train_loss:  2793.699 train_acc:  80.292 test_loss:  2799.062 test_acc:  79.941\n",
      "epoch:  87 train_loss:  2810.92 train_acc:  78.219 test_loss:  2856.061 test_acc:  77.844\n",
      "epoch:  88 train_loss:  2718.924 train_acc:  82.174 test_loss:  2716.682 test_acc:  81.999\n",
      "epoch:  89 train_loss:  2760.57 train_acc:  78.249 test_loss:  2839.493 test_acc:  77.46\n",
      "epoch:  90 train_loss:  2842.275 train_acc:  82.938 test_loss:  2825.999 test_acc:  83.277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  91 train_loss:  2642.893 train_acc:  79.971 test_loss:  2695.565 test_acc:  79.173\n",
      "epoch:  92 train_loss:  2629.792 train_acc:  83.269 test_loss:  2616.685 test_acc:  83.375\n",
      "epoch:  93 train_loss:  2581.961 train_acc:  80.287 test_loss:  2641.597 test_acc:  79.373\n",
      "epoch:  94 train_loss:  2521.452 train_acc:  82.055 test_loss:  2559.842 test_acc:  81.346\n",
      "epoch:  95 train_loss:  2497.079 train_acc:  82.435 test_loss:  2528.773 test_acc:  81.876\n",
      "epoch:  96 train_loss:  2467.858 train_acc:  82.061 test_loss:  2508.652 test_acc:  81.34\n",
      "epoch:  97 train_loss:  2470.977 train_acc:  83.462 test_loss:  2484.446 test_acc:  83.116\n",
      "epoch:  98 train_loss:  2430.374 train_acc:  81.672 test_loss:  2490.765 test_acc:  80.732\n",
      "epoch:  99 train_loss:  2414.661 train_acc:  83.576 test_loss:  2418.611 test_acc:  83.339\n",
      "epoch:  100 train_loss:  2381.085 train_acc:  81.756 test_loss:  2450.72 test_acc:  80.842\n",
      "epoch:  101 train_loss:  2376.052 train_acc:  81.158 test_loss:  2465.054 test_acc:  79.967\n",
      "epoch:  102 train_loss:  2440.506 train_acc:  84.144 test_loss:  2414.087 test_acc:  84.033\n",
      "epoch:  103 train_loss:  2307.515 train_acc:  83.553 test_loss:  2338.855 test_acc:  82.946\n",
      "epoch:  104 train_loss:  2284.878 train_acc:  82.781 test_loss:  2336.728 test_acc:  81.911\n",
      "epoch:  105 train_loss:  2265.839 train_acc:  83.736 test_loss:  2287.535 test_acc:  82.997\n",
      "epoch:  106 train_loss:  2246.351 train_acc:  83.814 test_loss:  2267.406 test_acc:  83.184\n",
      "epoch:  107 train_loss:  2267.55 train_acc:  81.05 test_loss:  2388.902 test_acc:  79.785\n",
      "epoch:  108 train_loss:  2192.495 train_acc:  83.579 test_loss:  2241.933 test_acc:  82.639\n",
      "epoch:  109 train_loss:  2187.353 train_acc:  82.554 test_loss:  2271.299 test_acc:  81.566\n",
      "epoch:  110 train_loss:  2152.533 train_acc:  83.168 test_loss:  2214.356 test_acc:  82.099\n",
      "epoch:  111 train_loss:  2135.105 train_acc:  83.231 test_loss:  2202.794 test_acc:  82.227\n",
      "epoch:  112 train_loss:  2146.579 train_acc:  84.143 test_loss:  2167.53 test_acc:  83.337\n",
      "epoch:  113 train_loss:  2199.694 train_acc:  84.382 test_loss:  2172.883 test_acc:  83.961\n",
      "epoch:  114 train_loss:  2072.583 train_acc:  83.857 test_loss:  2119.324 test_acc:  82.629\n",
      "epoch:  115 train_loss:  2052.363 train_acc:  84.071 test_loss:  2101.395 test_acc:  82.901\n",
      "epoch:  116 train_loss:  2036.465 train_acc:  84.177 test_loss:  2070.616 test_acc:  83.107\n",
      "epoch:  117 train_loss:  2017.224 train_acc:  83.666 test_loss:  2094.386 test_acc:  82.226\n",
      "epoch:  118 train_loss:  1996.676 train_acc:  84.198 test_loss:  2053.913 test_acc:  82.847\n",
      "epoch:  119 train_loss:  1984.307 train_acc:  84.427 test_loss:  2011.852 test_acc:  83.211\n",
      "epoch:  120 train_loss:  1975.252 train_acc:  83.049 test_loss:  2068.751 test_acc:  81.608\n",
      "epoch:  121 train_loss:  1947.84 train_acc:  83.524 test_loss:  2016.43 test_acc:  82.045\n",
      "epoch:  122 train_loss:  1918.241 train_acc:  84.241 test_loss:  1964.561 test_acc:  82.886\n",
      "epoch:  123 train_loss:  1937.237 train_acc:  84.659 test_loss:  1942.891 test_acc:  83.518\n",
      "epoch:  124 train_loss:  1927.667 train_acc:  84.857 test_loss:  1927.238 test_acc:  83.743\n",
      "epoch:  125 train_loss:  1889.943 train_acc:  82.858 test_loss:  1994.432 test_acc:  81.23\n",
      "epoch:  126 train_loss:  1842.923 train_acc:  83.9 test_loss:  1911.005 test_acc:  82.159\n",
      "epoch:  127 train_loss:  1834.269 train_acc:  83.457 test_loss:  1912.302 test_acc:  81.951\n",
      "epoch:  128 train_loss:  1790.835 train_acc:  84.296 test_loss:  1833.772 test_acc:  82.858\n",
      "epoch:  129 train_loss:  1774.163 train_acc:  84.315 test_loss:  1810.721 test_acc:  82.879\n",
      "epoch:  130 train_loss:  1806.625 train_acc:  82.567 test_loss:  1912.083 test_acc:  81.006\n",
      "epoch:  131 train_loss:  1826.613 train_acc:  84.896 test_loss:  1782.569 test_acc:  84.091\n",
      "epoch:  132 train_loss:  1711.54 train_acc:  84.574 test_loss:  1731.253 test_acc:  83.218\n",
      "epoch:  133 train_loss:  1690.883 train_acc:  84.187 test_loss:  1722.913 test_acc:  82.852\n",
      "epoch:  134 train_loss:  1673.831 train_acc:  84.145 test_loss:  1719.175 test_acc:  82.509\n",
      "epoch:  135 train_loss:  1669.526 train_acc:  84.996 test_loss:  1642.084 test_acc:  84.012\n",
      "epoch:  136 train_loss:  1631.947 train_acc:  84.783 test_loss:  1631.411 test_acc:  83.485\n",
      "epoch:  137 train_loss:  1607.8 train_acc:  84.377 test_loss:  1622.209 test_acc:  83.141\n",
      "epoch:  138 train_loss:  1602.695 train_acc:  84.85 test_loss:  1577.129 test_acc:  83.848\n",
      "epoch:  139 train_loss:  1571.378 train_acc:  84.244 test_loss:  1593.484 test_acc:  83.018\n",
      "epoch:  140 train_loss:  1544.71 train_acc:  84.933 test_loss:  1532.803 test_acc:  83.826\n",
      "epoch:  141 train_loss:  1537.38 train_acc:  83.733 test_loss:  1578.269 test_acc:  82.399\n",
      "epoch:  142 train_loss:  1515.329 train_acc:  85.214 test_loss:  1472.133 test_acc:  84.47\n",
      "epoch:  143 train_loss:  1492.033 train_acc:  84.744 test_loss:  1494.63 test_acc:  83.495\n",
      "epoch:  144 train_loss:  1478.043 train_acc:  84.959 test_loss:  1444.501 test_acc:  84.294\n",
      "epoch:  145 train_loss:  1455.298 train_acc:  85.315 test_loss:  1418.073 test_acc:  84.395\n",
      "epoch:  146 train_loss:  1432.035 train_acc:  85.438 test_loss:  1413.388 test_acc:  84.204\n",
      "epoch:  147 train_loss:  1424.152 train_acc:  84.507 test_loss:  1453.116 test_acc:  83.036\n",
      "epoch:  148 train_loss:  1439.372 train_acc:  85.467 test_loss:  1376.929 test_acc:  84.965\n",
      "epoch:  149 train_loss:  1388.601 train_acc:  84.977 test_loss:  1372.3 test_acc:  83.774\n",
      "epoch:  150 train_loss:  1403.229 train_acc:  85.797 test_loss:  1340.809 test_acc:  85.26\n",
      "epoch:  151 train_loss:  1354.078 train_acc:  84.819 test_loss:  1386.324 test_acc:  83.285\n",
      "epoch:  152 train_loss:  1369.776 train_acc:  85.958 test_loss:  1307.052 test_acc:  85.355\n",
      "epoch:  153 train_loss:  1336.182 train_acc:  84.437 test_loss:  1384.951 test_acc:  82.657\n",
      "epoch:  154 train_loss:  1293.533 train_acc:  85.611 test_loss:  1286.469 test_acc:  84.265\n",
      "epoch:  155 train_loss:  1275.312 train_acc:  85.534 test_loss:  1275.535 test_acc:  84.153\n",
      "epoch:  156 train_loss:  1358.036 train_acc:  86.006 test_loss:  1272.912 test_acc:  85.731\n",
      "epoch:  157 train_loss:  1278.542 train_acc:  84.484 test_loss:  1320.809 test_acc:  82.795\n",
      "epoch:  158 train_loss:  1248.338 train_acc:  84.884 test_loss:  1286.021 test_acc:  83.42\n",
      "epoch:  159 train_loss:  1231.182 train_acc:  84.923 test_loss:  1264.862 test_acc:  83.437\n",
      "epoch:  160 train_loss:  1203.807 train_acc:  85.568 test_loss:  1222.157 test_acc:  83.975\n",
      "epoch:  161 train_loss:  1247.58 train_acc:  83.85 test_loss:  1338.189 test_acc:  82.057\n",
      "epoch:  162 train_loss:  1203.319 train_acc:  86.35 test_loss:  1140.097 test_acc:  85.758\n",
      "epoch:  163 train_loss:  1180.579 train_acc:  85.226 test_loss:  1213.289 test_acc:  83.641\n",
      "epoch:  164 train_loss:  1259.5 train_acc:  85.962 test_loss:  1157.207 test_acc:  86.056\n",
      "epoch:  165 train_loss:  1146.899 train_acc:  85.439 test_loss:  1175.694 test_acc:  84.074\n",
      "epoch:  166 train_loss:  1237.605 train_acc:  86.169 test_loss:  1117.337 test_acc:  86.192\n",
      "epoch:  167 train_loss:  1176.395 train_acc:  86.671 test_loss:  1098.785 test_acc:  86.306\n",
      "epoch:  168 train_loss:  1189.7 train_acc:  86.422 test_loss:  1105.274 test_acc:  86.148\n",
      "epoch:  169 train_loss:  1109.901 train_acc:  85.812 test_loss:  1126.988 test_acc:  84.548\n",
      "epoch:  170 train_loss:  1097.825 train_acc:  86.33 test_loss:  1068.062 test_acc:  85.627\n",
      "epoch:  171 train_loss:  1091.541 train_acc:  85.703 test_loss:  1117.921 test_acc:  84.461\n",
      "epoch:  172 train_loss:  1075.79 train_acc:  86.459 test_loss:  1048.161 test_acc:  85.443\n",
      "epoch:  173 train_loss:  1086.019 train_acc:  85.61 test_loss:  1104.591 test_acc:  84.231\n",
      "epoch:  174 train_loss:  1150.215 train_acc:  84.06 test_loss:  1248.956 test_acc:  82.635\n",
      "epoch:  175 train_loss:  1097.29 train_acc:  85.083 test_loss:  1142.857 test_acc:  83.642\n",
      "epoch:  176 train_loss:  1031.492 train_acc:  86.834 test_loss:  1008.094 test_acc:  85.948\n",
      "epoch:  177 train_loss:  1022.49 train_acc:  86.688 test_loss:  1010.228 test_acc:  85.646\n",
      "epoch:  178 train_loss:  1055.444 train_acc:  87.165 test_loss:  988.402 test_acc:  86.642\n",
      "epoch:  179 train_loss:  1082.453 train_acc:  87.108 test_loss:  1000.066 test_acc:  86.57\n",
      "epoch:  180 train_loss:  1118.235 train_acc:  86.907 test_loss:  1010.624 test_acc:  86.84\n",
      "epoch:  181 train_loss:  1019.805 train_acc:  85.835 test_loss:  1065.12 test_acc:  84.435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  182 train_loss:  984.941 train_acc:  86.97 test_loss:  965.904 test_acc:  86.009\n",
      "epoch:  183 train_loss:  1071.455 train_acc:  84.855 test_loss:  1158.142 test_acc:  83.711\n",
      "epoch:  184 train_loss:  984.991 train_acc:  86.477 test_loss:  1013.164 test_acc:  85.071\n",
      "epoch:  185 train_loss:  971.542 train_acc:  86.679 test_loss:  989.057 test_acc:  85.546\n",
      "epoch:  186 train_loss:  1033.557 train_acc:  87.274 test_loss:  945.382 test_acc:  87.038\n",
      "epoch:  187 train_loss:  961.245 train_acc:  87.495 test_loss:  914.338 test_acc:  86.912\n",
      "epoch:  188 train_loss:  1031.09 train_acc:  86.894 test_loss:  983.181 test_acc:  86.59\n",
      "epoch:  189 train_loss:  950.955 train_acc:  87.683 test_loss:  910.696 test_acc:  87.02\n",
      "epoch:  190 train_loss:  998.152 train_acc:  85.332 test_loss:  1094.06 test_acc:  84.026\n",
      "epoch:  191 train_loss:  964.048 train_acc:  86.18 test_loss:  1004.916 test_acc:  84.891\n",
      "epoch:  192 train_loss:  956.05 train_acc:  86.34 test_loss:  995.191 test_acc:  85.443\n",
      "epoch:  193 train_loss:  928.146 train_acc:  87.727 test_loss:  871.396 test_acc:  87.253\n",
      "epoch:  194 train_loss:  916.36 train_acc:  87.875 test_loss:  861.513 test_acc:  87.259\n",
      "epoch:  195 train_loss:  898.821 train_acc:  87.511 test_loss:  885.957 test_acc:  86.58\n",
      "epoch:  196 train_loss:  931.286 train_acc:  87.961 test_loss:  859.016 test_acc:  87.532\n",
      "epoch:  197 train_loss:  1014.081 train_acc:  84.858 test_loss:  1086.405 test_acc:  83.386\n",
      "epoch:  198 train_loss:  898.842 train_acc:  87.045 test_loss:  900.249 test_acc:  86.103\n",
      "epoch:  199 train_loss:  874.555 train_acc:  87.622 test_loss:  873.928 test_acc:  86.599\n",
      "epoch:  200 train_loss:  874.998 train_acc:  87.937 test_loss:  845.74 test_acc:  87.106\n",
      "epoch:  201 train_loss:  912.438 train_acc:  88.145 test_loss:  839.285 test_acc:  87.541\n",
      "epoch:  202 train_loss:  881.41 train_acc:  87.744 test_loss:  844.888 test_acc:  86.97\n",
      "epoch:  203 train_loss:  854.678 train_acc:  87.748 test_loss:  828.261 test_acc:  86.706\n",
      "epoch:  204 train_loss:  888.512 train_acc:  88.554 test_loss:  808.319 test_acc:  87.947\n",
      "epoch:  205 train_loss:  863.604 train_acc:  86.873 test_loss:  868.957 test_acc:  85.997\n",
      "epoch:  206 train_loss:  874.923 train_acc:  86.63 test_loss:  898.174 test_acc:  85.601\n",
      "epoch:  207 train_loss:  957.634 train_acc:  87.905 test_loss:  834.134 test_acc:  87.901\n",
      "epoch:  208 train_loss:  855.804 train_acc:  88.521 test_loss:  776.683 test_acc:  87.872\n",
      "epoch:  209 train_loss:  853.797 train_acc:  88.483 test_loss:  783.338 test_acc:  88.076\n",
      "epoch:  210 train_loss:  818.567 train_acc:  87.732 test_loss:  803.977 test_acc:  86.819\n",
      "epoch:  211 train_loss:  809.872 train_acc:  88.663 test_loss:  764.798 test_acc:  87.868\n",
      "epoch:  212 train_loss:  820.987 train_acc:  87.543 test_loss:  822.534 test_acc:  86.383\n",
      "epoch:  213 train_loss:  840.328 train_acc:  88.714 test_loss:  761.295 test_acc:  88.161\n",
      "epoch:  214 train_loss:  813.316 train_acc:  87.635 test_loss:  815.363 test_acc:  86.639\n",
      "epoch:  215 train_loss:  799.948 train_acc:  88.012 test_loss:  785.318 test_acc:  86.872\n",
      "epoch:  216 train_loss:  813.049 train_acc:  87.558 test_loss:  845.143 test_acc:  86.844\n",
      "epoch:  217 train_loss:  790.71 train_acc:  88.059 test_loss:  802.064 test_acc:  87.004\n",
      "epoch:  218 train_loss:  789.907 train_acc:  88.81 test_loss:  732.314 test_acc:  88.329\n",
      "epoch:  219 train_loss:  773.626 train_acc:  89.0 test_loss:  735.316 test_acc:  88.337\n",
      "epoch:  220 train_loss:  785.575 train_acc:  88.997 test_loss:  730.491 test_acc:  88.568\n",
      "epoch:  221 train_loss:  788.988 train_acc:  88.24 test_loss:  779.189 test_acc:  87.427\n",
      "epoch:  222 train_loss:  763.868 train_acc:  88.947 test_loss:  729.579 test_acc:  88.128\n",
      "epoch:  223 train_loss:  769.556 train_acc:  88.12 test_loss:  787.263 test_acc:  87.337\n",
      "epoch:  224 train_loss:  765.062 train_acc:  88.93 test_loss:  709.019 test_acc:  88.492\n",
      "epoch:  225 train_loss:  774.538 train_acc:  88.05 test_loss:  773.063 test_acc:  87.588\n",
      "epoch:  226 train_loss:  756.216 train_acc:  88.416 test_loss:  726.555 test_acc:  88.085\n",
      "epoch:  227 train_loss:  747.761 train_acc:  88.381 test_loss:  751.776 test_acc:  87.663\n",
      "epoch:  228 train_loss:  740.515 train_acc:  88.94 test_loss:  710.047 test_acc:  88.511\n",
      "epoch:  229 train_loss:  738.401 train_acc:  88.577 test_loss:  736.076 test_acc:  88.034\n",
      "epoch:  230 train_loss:  738.592 train_acc:  88.224 test_loss:  731.538 test_acc:  87.689\n",
      "epoch:  231 train_loss:  747.417 train_acc:  89.147 test_loss:  735.083 test_acc:  88.377\n",
      "epoch:  232 train_loss:  752.731 train_acc:  88.08 test_loss:  777.712 test_acc:  87.468\n",
      "epoch:  233 train_loss:  720.729 train_acc:  88.943 test_loss:  701.087 test_acc:  88.443\n",
      "epoch:  234 train_loss:  736.498 train_acc:  88.356 test_loss:  746.006 test_acc:  87.613\n",
      "epoch:  235 train_loss:  712.508 train_acc:  89.041 test_loss:  711.224 test_acc:  88.527\n",
      "epoch:  236 train_loss:  849.499 train_acc:  88.68 test_loss:  781.327 test_acc:  87.724\n",
      "epoch:  237 train_loss:  736.296 train_acc:  88.422 test_loss:  738.439 test_acc:  87.859\n",
      "epoch:  238 train_loss:  714.57 train_acc:  88.718 test_loss:  700.532 test_acc:  88.4\n",
      "epoch:  239 train_loss:  695.269 train_acc:  89.284 test_loss:  676.436 test_acc:  88.803\n",
      "epoch:  240 train_loss:  706.581 train_acc:  89.556 test_loss:  671.783 test_acc:  88.881\n",
      "epoch:  241 train_loss:  691.258 train_acc:  89.416 test_loss:  688.693 test_acc:  88.805\n",
      "epoch:  242 train_loss:  722.643 train_acc:  88.923 test_loss:  708.132 test_acc:  88.403\n",
      "epoch:  243 train_loss:  704.957 train_acc:  88.53 test_loss:  723.148 test_acc:  88.176\n",
      "epoch:  244 train_loss:  687.311 train_acc:  88.949 test_loss:  698.545 test_acc:  88.429\n",
      "epoch:  245 train_loss:  701.787 train_acc:  89.668 test_loss:  671.595 test_acc:  89.101\n",
      "epoch:  246 train_loss:  708.28 train_acc:  88.219 test_loss:  716.424 test_acc:  87.811\n",
      "epoch:  247 train_loss:  732.976 train_acc:  88.568 test_loss:  742.369 test_acc:  87.559\n",
      "epoch:  248 train_loss:  670.87 train_acc:  89.134 test_loss:  663.01 test_acc:  88.74\n",
      "epoch:  249 train_loss:  689.247 train_acc:  88.679 test_loss:  717.213 test_acc:  88.147\n",
      "epoch:  250 train_loss:  669.604 train_acc:  89.228 test_loss:  651.586 test_acc:  88.658\n",
      "epoch:  251 train_loss:  672.113 train_acc:  89.769 test_loss:  633.981 test_acc:  89.37\n",
      "epoch:  252 train_loss:  692.189 train_acc:  88.698 test_loss:  695.691 test_acc:  88.024\n",
      "epoch:  253 train_loss:  723.303 train_acc:  87.839 test_loss:  779.138 test_acc:  87.294\n",
      "epoch:  254 train_loss:  732.268 train_acc:  89.134 test_loss:  699.613 test_acc:  88.363\n",
      "epoch:  255 train_loss:  643.44 train_acc:  89.821 test_loss:  615.236 test_acc:  89.389\n",
      "epoch:  256 train_loss:  638.237 train_acc:  89.524 test_loss:  634.448 test_acc:  89.109\n",
      "epoch:  257 train_loss:  671.511 train_acc:  89.005 test_loss:  670.942 test_acc:  88.196\n",
      "epoch:  258 train_loss:  680.603 train_acc:  89.747 test_loss:  651.796 test_acc:  89.052\n",
      "epoch:  259 train_loss:  805.228 train_acc:  86.192 test_loss:  883.065 test_acc:  85.949\n",
      "epoch:  260 train_loss:  627.427 train_acc:  89.834 test_loss:  609.254 test_acc:  89.35\n",
      "epoch:  261 train_loss:  643.222 train_acc:  89.046 test_loss:  644.936 test_acc:  88.804\n",
      "epoch:  262 train_loss:  641.627 train_acc:  89.581 test_loss:  630.57 test_acc:  88.953\n",
      "epoch:  263 train_loss:  634.906 train_acc:  89.34 test_loss:  618.03 test_acc:  89.051\n",
      "epoch:  264 train_loss:  621.422 train_acc:  89.961 test_loss:  594.643 test_acc:  89.457\n",
      "epoch:  265 train_loss:  619.496 train_acc:  89.51 test_loss:  617.34 test_acc:  89.259\n",
      "epoch:  266 train_loss:  620.839 train_acc:  89.339 test_loss:  632.818 test_acc:  88.986\n",
      "epoch:  267 train_loss:  647.479 train_acc:  89.709 test_loss:  620.243 test_acc:  89.467\n",
      "epoch:  268 train_loss:  645.047 train_acc:  88.952 test_loss:  636.374 test_acc:  88.362\n",
      "epoch:  269 train_loss:  636.011 train_acc:  89.959 test_loss:  595.006 test_acc:  89.649\n",
      "epoch:  270 train_loss:  618.277 train_acc:  89.335 test_loss:  614.375 test_acc:  89.198\n",
      "epoch:  271 train_loss:  604.176 train_acc:  89.732 test_loss:  598.251 test_acc:  89.406\n",
      "epoch:  272 train_loss:  658.159 train_acc:  88.772 test_loss:  675.932 test_acc:  87.819\n",
      "epoch:  273 train_loss:  620.873 train_acc:  89.97 test_loss:  593.777 test_acc:  89.464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  274 train_loss:  624.946 train_acc:  89.785 test_loss:  608.786 test_acc:  89.05\n",
      "epoch:  275 train_loss:  655.275 train_acc:  90.008 test_loss:  618.721 test_acc:  89.17\n",
      "epoch:  276 train_loss:  591.588 train_acc:  89.91 test_loss:  580.813 test_acc:  89.54\n",
      "epoch:  277 train_loss:  696.186 train_acc:  89.762 test_loss:  652.568 test_acc:  89.512\n",
      "epoch:  278 train_loss:  698.216 train_acc:  89.433 test_loss:  619.324 test_acc:  89.241\n",
      "epoch:  279 train_loss:  599.349 train_acc:  90.189 test_loss:  599.92 test_acc:  89.7\n",
      "epoch:  280 train_loss:  589.036 train_acc:  90.231 test_loss:  569.543 test_acc:  89.763\n",
      "epoch:  281 train_loss:  676.71 train_acc:  87.971 test_loss:  712.236 test_acc:  88.119\n",
      "epoch:  282 train_loss:  623.18 train_acc:  88.928 test_loss:  643.393 test_acc:  88.667\n",
      "epoch:  283 train_loss:  648.035 train_acc:  89.797 test_loss:  635.383 test_acc:  89.549\n",
      "epoch:  284 train_loss:  578.269 train_acc:  89.811 test_loss:  576.893 test_acc:  89.427\n",
      "epoch:  285 train_loss:  582.266 train_acc:  90.285 test_loss:  556.086 test_acc:  89.782\n",
      "epoch:  286 train_loss:  597.66 train_acc:  89.841 test_loss:  602.481 test_acc:  89.021\n",
      "epoch:  287 train_loss:  581.865 train_acc:  89.797 test_loss:  569.257 test_acc:  89.586\n",
      "epoch:  288 train_loss:  612.513 train_acc:  89.717 test_loss:  604.926 test_acc:  88.882\n",
      "epoch:  289 train_loss:  591.803 train_acc:  89.405 test_loss:  581.814 test_acc:  89.227\n",
      "epoch:  290 train_loss:  626.436 train_acc:  88.765 test_loss:  620.742 test_acc:  88.634\n",
      "epoch:  291 train_loss:  643.049 train_acc:  88.243 test_loss:  706.509 test_acc:  88.279\n",
      "epoch:  292 train_loss:  571.355 train_acc:  90.191 test_loss:  566.434 test_acc:  89.691\n",
      "epoch:  293 train_loss:  577.295 train_acc:  90.126 test_loss:  592.372 test_acc:  89.32\n",
      "epoch:  294 train_loss:  579.519 train_acc:  90.401 test_loss:  561.934 test_acc:  89.736\n",
      "epoch:  295 train_loss:  595.196 train_acc:  89.31 test_loss:  632.791 test_acc:  88.536\n",
      "epoch:  296 train_loss:  585.797 train_acc:  90.13 test_loss:  582.293 test_acc:  89.237\n",
      "epoch:  297 train_loss:  643.589 train_acc:  88.341 test_loss:  690.85 test_acc:  88.297\n",
      "epoch:  298 train_loss:  562.544 train_acc:  90.022 test_loss:  569.585 test_acc:  89.46\n",
      "epoch:  299 train_loss:  558.363 train_acc:  90.116 test_loss:  567.39 test_acc:  89.67\n",
      "epoch:  300 train_loss:  555.877 train_acc:  89.836 test_loss:  566.112 test_acc:  89.446\n",
      "epoch:  301 train_loss:  584.656 train_acc:  89.332 test_loss:  616.178 test_acc:  89.183\n",
      "epoch:  302 train_loss:  559.045 train_acc:  90.432 test_loss:  572.608 test_acc:  89.721\n",
      "epoch:  303 train_loss:  569.899 train_acc:  89.4 test_loss:  579.915 test_acc:  89.302\n",
      "epoch:  304 train_loss:  609.857 train_acc:  88.869 test_loss:  637.593 test_acc:  88.745\n",
      "epoch:  305 train_loss:  584.754 train_acc:  90.067 test_loss:  577.782 test_acc:  89.138\n",
      "epoch:  306 train_loss:  583.541 train_acc:  89.283 test_loss:  595.444 test_acc:  89.067\n",
      "epoch:  307 train_loss:  570.708 train_acc:  90.131 test_loss:  567.344 test_acc:  89.592\n",
      "epoch:  308 train_loss:  553.902 train_acc:  89.585 test_loss:  607.852 test_acc:  89.251\n",
      "epoch:  309 train_loss:  647.872 train_acc:  89.382 test_loss:  642.529 test_acc:  89.353\n",
      "epoch:  310 train_loss:  539.674 train_acc:  90.403 test_loss:  516.459 test_acc:  90.089\n",
      "max acc epoch：310        max acc：90.089\n",
      "epoch:  311 train_loss:  532.437 train_acc:  90.277 test_loss:  544.085 test_acc:  89.804\n",
      "epoch:  312 train_loss:  567.207 train_acc:  89.9 test_loss:  592.073 test_acc:  89.093\n",
      "epoch:  313 train_loss:  590.167 train_acc:  90.087 test_loss:  602.539 test_acc:  89.002\n",
      "epoch:  314 train_loss:  542.54 train_acc:  90.034 test_loss:  564.286 test_acc:  89.728\n",
      "epoch:  315 train_loss:  573.552 train_acc:  89.336 test_loss:  601.011 test_acc:  88.922\n",
      "epoch:  316 train_loss:  538.297 train_acc:  89.984 test_loss:  561.295 test_acc:  89.709\n",
      "epoch:  317 train_loss:  552.749 train_acc:  90.487 test_loss:  549.156 test_acc:  89.556\n",
      "epoch:  318 train_loss:  570.451 train_acc:  89.221 test_loss:  633.652 test_acc:  88.714\n",
      "epoch:  319 train_loss:  559.252 train_acc:  89.501 test_loss:  588.274 test_acc:  89.529\n",
      "epoch:  320 train_loss:  552.649 train_acc:  89.564 test_loss:  631.272 test_acc:  88.503\n",
      "epoch:  321 train_loss:  516.899 train_acc:  90.465 test_loss:  544.385 test_acc:  89.881\n",
      "epoch:  322 train_loss:  532.857 train_acc:  90.699 test_loss:  528.559 test_acc:  90.045\n",
      "epoch:  323 train_loss:  549.058 train_acc:  90.074 test_loss:  539.035 test_acc:  90.019\n",
      "epoch:  324 train_loss:  519.892 train_acc:  90.216 test_loss:  548.595 test_acc:  89.538\n",
      "epoch:  325 train_loss:  530.953 train_acc:  90.092 test_loss:  556.303 test_acc:  89.565\n",
      "epoch:  326 train_loss:  553.183 train_acc:  89.348 test_loss:  601.907 test_acc:  89.293\n",
      "epoch:  327 train_loss:  554.796 train_acc:  90.426 test_loss:  565.293 test_acc:  89.475\n",
      "epoch:  328 train_loss:  553.754 train_acc:  90.547 test_loss:  572.849 test_acc:  89.333\n",
      "epoch:  329 train_loss:  506.184 train_acc:  90.56 test_loss:  537.497 test_acc:  89.708\n",
      "epoch:  330 train_loss:  520.536 train_acc:  90.562 test_loss:  534.146 test_acc:  90.009\n",
      "epoch:  331 train_loss:  514.05 train_acc:  90.612 test_loss:  541.01 test_acc:  90.201\n",
      "max acc epoch：331        max acc：90.201\n",
      "epoch:  332 train_loss:  553.114 train_acc:  90.457 test_loss:  566.018 test_acc:  89.891\n",
      "epoch:  333 train_loss:  508.134 train_acc:  90.446 test_loss:  521.799 test_acc:  90.23\n",
      "max acc epoch：333        max acc：90.23\n",
      "epoch:  334 train_loss:  578.277 train_acc:  88.712 test_loss:  652.961 test_acc:  88.636\n",
      "epoch:  335 train_loss:  554.201 train_acc:  89.107 test_loss:  658.945 test_acc:  88.229\n",
      "epoch:  336 train_loss:  521.574 train_acc:  90.511 test_loss:  545.953 test_acc:  90.177\n",
      "epoch:  337 train_loss:  557.754 train_acc:  90.424 test_loss:  584.906 test_acc:  89.484\n",
      "epoch:  338 train_loss:  527.665 train_acc:  89.54 test_loss:  588.626 test_acc:  89.503\n",
      "epoch:  339 train_loss:  536.273 train_acc:  90.345 test_loss:  584.966 test_acc:  89.846\n",
      "epoch:  340 train_loss:  563.553 train_acc:  88.99 test_loss:  651.544 test_acc:  88.665\n",
      "epoch:  341 train_loss:  498.115 train_acc:  90.353 test_loss:  541.45 test_acc:  89.821\n",
      "epoch:  342 train_loss:  488.247 train_acc:  90.89 test_loss:  517.395 test_acc:  90.008\n",
      "epoch:  343 train_loss:  506.685 train_acc:  89.725 test_loss:  598.842 test_acc:  89.245\n",
      "epoch:  344 train_loss:  589.605 train_acc:  89.785 test_loss:  657.525 test_acc:  87.95\n",
      "epoch:  345 train_loss:  477.967 train_acc:  90.899 test_loss:  521.633 test_acc:  90.014\n",
      "epoch:  346 train_loss:  498.254 train_acc:  90.001 test_loss:  570.889 test_acc:  89.659\n",
      "epoch:  347 train_loss:  481.357 train_acc:  90.68 test_loss:  535.265 test_acc:  89.867\n",
      "epoch:  348 train_loss:  488.562 train_acc:  90.287 test_loss:  573.647 test_acc:  88.95\n",
      "epoch:  349 train_loss:  487.846 train_acc:  90.775 test_loss:  560.368 test_acc:  89.426\n",
      "epoch:  350 train_loss:  552.55 train_acc:  90.081 test_loss:  620.598 test_acc:  88.23\n",
      "epoch:  351 train_loss:  474.625 train_acc:  90.663 test_loss:  533.917 test_acc:  89.471\n",
      "epoch:  352 train_loss:  513.275 train_acc:  89.36 test_loss:  642.275 test_acc:  88.63\n",
      "epoch:  353 train_loss:  479.127 train_acc:  90.757 test_loss:  531.586 test_acc:  89.421\n",
      "epoch:  354 train_loss:  482.222 train_acc:  90.877 test_loss:  523.053 test_acc:  89.995\n",
      "epoch:  355 train_loss:  496.822 train_acc:  90.994 test_loss:  555.368 test_acc:  89.489\n",
      "epoch:  356 train_loss:  475.526 train_acc:  90.919 test_loss:  537.629 test_acc:  90.092\n",
      "epoch:  357 train_loss:  476.261 train_acc:  90.299 test_loss:  552.609 test_acc:  89.978\n",
      "epoch:  358 train_loss:  542.254 train_acc:  89.956 test_loss:  614.735 test_acc:  88.057\n",
      "epoch:  359 train_loss:  496.029 train_acc:  90.973 test_loss:  552.318 test_acc:  89.614\n",
      "epoch:  360 train_loss:  468.096 train_acc:  90.875 test_loss:  506.239 test_acc:  90.214\n",
      "epoch:  361 train_loss:  487.69 train_acc:  90.04 test_loss:  578.165 test_acc:  89.571\n",
      "epoch:  362 train_loss:  462.768 train_acc:  90.311 test_loss:  557.295 test_acc:  89.112\n",
      "epoch:  363 train_loss:  542.564 train_acc:  89.198 test_loss:  648.343 test_acc:  89.062\n",
      "epoch:  364 train_loss:  507.405 train_acc:  89.556 test_loss:  616.931 test_acc:  88.442\n",
      "epoch:  365 train_loss:  455.464 train_acc:  90.868 test_loss:  542.63 test_acc:  89.314\n",
      "epoch:  366 train_loss:  466.501 train_acc:  90.823 test_loss:  543.583 test_acc:  89.058\n",
      "epoch:  367 train_loss:  451.313 train_acc:  90.796 test_loss:  523.546 test_acc:  90.113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  368 train_loss:  476.884 train_acc:  90.318 test_loss:  565.654 test_acc:  89.611\n",
      "epoch:  369 train_loss:  449.484 train_acc:  90.85 test_loss:  505.036 test_acc:  90.076\n",
      "epoch:  370 train_loss:  438.673 train_acc:  91.098 test_loss:  487.679 test_acc:  89.94\n",
      "epoch:  371 train_loss:  549.801 train_acc:  88.617 test_loss:  669.44 test_acc:  87.785\n",
      "epoch:  372 train_loss:  454.052 train_acc:  91.088 test_loss:  490.452 test_acc:  89.9\n",
      "epoch:  373 train_loss:  460.704 train_acc:  90.361 test_loss:  543.593 test_acc:  89.163\n",
      "epoch:  374 train_loss:  458.473 train_acc:  90.512 test_loss:  561.8 test_acc:  89.641\n",
      "epoch:  375 train_loss:  488.365 train_acc:  90.403 test_loss:  566.165 test_acc:  88.499\n",
      "epoch:  376 train_loss:  460.868 train_acc:  90.358 test_loss:  569.176 test_acc:  89.849\n",
      "epoch:  377 train_loss:  442.792 train_acc:  90.79 test_loss:  518.446 test_acc:  89.323\n",
      "epoch:  378 train_loss:  424.936 train_acc:  91.138 test_loss:  476.269 test_acc:  90.282\n",
      "max acc epoch：378        max acc：90.282\n",
      "epoch:  379 train_loss:  505.712 train_acc:  89.5 test_loss:  540.542 test_acc:  89.277\n",
      "epoch:  380 train_loss:  439.247 train_acc:  90.769 test_loss:  495.936 test_acc:  89.525\n",
      "epoch:  381 train_loss:  444.393 train_acc:  90.6 test_loss:  485.759 test_acc:  90.077\n",
      "epoch:  382 train_loss:  469.518 train_acc:  90.85 test_loss:  493.936 test_acc:  89.8\n",
      "epoch:  383 train_loss:  423.974 train_acc:  91.177 test_loss:  466.768 test_acc:  90.257\n",
      "epoch:  384 train_loss:  456.38 train_acc:  90.391 test_loss:  533.388 test_acc:  90.007\n",
      "epoch:  385 train_loss:  416.458 train_acc:  91.226 test_loss:  466.589 test_acc:  90.493\n",
      "max acc epoch：385        max acc：90.493\n",
      "epoch:  386 train_loss:  428.113 train_acc:  90.762 test_loss:  491.338 test_acc:  90.028\n",
      "epoch:  387 train_loss:  482.89 train_acc:  89.67 test_loss:  531.011 test_acc:  89.826\n",
      "epoch:  388 train_loss:  443.091 train_acc:  90.936 test_loss:  484.6 test_acc:  89.823\n",
      "epoch:  389 train_loss:  417.341 train_acc:  91.36 test_loss:  454.958 test_acc:  90.388\n",
      "epoch:  390 train_loss:  458.609 train_acc:  90.122 test_loss:  550.373 test_acc:  89.544\n",
      "epoch:  391 train_loss:  431.06 train_acc:  90.549 test_loss:  492.991 test_acc:  90.257\n",
      "epoch:  392 train_loss:  403.976 train_acc:  91.342 test_loss:  455.682 test_acc:  90.303\n",
      "epoch:  393 train_loss:  439.266 train_acc:  90.455 test_loss:  526.128 test_acc:  88.621\n",
      "epoch:  394 train_loss:  417.404 train_acc:  90.889 test_loss:  479.774 test_acc:  90.355\n",
      "epoch:  395 train_loss:  464.039 train_acc:  90.056 test_loss:  538.203 test_acc:  90.003\n",
      "epoch:  396 train_loss:  402.21 train_acc:  91.066 test_loss:  456.07 test_acc:  90.071\n",
      "epoch:  397 train_loss:  452.771 train_acc:  91.041 test_loss:  498.291 test_acc:  89.628\n",
      "epoch:  398 train_loss:  420.09 train_acc:  90.497 test_loss:  509.531 test_acc:  89.479\n",
      "epoch:  399 train_loss:  423.822 train_acc:  90.814 test_loss:  453.514 test_acc:  90.573\n",
      "max acc epoch：399        max acc：90.573\n",
      "epoch:  400 train_loss:  422.034 train_acc:  91.283 test_loss:  478.052 test_acc:  90.324\n",
      "epoch:  401 train_loss:  445.676 train_acc:  89.887 test_loss:  555.661 test_acc:  88.494\n",
      "epoch:  402 train_loss:  417.358 train_acc:  90.797 test_loss:  502.303 test_acc:  90.209\n",
      "epoch:  403 train_loss:  423.492 train_acc:  90.97 test_loss:  515.627 test_acc:  89.178\n",
      "epoch:  404 train_loss:  420.191 train_acc:  90.629 test_loss:  505.561 test_acc:  89.844\n",
      "epoch:  405 train_loss:  443.422 train_acc:  90.165 test_loss:  512.132 test_acc:  89.972\n",
      "epoch:  406 train_loss:  437.628 train_acc:  91.009 test_loss:  481.748 test_acc:  90.201\n",
      "epoch:  407 train_loss:  407.784 train_acc:  91.273 test_loss:  471.153 test_acc:  89.814\n",
      "epoch:  408 train_loss:  391.356 train_acc:  91.37 test_loss:  447.571 test_acc:  90.311\n",
      "epoch:  409 train_loss:  420.656 train_acc:  90.83 test_loss:  485.163 test_acc:  89.744\n",
      "epoch:  410 train_loss:  429.414 train_acc:  91.121 test_loss:  458.229 test_acc:  90.707\n",
      "max acc epoch：410        max acc：90.707\n",
      "epoch:  411 train_loss:  403.585 train_acc:  91.182 test_loss:  453.811 test_acc:  89.982\n",
      "epoch:  412 train_loss:  383.56 train_acc:  91.575 test_loss:  435.073 test_acc:  90.685\n",
      "epoch:  413 train_loss:  378.517 train_acc:  91.548 test_loss:  418.708 test_acc:  90.748\n",
      "max acc epoch：413        max acc：90.748\n",
      "epoch:  414 train_loss:  427.153 train_acc:  90.976 test_loss:  498.417 test_acc:  89.263\n",
      "epoch:  415 train_loss:  376.858 train_acc:  91.509 test_loss:  446.002 test_acc:  90.662\n",
      "epoch:  416 train_loss:  438.607 train_acc:  90.137 test_loss:  519.166 test_acc:  89.759\n",
      "epoch:  417 train_loss:  396.588 train_acc:  91.111 test_loss:  486.942 test_acc:  90.243\n",
      "epoch:  418 train_loss:  407.634 train_acc:  90.459 test_loss:  497.031 test_acc:  89.987\n",
      "epoch:  419 train_loss:  384.009 train_acc:  91.254 test_loss:  472.185 test_acc:  90.103\n",
      "epoch:  420 train_loss:  378.286 train_acc:  91.416 test_loss:  410.074 test_acc:  90.801\n",
      "max acc epoch：420        max acc：90.801\n",
      "epoch:  421 train_loss:  368.436 train_acc:  91.569 test_loss:  432.768 test_acc:  90.375\n",
      "epoch:  422 train_loss:  380.019 train_acc:  91.417 test_loss:  450.009 test_acc:  90.494\n",
      "epoch:  423 train_loss:  413.885 train_acc:  91.181 test_loss:  448.37 test_acc:  90.462\n",
      "epoch:  424 train_loss:  399.169 train_acc:  90.89 test_loss:  480.847 test_acc:  90.287\n",
      "epoch:  425 train_loss:  379.498 train_acc:  91.27 test_loss:  467.38 test_acc:  89.865\n",
      "epoch:  426 train_loss:  367.002 train_acc:  91.445 test_loss:  426.758 test_acc:  90.456\n",
      "epoch:  427 train_loss:  362.422 train_acc:  91.77 test_loss:  445.72 test_acc:  90.434\n",
      "epoch:  428 train_loss:  389.136 train_acc:  91.557 test_loss:  450.327 test_acc:  90.466\n",
      "epoch:  429 train_loss:  364.44 train_acc:  91.562 test_loss:  419.428 test_acc:  90.515\n",
      "epoch:  430 train_loss:  428.417 train_acc:  90.356 test_loss:  560.61 test_acc:  89.017\n",
      "epoch:  431 train_loss:  364.088 train_acc:  91.63 test_loss:  439.07 test_acc:  90.179\n",
      "epoch:  432 train_loss:  365.669 train_acc:  91.807 test_loss:  412.759 test_acc:  90.812\n",
      "max acc epoch：432        max acc：90.812\n",
      "epoch:  433 train_loss:  371.71 train_acc:  91.738 test_loss:  434.872 test_acc:  90.277\n",
      "epoch:  434 train_loss:  378.529 train_acc:  91.273 test_loss:  460.338 test_acc:  90.111\n",
      "epoch:  435 train_loss:  384.788 train_acc:  91.144 test_loss:  470.509 test_acc:  89.665\n",
      "epoch:  436 train_loss:  355.528 train_acc:  91.794 test_loss:  418.506 test_acc:  90.688\n",
      "epoch:  437 train_loss:  393.57 train_acc:  91.035 test_loss:  445.099 test_acc:  90.544\n",
      "epoch:  438 train_loss:  360.05 train_acc:  91.922 test_loss:  406.465 test_acc:  90.666\n",
      "epoch:  439 train_loss:  414.842 train_acc:  90.804 test_loss:  484.08 test_acc:  90.189\n",
      "epoch:  440 train_loss:  382.1 train_acc:  90.992 test_loss:  440.042 test_acc:  90.567\n",
      "epoch:  441 train_loss:  423.688 train_acc:  90.528 test_loss:  524.841 test_acc:  90.061\n",
      "epoch:  442 train_loss:  496.868 train_acc:  89.014 test_loss:  606.33 test_acc:  88.678\n",
      "epoch:  443 train_loss:  364.708 train_acc:  91.731 test_loss:  459.678 test_acc:  90.167\n",
      "epoch:  444 train_loss:  350.142 train_acc:  91.738 test_loss:  434.632 test_acc:  90.456\n",
      "epoch:  445 train_loss:  372.292 train_acc:  91.647 test_loss:  427.837 test_acc:  90.877\n",
      "max acc epoch：445        max acc：90.877\n",
      "epoch:  446 train_loss:  366.027 train_acc:  91.511 test_loss:  404.583 test_acc:  90.98\n",
      "max acc epoch：446        max acc：90.98\n",
      "epoch:  447 train_loss:  376.238 train_acc:  91.79 test_loss:  454.086 test_acc:  90.091\n",
      "epoch:  448 train_loss:  352.567 train_acc:  91.696 test_loss:  436.989 test_acc:  90.834\n",
      "epoch:  449 train_loss:  466.499 train_acc:  89.372 test_loss:  520.963 test_acc:  89.347\n",
      "epoch:  450 train_loss:  369.946 train_acc:  91.571 test_loss:  487.682 test_acc:  89.966\n",
      "epoch:  451 train_loss:  356.367 train_acc:  91.787 test_loss:  443.647 test_acc:  90.243\n",
      "epoch:  452 train_loss:  345.027 train_acc:  91.713 test_loss:  415.376 test_acc:  90.974\n",
      "epoch:  453 train_loss:  344.231 train_acc:  92.003 test_loss:  408.864 test_acc:  91.053\n",
      "max acc epoch：453        max acc：91.053\n",
      "epoch:  454 train_loss:  424.032 train_acc:  91.375 test_loss:  497.448 test_acc:  89.742\n",
      "epoch:  455 train_loss:  340.04 train_acc:  92.0 test_loss:  428.191 test_acc:  90.572\n",
      "epoch:  456 train_loss:  364.823 train_acc:  91.794 test_loss:  430.376 test_acc:  90.403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  457 train_loss:  355.857 train_acc:  91.657 test_loss:  452.338 test_acc:  90.064\n",
      "epoch:  458 train_loss:  350.281 train_acc:  91.892 test_loss:  415.477 test_acc:  91.106\n",
      "max acc epoch：458        max acc：91.106\n",
      "epoch:  459 train_loss:  336.685 train_acc:  92.02 test_loss:  387.488 test_acc:  91.143\n",
      "max acc epoch：459        max acc：91.143\n",
      "epoch:  460 train_loss:  340.853 train_acc:  91.889 test_loss:  430.951 test_acc:  90.501\n",
      "epoch:  461 train_loss:  347.68 train_acc:  92.193 test_loss:  419.416 test_acc:  90.944\n",
      "epoch:  462 train_loss:  432.53 train_acc:  89.892 test_loss:  535.884 test_acc:  89.609\n",
      "epoch:  463 train_loss:  360.064 train_acc:  91.804 test_loss:  389.539 test_acc:  91.204\n",
      "max acc epoch：463        max acc：91.204\n",
      "epoch:  464 train_loss:  350.649 train_acc:  92.121 test_loss:  406.555 test_acc:  91.16\n",
      "epoch:  465 train_loss:  348.265 train_acc:  91.772 test_loss:  441.677 test_acc:  90.643\n",
      "epoch:  466 train_loss:  401.616 train_acc:  91.371 test_loss:  481.541 test_acc:  89.923\n",
      "epoch:  467 train_loss:  343.702 train_acc:  92.009 test_loss:  396.146 test_acc:  91.196\n",
      "epoch:  468 train_loss:  379.754 train_acc:  91.724 test_loss:  436.599 test_acc:  90.294\n",
      "epoch:  469 train_loss:  346.194 train_acc:  91.625 test_loss:  406.007 test_acc:  90.7\n",
      "epoch:  470 train_loss:  347.337 train_acc:  92.092 test_loss:  410.242 test_acc:  90.536\n",
      "epoch:  471 train_loss:  333.103 train_acc:  91.916 test_loss:  389.02 test_acc:  91.098\n",
      "epoch:  472 train_loss:  341.786 train_acc:  92.117 test_loss:  432.798 test_acc:  91.055\n",
      "epoch:  473 train_loss:  353.513 train_acc:  91.567 test_loss:  450.811 test_acc:  90.018\n",
      "epoch:  474 train_loss:  347.13 train_acc:  92.054 test_loss:  437.107 test_acc:  90.518\n",
      "epoch:  475 train_loss:  354.596 train_acc:  91.899 test_loss:  439.454 test_acc:  90.454\n",
      "epoch:  476 train_loss:  338.393 train_acc:  91.822 test_loss:  410.141 test_acc:  91.142\n",
      "epoch:  477 train_loss:  394.1 train_acc:  91.545 test_loss:  468.89 test_acc:  89.749\n",
      "epoch:  478 train_loss:  356.36 train_acc:  91.957 test_loss:  431.023 test_acc:  90.378\n",
      "epoch:  479 train_loss:  391.265 train_acc:  91.029 test_loss:  475.349 test_acc:  90.49\n",
      "epoch:  480 train_loss:  338.38 train_acc:  92.05 test_loss:  384.044 test_acc:  91.422\n",
      "max acc epoch：480        max acc：91.422\n",
      "epoch:  481 train_loss:  340.53 train_acc:  92.287 test_loss:  430.874 test_acc:  90.774\n",
      "epoch:  482 train_loss:  357.232 train_acc:  91.364 test_loss:  444.329 test_acc:  90.21\n",
      "epoch:  483 train_loss:  330.153 train_acc:  92.398 test_loss:  397.343 test_acc:  91.112\n",
      "epoch:  484 train_loss:  339.811 train_acc:  91.839 test_loss:  435.49 test_acc:  90.943\n",
      "epoch:  485 train_loss:  328.44 train_acc:  92.092 test_loss:  397.754 test_acc:  91.307\n",
      "epoch:  486 train_loss:  352.305 train_acc:  92.048 test_loss:  412.414 test_acc:  90.664\n",
      "epoch:  487 train_loss:  316.945 train_acc:  92.27 test_loss:  390.457 test_acc:  91.018\n",
      "epoch:  488 train_loss:  385.043 train_acc:  91.904 test_loss:  411.32 test_acc:  90.706\n",
      "epoch:  489 train_loss:  369.88 train_acc:  91.509 test_loss:  417.854 test_acc:  91.044\n",
      "epoch:  490 train_loss:  351.209 train_acc:  92.087 test_loss:  428.539 test_acc:  90.77\n",
      "epoch:  491 train_loss:  341.132 train_acc:  91.86 test_loss:  401.25 test_acc:  90.949\n",
      "epoch:  492 train_loss:  346.589 train_acc:  91.522 test_loss:  426.882 test_acc:  90.775\n",
      "epoch:  493 train_loss:  362.484 train_acc:  91.653 test_loss:  390.387 test_acc:  91.316\n",
      "epoch:  494 train_loss:  339.499 train_acc:  92.098 test_loss:  402.857 test_acc:  90.94\n",
      "epoch:  495 train_loss:  320.324 train_acc:  92.3 test_loss:  410.595 test_acc:  90.958\n",
      "epoch:  496 train_loss:  323.398 train_acc:  92.326 test_loss:  382.993 test_acc:  91.224\n",
      "epoch:  497 train_loss:  345.179 train_acc:  92.348 test_loss:  398.365 test_acc:  90.987\n",
      "epoch:  498 train_loss:  321.885 train_acc:  92.104 test_loss:  400.789 test_acc:  91.305\n",
      "epoch:  499 train_loss:  417.24 train_acc:  91.612 test_loss:  482.158 test_acc:  90.211\n",
      "epoch:  500 train_loss:  348.858 train_acc:  92.102 test_loss:  436.384 test_acc:  90.455\n",
      "epoch:  501 train_loss:  316.233 train_acc:  92.516 test_loss:  373.256 test_acc:  91.25\n",
      "epoch:  502 train_loss:  346.574 train_acc:  91.648 test_loss:  432.76 test_acc:  90.374\n",
      "epoch:  503 train_loss:  344.71 train_acc:  92.152 test_loss:  437.094 test_acc:  90.461\n",
      "epoch:  504 train_loss:  315.387 train_acc:  92.394 test_loss:  384.326 test_acc:  91.373\n",
      "epoch:  505 train_loss:  308.073 train_acc:  92.56 test_loss:  372.801 test_acc:  91.5\n",
      "max acc epoch：505        max acc：91.5\n",
      "epoch:  506 train_loss:  324.285 train_acc:  92.261 test_loss:  404.584 test_acc:  91.399\n",
      "epoch:  507 train_loss:  312.818 train_acc:  92.405 test_loss:  381.165 test_acc:  91.24\n",
      "epoch:  508 train_loss:  331.357 train_acc:  92.382 test_loss:  418.294 test_acc:  90.926\n",
      "epoch:  509 train_loss:  382.598 train_acc:  91.909 test_loss:  493.814 test_acc:  90.819\n",
      "epoch:  510 train_loss:  311.202 train_acc:  92.416 test_loss:  394.123 test_acc:  91.402\n",
      "epoch:  511 train_loss:  336.562 train_acc:  91.864 test_loss:  416.332 test_acc:  91.161\n",
      "epoch:  512 train_loss:  311.645 train_acc:  92.525 test_loss:  367.354 test_acc:  91.6\n",
      "max acc epoch：512        max acc：91.6\n",
      "epoch:  513 train_loss:  395.867 train_acc:  90.644 test_loss:  459.377 test_acc:  90.254\n",
      "epoch:  514 train_loss:  369.137 train_acc:  92.053 test_loss:  460.431 test_acc:  90.727\n",
      "epoch:  515 train_loss:  365.304 train_acc:  91.146 test_loss:  456.462 test_acc:  90.442\n",
      "epoch:  516 train_loss:  335.181 train_acc:  91.962 test_loss:  436.08 test_acc:  90.314\n",
      "epoch:  517 train_loss:  339.378 train_acc:  91.465 test_loss:  421.168 test_acc:  90.634\n",
      "epoch:  518 train_loss:  321.763 train_acc:  91.936 test_loss:  384.59 test_acc:  91.225\n",
      "epoch:  519 train_loss:  329.049 train_acc:  91.984 test_loss:  406.627 test_acc:  91.291\n",
      "epoch:  520 train_loss:  310.574 train_acc:  92.545 test_loss:  380.178 test_acc:  91.302\n",
      "epoch:  521 train_loss:  334.321 train_acc:  92.42 test_loss:  421.352 test_acc:  90.652\n",
      "epoch:  522 train_loss:  309.332 train_acc:  92.516 test_loss:  385.33 test_acc:  91.424\n",
      "epoch:  523 train_loss:  318.665 train_acc:  92.348 test_loss:  392.444 test_acc:  91.473\n",
      "epoch:  524 train_loss:  347.639 train_acc:  92.159 test_loss:  448.752 test_acc:  90.353\n",
      "epoch:  525 train_loss:  338.76 train_acc:  92.071 test_loss:  446.894 test_acc:  90.958\n",
      "epoch:  526 train_loss:  329.339 train_acc:  92.475 test_loss:  389.587 test_acc:  91.211\n",
      "epoch:  527 train_loss:  342.413 train_acc:  92.188 test_loss:  402.853 test_acc:  91.219\n",
      "epoch:  528 train_loss:  317.736 train_acc:  92.26 test_loss:  379.536 test_acc:  91.414\n",
      "epoch:  529 train_loss:  391.768 train_acc:  90.947 test_loss:  491.412 test_acc:  90.166\n",
      "epoch:  530 train_loss:  305.846 train_acc:  92.431 test_loss:  395.48 test_acc:  91.061\n",
      "epoch:  531 train_loss:  319.426 train_acc:  92.503 test_loss:  394.879 test_acc:  91.267\n",
      "epoch:  532 train_loss:  301.584 train_acc:  92.603 test_loss:  378.712 test_acc:  91.627\n",
      "max acc epoch：532        max acc：91.627\n",
      "epoch:  533 train_loss:  301.625 train_acc:  92.66 test_loss:  363.045 test_acc:  91.352\n",
      "epoch:  534 train_loss:  371.674 train_acc:  91.062 test_loss:  444.752 test_acc:  90.554\n",
      "epoch:  535 train_loss:  326.707 train_acc:  92.257 test_loss:  381.362 test_acc:  91.082\n",
      "epoch:  536 train_loss:  303.35 train_acc:  92.449 test_loss:  380.79 test_acc:  91.356\n",
      "epoch:  537 train_loss:  312.29 train_acc:  92.462 test_loss:  407.236 test_acc:  90.823\n",
      "epoch:  538 train_loss:  311.175 train_acc:  92.17 test_loss:  380.183 test_acc:  91.17\n",
      "epoch:  539 train_loss:  297.826 train_acc:  92.586 test_loss:  386.951 test_acc:  91.159\n",
      "epoch:  540 train_loss:  364.566 train_acc:  92.099 test_loss:  489.262 test_acc:  90.057\n",
      "epoch:  541 train_loss:  329.748 train_acc:  92.448 test_loss:  420.72 test_acc:  90.566\n",
      "epoch:  542 train_loss:  329.129 train_acc:  91.989 test_loss:  427.949 test_acc:  91.082\n",
      "epoch:  543 train_loss:  302.071 train_acc:  92.53 test_loss:  364.571 test_acc:  91.431\n",
      "epoch:  544 train_loss:  437.87 train_acc:  89.651 test_loss:  515.575 test_acc:  89.597\n",
      "epoch:  545 train_loss:  355.065 train_acc:  92.075 test_loss:  443.801 test_acc:  90.49\n",
      "epoch:  546 train_loss:  308.055 train_acc:  92.461 test_loss:  372.1 test_acc:  91.694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max acc epoch：546        max acc：91.694\n",
      "epoch:  547 train_loss:  319.162 train_acc:  92.13 test_loss:  409.629 test_acc:  90.561\n",
      "epoch:  548 train_loss:  366.887 train_acc:  91.368 test_loss:  437.909 test_acc:  90.711\n",
      "epoch:  549 train_loss:  348.176 train_acc:  91.845 test_loss:  469.477 test_acc:  90.368\n",
      "epoch:  550 train_loss:  308.934 train_acc:  92.391 test_loss:  394.948 test_acc:  91.44\n",
      "epoch:  551 train_loss:  304.362 train_acc:  92.646 test_loss:  385.535 test_acc:  91.343\n",
      "epoch:  552 train_loss:  295.181 train_acc:  92.653 test_loss:  384.907 test_acc:  90.91\n",
      "epoch:  553 train_loss:  312.858 train_acc:  92.45 test_loss:  429.783 test_acc:  90.7\n",
      "epoch:  554 train_loss:  303.233 train_acc:  92.569 test_loss:  365.471 test_acc:  91.529\n",
      "epoch:  555 train_loss:  336.357 train_acc:  92.046 test_loss:  415.651 test_acc:  91.273\n",
      "epoch:  556 train_loss:  298.855 train_acc:  92.735 test_loss:  374.753 test_acc:  91.546\n",
      "epoch:  557 train_loss:  330.051 train_acc:  92.154 test_loss:  444.668 test_acc:  90.52\n",
      "epoch:  558 train_loss:  401.853 train_acc:  90.184 test_loss:  491.021 test_acc:  89.921\n",
      "epoch:  559 train_loss:  344.676 train_acc:  92.176 test_loss:  440.028 test_acc:  90.334\n",
      "epoch:  560 train_loss:  348.266 train_acc:  91.263 test_loss:  432.565 test_acc:  90.548\n",
      "epoch:  561 train_loss:  320.346 train_acc:  92.326 test_loss:  390.541 test_acc:  91.47\n",
      "epoch:  562 train_loss:  285.566 train_acc:  92.851 test_loss:  361.496 test_acc:  91.443\n",
      "epoch:  563 train_loss:  320.24 train_acc:  92.101 test_loss:  385.479 test_acc:  91.488\n",
      "epoch:  564 train_loss:  298.102 train_acc:  92.475 test_loss:  388.049 test_acc:  91.347\n",
      "epoch:  565 train_loss:  300.595 train_acc:  92.641 test_loss:  383.248 test_acc:  91.313\n",
      "epoch:  566 train_loss:  285.906 train_acc:  92.932 test_loss:  358.85 test_acc:  91.6\n",
      "epoch:  567 train_loss:  324.482 train_acc:  92.408 test_loss:  419.101 test_acc:  90.567\n",
      "epoch:  568 train_loss:  327.583 train_acc:  92.127 test_loss:  481.487 test_acc:  90.905\n",
      "epoch:  569 train_loss:  286.32 train_acc:  92.833 test_loss:  352.418 test_acc:  91.6\n",
      "epoch:  570 train_loss:  289.162 train_acc:  92.893 test_loss:  371.816 test_acc:  91.392\n",
      "epoch:  571 train_loss:  300.548 train_acc:  92.461 test_loss:  405.522 test_acc:  91.111\n",
      "epoch:  572 train_loss:  297.025 train_acc:  92.829 test_loss:  396.851 test_acc:  91.046\n",
      "epoch:  573 train_loss:  302.329 train_acc:  92.591 test_loss:  388.019 test_acc:  91.563\n",
      "epoch:  574 train_loss:  376.285 train_acc:  91.769 test_loss:  474.21 test_acc:  89.842\n",
      "epoch:  575 train_loss:  345.202 train_acc:  92.344 test_loss:  433.651 test_acc:  90.457\n",
      "epoch:  576 train_loss:  329.297 train_acc:  92.435 test_loss:  405.525 test_acc:  90.777\n",
      "epoch:  577 train_loss:  318.764 train_acc:  92.498 test_loss:  400.015 test_acc:  91.064\n",
      "epoch:  578 train_loss:  287.261 train_acc:  92.883 test_loss:  369.7 test_acc:  91.514\n",
      "epoch:  579 train_loss:  312.019 train_acc:  92.563 test_loss:  398.235 test_acc:  90.9\n",
      "epoch:  580 train_loss:  290.811 train_acc:  92.67 test_loss:  381.786 test_acc:  91.468\n",
      "epoch:  581 train_loss:  290.616 train_acc:  92.756 test_loss:  386.209 test_acc:  91.376\n",
      "epoch:  582 train_loss:  375.701 train_acc:  92.053 test_loss:  476.694 test_acc:  90.041\n",
      "epoch:  583 train_loss:  362.358 train_acc:  92.155 test_loss:  463.672 test_acc:  90.547\n",
      "epoch:  584 train_loss:  297.01 train_acc:  92.419 test_loss:  363.808 test_acc:  91.49\n",
      "epoch:  585 train_loss:  301.544 train_acc:  92.75 test_loss:  386.515 test_acc:  91.315\n",
      "epoch:  586 train_loss:  282.539 train_acc:  92.749 test_loss:  380.121 test_acc:  91.224\n",
      "epoch:  587 train_loss:  302.292 train_acc:  92.56 test_loss:  391.888 test_acc:  91.097\n",
      "epoch:  588 train_loss:  301.332 train_acc:  92.408 test_loss:  421.584 test_acc:  90.699\n",
      "epoch:  589 train_loss:  351.45 train_acc:  92.183 test_loss:  430.025 test_acc:  90.554\n",
      "epoch:  590 train_loss:  304.478 train_acc:  92.402 test_loss:  371.146 test_acc:  91.454\n",
      "epoch:  591 train_loss:  296.743 train_acc:  92.71 test_loss:  367.919 test_acc:  91.416\n",
      "epoch:  592 train_loss:  338.775 train_acc:  91.392 test_loss:  437.651 test_acc:  90.535\n",
      "epoch:  593 train_loss:  300.467 train_acc:  92.74 test_loss:  389.069 test_acc:  91.1\n",
      "epoch:  594 train_loss:  306.227 train_acc:  92.09 test_loss:  401.794 test_acc:  91.046\n",
      "epoch:  595 train_loss:  310.541 train_acc:  92.111 test_loss:  394.568 test_acc:  91.283\n",
      "epoch:  596 train_loss:  286.873 train_acc:  92.761 test_loss:  353.663 test_acc:  91.702\n",
      "max acc epoch：596        max acc：91.702\n",
      "epoch:  597 train_loss:  299.583 train_acc:  92.58 test_loss:  336.888 test_acc:  91.707\n",
      "max acc epoch：597        max acc：91.707\n",
      "epoch:  598 train_loss:  329.175 train_acc:  91.882 test_loss:  414.159 test_acc:  90.89\n",
      "epoch:  599 train_loss:  301.756 train_acc:  92.289 test_loss:  357.917 test_acc:  91.636\n",
      "epoch:  600 train_loss:  320.396 train_acc:  92.664 test_loss:  398.799 test_acc:  90.841\n",
      "epoch:  601 train_loss:  426.289 train_acc:  91.218 test_loss:  554.016 test_acc:  89.262\n",
      "epoch:  602 train_loss:  297.005 train_acc:  92.798 test_loss:  382.029 test_acc:  91.263\n",
      "epoch:  603 train_loss:  285.477 train_acc:  92.877 test_loss:  378.221 test_acc:  91.312\n",
      "epoch:  604 train_loss:  282.623 train_acc:  92.66 test_loss:  367.261 test_acc:  91.587\n",
      "epoch:  605 train_loss:  303.197 train_acc:  92.5 test_loss:  418.509 test_acc:  90.919\n",
      "epoch:  606 train_loss:  313.91 train_acc:  92.217 test_loss:  411.998 test_acc:  91.294\n",
      "epoch:  607 train_loss:  574.387 train_acc:  87.776 test_loss:  656.82 test_acc:  87.965\n",
      "epoch:  608 train_loss:  287.626 train_acc:  92.688 test_loss:  377.463 test_acc:  91.308\n",
      "epoch:  609 train_loss:  296.237 train_acc:  92.47 test_loss:  364.483 test_acc:  91.628\n",
      "epoch:  610 train_loss:  311.105 train_acc:  92.143 test_loss:  374.929 test_acc:  91.365\n",
      "epoch:  611 train_loss:  284.625 train_acc:  92.695 test_loss:  346.567 test_acc:  91.831\n",
      "max acc epoch：611        max acc：91.831\n",
      "epoch:  612 train_loss:  275.644 train_acc:  92.96 test_loss:  364.692 test_acc:  91.616\n",
      "epoch:  613 train_loss:  321.486 train_acc:  91.821 test_loss:  404.362 test_acc:  91.083\n",
      "epoch:  614 train_loss:  284.291 train_acc:  92.908 test_loss:  360.189 test_acc:  91.333\n",
      "epoch:  615 train_loss:  286.453 train_acc:  92.754 test_loss:  379.956 test_acc:  91.173\n",
      "epoch:  616 train_loss:  273.77 train_acc:  92.931 test_loss:  359.925 test_acc:  91.793\n",
      "epoch:  617 train_loss:  277.679 train_acc:  92.82 test_loss:  368.613 test_acc:  91.569\n",
      "epoch:  618 train_loss:  326.768 train_acc:  92.665 test_loss:  405.243 test_acc:  91.346\n",
      "epoch:  619 train_loss:  279.261 train_acc:  92.967 test_loss:  366.354 test_acc:  91.789\n",
      "epoch:  620 train_loss:  283.304 train_acc:  92.843 test_loss:  397.1 test_acc:  91.353\n",
      "epoch:  621 train_loss:  308.362 train_acc:  92.331 test_loss:  426.046 test_acc:  90.961\n",
      "epoch:  622 train_loss:  280.143 train_acc:  92.921 test_loss:  397.252 test_acc:  91.335\n",
      "epoch:  623 train_loss:  301.916 train_acc:  92.274 test_loss:  360.558 test_acc:  91.437\n",
      "epoch:  624 train_loss:  315.481 train_acc:  91.871 test_loss:  377.51 test_acc:  91.096\n",
      "epoch:  625 train_loss:  291.651 train_acc:  92.438 test_loss:  376.169 test_acc:  91.158\n",
      "epoch:  626 train_loss:  325.896 train_acc:  91.858 test_loss:  398.513 test_acc:  91.137\n",
      "epoch:  627 train_loss:  272.747 train_acc:  92.97 test_loss:  352.748 test_acc:  91.638\n",
      "epoch:  628 train_loss:  404.158 train_acc:  90.957 test_loss:  436.777 test_acc:  90.7\n",
      "epoch:  629 train_loss:  273.744 train_acc:  92.869 test_loss:  345.233 test_acc:  91.739\n",
      "epoch:  630 train_loss:  290.965 train_acc:  92.791 test_loss:  365.867 test_acc:  91.577\n",
      "epoch:  631 train_loss:  309.414 train_acc:  92.542 test_loss:  396.422 test_acc:  90.868\n",
      "epoch:  632 train_loss:  278.613 train_acc:  92.956 test_loss:  371.131 test_acc:  91.661\n",
      "epoch:  633 train_loss:  297.137 train_acc:  92.885 test_loss:  381.956 test_acc:  91.175\n",
      "epoch:  634 train_loss:  272.094 train_acc:  92.969 test_loss:  368.743 test_acc:  91.534\n",
      "epoch:  635 train_loss:  276.378 train_acc:  92.81 test_loss:  364.176 test_acc:  91.485\n",
      "epoch:  636 train_loss:  291.263 train_acc:  92.578 test_loss:  376.735 test_acc:  91.559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  637 train_loss:  269.775 train_acc:  93.042 test_loss:  364.85 test_acc:  91.618\n",
      "epoch:  638 train_loss:  317.346 train_acc:  92.581 test_loss:  405.289 test_acc:  91.164\n",
      "epoch:  639 train_loss:  292.128 train_acc:  92.727 test_loss:  382.412 test_acc:  91.436\n",
      "epoch:  640 train_loss:  266.846 train_acc:  93.035 test_loss:  349.907 test_acc:  91.72\n",
      "epoch:  641 train_loss:  270.414 train_acc:  93.008 test_loss:  365.342 test_acc:  91.488\n",
      "epoch:  642 train_loss:  284.171 train_acc:  92.767 test_loss:  379.611 test_acc:  91.598\n",
      "epoch:  643 train_loss:  272.864 train_acc:  93.057 test_loss:  352.845 test_acc:  91.451\n",
      "epoch:  644 train_loss:  310.773 train_acc:  92.852 test_loss:  437.348 test_acc:  91.056\n",
      "epoch:  645 train_loss:  295.349 train_acc:  92.379 test_loss:  377.478 test_acc:  91.402\n",
      "epoch:  646 train_loss:  280.663 train_acc:  92.975 test_loss:  377.573 test_acc:  91.641\n",
      "epoch:  647 train_loss:  273.95 train_acc:  93.104 test_loss:  353.743 test_acc:  91.674\n",
      "epoch:  648 train_loss:  271.555 train_acc:  92.939 test_loss:  340.571 test_acc:  91.933\n",
      "max acc epoch：648        max acc：91.933\n",
      "epoch:  649 train_loss:  307.408 train_acc:  92.659 test_loss:  394.349 test_acc:  91.487\n",
      "epoch:  650 train_loss:  279.453 train_acc:  92.844 test_loss:  384.793 test_acc:  91.36\n",
      "epoch:  651 train_loss:  290.715 train_acc:  92.534 test_loss:  359.425 test_acc:  91.685\n",
      "epoch:  652 train_loss:  288.934 train_acc:  92.401 test_loss:  396.226 test_acc:  91.06\n",
      "epoch:  653 train_loss:  275.792 train_acc:  92.946 test_loss:  340.514 test_acc:  91.747\n",
      "epoch:  654 train_loss:  323.196 train_acc:  92.098 test_loss:  382.189 test_acc:  91.375\n",
      "epoch:  655 train_loss:  265.928 train_acc:  93.139 test_loss:  355.303 test_acc:  91.932\n",
      "epoch:  656 train_loss:  300.963 train_acc:  92.813 test_loss:  377.216 test_acc:  91.336\n",
      "epoch:  657 train_loss:  272.288 train_acc:  92.962 test_loss:  362.271 test_acc:  91.563\n",
      "epoch:  658 train_loss:  282.693 train_acc:  92.788 test_loss:  400.785 test_acc:  91.653\n",
      "epoch:  659 train_loss:  294.974 train_acc:  92.968 test_loss:  391.284 test_acc:  91.639\n",
      "epoch:  660 train_loss:  277.252 train_acc:  92.748 test_loss:  363.081 test_acc:  91.702\n",
      "epoch:  661 train_loss:  375.185 train_acc:  92.2 test_loss:  508.177 test_acc:  90.321\n",
      "epoch:  662 train_loss:  282.649 train_acc:  92.629 test_loss:  355.973 test_acc:  91.747\n",
      "epoch:  663 train_loss:  284.545 train_acc:  92.932 test_loss:  388.952 test_acc:  91.402\n",
      "epoch:  664 train_loss:  272.712 train_acc:  93.069 test_loss:  346.67 test_acc:  91.887\n",
      "epoch:  665 train_loss:  265.403 train_acc:  93.043 test_loss:  369.231 test_acc:  91.614\n",
      "epoch:  666 train_loss:  273.541 train_acc:  93.014 test_loss:  372.534 test_acc:  91.546\n",
      "epoch:  667 train_loss:  270.235 train_acc:  93.185 test_loss:  345.619 test_acc:  91.864\n",
      "epoch:  668 train_loss:  288.799 train_acc:  93.01 test_loss:  352.529 test_acc:  91.795\n",
      "epoch:  669 train_loss:  258.14 train_acc:  93.167 test_loss:  332.741 test_acc:  91.942\n",
      "max acc epoch：669        max acc：91.942\n",
      "epoch:  670 train_loss:  316.473 train_acc:  91.808 test_loss:  423.7 test_acc:  90.764\n",
      "epoch:  671 train_loss:  279.845 train_acc:  92.777 test_loss:  365.791 test_acc:  91.727\n",
      "epoch:  672 train_loss:  305.17 train_acc:  92.64 test_loss:  411.857 test_acc:  90.873\n",
      "epoch:  673 train_loss:  309.275 train_acc:  92.525 test_loss:  364.847 test_acc:  91.77\n",
      "epoch:  674 train_loss:  282.031 train_acc:  92.546 test_loss:  349.699 test_acc:  91.558\n",
      "epoch:  675 train_loss:  265.621 train_acc:  93.027 test_loss:  345.632 test_acc:  91.602\n",
      "epoch:  676 train_loss:  271.281 train_acc:  92.905 test_loss:  347.856 test_acc:  91.899\n",
      "epoch:  677 train_loss:  265.847 train_acc:  92.923 test_loss:  346.144 test_acc:  91.775\n",
      "epoch:  678 train_loss:  299.163 train_acc:  92.431 test_loss:  395.816 test_acc:  91.567\n",
      "epoch:  679 train_loss:  270.707 train_acc:  92.969 test_loss:  375.449 test_acc:  91.282\n",
      "epoch:  680 train_loss:  309.788 train_acc:  92.088 test_loss:  388.178 test_acc:  91.26\n",
      "epoch:  681 train_loss:  265.573 train_acc:  93.132 test_loss:  328.442 test_acc:  92.164\n",
      "max acc epoch：681        max acc：92.164\n",
      "epoch:  682 train_loss:  268.296 train_acc:  92.79 test_loss:  359.998 test_acc:  91.627\n",
      "epoch:  683 train_loss:  270.38 train_acc:  92.912 test_loss:  326.697 test_acc:  91.966\n",
      "epoch:  684 train_loss:  343.642 train_acc:  92.272 test_loss:  466.906 test_acc:  90.478\n",
      "epoch:  685 train_loss:  276.072 train_acc:  92.974 test_loss:  364.375 test_acc:  91.408\n",
      "epoch:  686 train_loss:  260.052 train_acc:  93.269 test_loss:  354.525 test_acc:  91.835\n",
      "epoch:  687 train_loss:  345.463 train_acc:  92.478 test_loss:  438.972 test_acc:  91.28\n",
      "epoch:  688 train_loss:  259.797 train_acc:  93.062 test_loss:  335.783 test_acc:  91.787\n",
      "epoch:  689 train_loss:  267.249 train_acc:  93.139 test_loss:  352.406 test_acc:  91.998\n",
      "epoch:  690 train_loss:  283.119 train_acc:  92.572 test_loss:  354.158 test_acc:  91.8\n",
      "epoch:  691 train_loss:  266.019 train_acc:  92.928 test_loss:  336.995 test_acc:  91.835\n",
      "epoch:  692 train_loss:  279.831 train_acc:  93.059 test_loss:  364.337 test_acc:  91.466\n",
      "epoch:  693 train_loss:  283.212 train_acc:  92.924 test_loss:  377.62 test_acc:  91.875\n",
      "epoch:  694 train_loss:  257.67 train_acc:  93.154 test_loss:  339.895 test_acc:  91.877\n",
      "epoch:  695 train_loss:  260.016 train_acc:  93.076 test_loss:  343.554 test_acc:  91.968\n",
      "epoch:  696 train_loss:  347.902 train_acc:  92.448 test_loss:  408.231 test_acc:  91.155\n",
      "epoch:  697 train_loss:  305.428 train_acc:  92.671 test_loss:  398.269 test_acc:  91.037\n",
      "epoch:  698 train_loss:  271.166 train_acc:  93.048 test_loss:  350.296 test_acc:  91.998\n",
      "epoch:  699 train_loss:  260.222 train_acc:  93.191 test_loss:  365.049 test_acc:  91.728\n",
      "epoch:  700 train_loss:  258.992 train_acc:  93.057 test_loss:  342.995 test_acc:  91.842\n",
      "epoch:  701 train_loss:  316.895 train_acc:  91.903 test_loss:  445.984 test_acc:  90.705\n",
      "epoch:  702 train_loss:  321.048 train_acc:  92.608 test_loss:  390.646 test_acc:  91.114\n",
      "epoch:  703 train_loss:  266.972 train_acc:  93.17 test_loss:  369.887 test_acc:  91.567\n",
      "epoch:  704 train_loss:  379.833 train_acc:  92.016 test_loss:  523.806 test_acc:  90.104\n",
      "epoch:  705 train_loss:  264.416 train_acc:  93.184 test_loss:  335.038 test_acc:  91.691\n",
      "epoch:  706 train_loss:  277.157 train_acc:  92.765 test_loss:  353.811 test_acc:  91.96\n",
      "epoch:  707 train_loss:  275.971 train_acc:  92.843 test_loss:  355.365 test_acc:  91.642\n",
      "epoch:  708 train_loss:  272.98 train_acc:  93.018 test_loss:  391.431 test_acc:  91.317\n",
      "epoch:  709 train_loss:  259.596 train_acc:  93.208 test_loss:  345.078 test_acc:  91.792\n",
      "epoch:  710 train_loss:  256.535 train_acc:  93.277 test_loss:  332.578 test_acc:  92.02\n",
      "epoch:  711 train_loss:  313.324 train_acc:  92.822 test_loss:  390.21 test_acc:  91.421\n",
      "epoch:  712 train_loss:  262.319 train_acc:  93.219 test_loss:  353.573 test_acc:  91.88\n",
      "epoch:  713 train_loss:  293.706 train_acc:  92.372 test_loss:  409.748 test_acc:  91.33\n",
      "epoch:  714 train_loss:  262.067 train_acc:  93.001 test_loss:  340.615 test_acc:  91.912\n",
      "epoch:  715 train_loss:  263.052 train_acc:  93.256 test_loss:  350.631 test_acc:  91.845\n",
      "epoch:  716 train_loss:  255.235 train_acc:  93.352 test_loss:  336.374 test_acc:  91.864\n",
      "epoch:  717 train_loss:  273.317 train_acc:  92.825 test_loss:  365.964 test_acc:  91.552\n",
      "epoch:  718 train_loss:  268.599 train_acc:  93.135 test_loss:  348.45 test_acc:  91.917\n",
      "epoch:  719 train_loss:  288.265 train_acc:  93.053 test_loss:  381.689 test_acc:  91.462\n",
      "epoch:  720 train_loss:  265.644 train_acc:  92.955 test_loss:  360.503 test_acc:  91.804\n",
      "epoch:  721 train_loss:  289.205 train_acc:  92.573 test_loss:  389.295 test_acc:  91.45\n",
      "epoch:  722 train_loss:  258.468 train_acc:  93.165 test_loss:  340.004 test_acc:  92.088\n",
      "epoch:  723 train_loss:  316.976 train_acc:  92.113 test_loss:  385.765 test_acc:  91.348\n",
      "epoch:  724 train_loss:  257.925 train_acc:  93.223 test_loss:  334.709 test_acc:  92.079\n",
      "epoch:  725 train_loss:  274.597 train_acc:  93.22 test_loss:  345.471 test_acc:  91.872\n",
      "epoch:  726 train_loss:  291.406 train_acc:  92.86 test_loss:  367.605 test_acc:  91.484\n",
      "epoch:  727 train_loss:  282.165 train_acc:  92.492 test_loss:  383.154 test_acc:  91.314\n",
      "epoch:  728 train_loss:  250.487 train_acc:  93.344 test_loss:  332.1 test_acc:  92.141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  729 train_loss:  272.138 train_acc:  92.914 test_loss:  395.741 test_acc:  91.581\n",
      "epoch:  730 train_loss:  259.329 train_acc:  93.358 test_loss:  344.925 test_acc:  92.017\n",
      "epoch:  731 train_loss:  270.396 train_acc:  92.893 test_loss:  375.683 test_acc:  91.592\n",
      "epoch:  732 train_loss:  278.612 train_acc:  93.046 test_loss:  347.072 test_acc:  92.273\n",
      "max acc epoch：732        max acc：92.273\n",
      "epoch:  733 train_loss:  273.672 train_acc:  92.886 test_loss:  339.889 test_acc:  91.911\n",
      "epoch:  734 train_loss:  257.739 train_acc:  93.171 test_loss:  332.276 test_acc:  92.067\n",
      "epoch:  735 train_loss:  260.51 train_acc:  92.844 test_loss:  331.789 test_acc:  91.862\n",
      "epoch:  736 train_loss:  281.144 train_acc:  93.066 test_loss:  363.959 test_acc:  91.911\n",
      "epoch:  737 train_loss:  257.112 train_acc:  93.097 test_loss:  349.514 test_acc:  91.682\n",
      "epoch:  738 train_loss:  259.029 train_acc:  93.208 test_loss:  367.763 test_acc:  91.736\n",
      "epoch:  739 train_loss:  365.698 train_acc:  92.317 test_loss:  457.039 test_acc:  90.609\n",
      "epoch:  740 train_loss:  257.209 train_acc:  93.332 test_loss:  331.561 test_acc:  92.401\n",
      "max acc epoch：740        max acc：92.401\n",
      "epoch:  741 train_loss:  273.812 train_acc:  93.151 test_loss:  389.184 test_acc:  91.875\n",
      "epoch:  742 train_loss:  259.559 train_acc:  92.947 test_loss:  361.965 test_acc:  91.724\n",
      "epoch:  743 train_loss:  287.427 train_acc:  92.712 test_loss:  373.231 test_acc:  91.883\n",
      "epoch:  744 train_loss:  259.817 train_acc:  93.286 test_loss:  350.257 test_acc:  92.058\n",
      "epoch:  745 train_loss:  298.712 train_acc:  92.211 test_loss:  390.093 test_acc:  91.332\n",
      "epoch:  746 train_loss:  259.271 train_acc:  92.973 test_loss:  352.993 test_acc:  91.591\n",
      "epoch:  747 train_loss:  277.136 train_acc:  93.002 test_loss:  368.034 test_acc:  91.826\n",
      "epoch:  748 train_loss:  265.539 train_acc:  93.139 test_loss:  337.348 test_acc:  92.049\n",
      "epoch:  749 train_loss:  244.934 train_acc:  93.38 test_loss:  331.858 test_acc:  92.064\n",
      "epoch:  750 train_loss:  285.81 train_acc:  93.031 test_loss:  454.261 test_acc:  91.172\n",
      "epoch:  751 train_loss:  247.887 train_acc:  93.337 test_loss:  329.794 test_acc:  92.106\n",
      "epoch:  752 train_loss:  384.517 train_acc:  92.073 test_loss:  476.758 test_acc:  90.881\n",
      "epoch:  753 train_loss:  257.267 train_acc:  92.967 test_loss:  330.981 test_acc:  91.907\n",
      "epoch:  754 train_loss:  246.642 train_acc:  93.466 test_loss:  337.517 test_acc:  92.125\n",
      "epoch:  755 train_loss:  247.565 train_acc:  93.306 test_loss:  312.737 test_acc:  92.324\n",
      "epoch:  756 train_loss:  263.176 train_acc:  93.17 test_loss:  367.452 test_acc:  92.102\n",
      "epoch:  757 train_loss:  259.495 train_acc:  93.121 test_loss:  348.069 test_acc:  92.132\n",
      "epoch:  758 train_loss:  253.777 train_acc:  93.388 test_loss:  338.949 test_acc:  92.014\n",
      "epoch:  759 train_loss:  288.043 train_acc:  92.583 test_loss:  388.575 test_acc:  91.723\n",
      "epoch:  760 train_loss:  329.524 train_acc:  92.788 test_loss:  439.29 test_acc:  91.685\n",
      "epoch:  761 train_loss:  305.887 train_acc:  92.768 test_loss:  386.671 test_acc:  91.276\n",
      "epoch:  762 train_loss:  276.773 train_acc:  93.17 test_loss:  383.725 test_acc:  91.638\n",
      "epoch:  763 train_loss:  259.241 train_acc:  93.264 test_loss:  330.043 test_acc:  92.205\n",
      "epoch:  764 train_loss:  254.62 train_acc:  93.425 test_loss:  353.9 test_acc:  92.265\n",
      "epoch:  765 train_loss:  256.321 train_acc:  93.349 test_loss:  341.009 test_acc:  91.971\n",
      "epoch:  766 train_loss:  274.321 train_acc:  93.346 test_loss:  350.82 test_acc:  92.2\n",
      "epoch:  767 train_loss:  256.517 train_acc:  93.093 test_loss:  371.576 test_acc:  91.492\n",
      "epoch:  768 train_loss:  315.941 train_acc:  91.967 test_loss:  372.694 test_acc:  91.4\n",
      "epoch:  769 train_loss:  297.049 train_acc:  93.138 test_loss:  384.731 test_acc:  91.484\n",
      "epoch:  770 train_loss:  276.538 train_acc:  93.264 test_loss:  340.356 test_acc:  92.147\n",
      "epoch:  771 train_loss:  282.575 train_acc:  93.109 test_loss:  358.548 test_acc:  91.599\n",
      "epoch:  772 train_loss:  267.719 train_acc:  93.05 test_loss:  378.283 test_acc:  91.53\n",
      "epoch:  773 train_loss:  283.405 train_acc:  92.426 test_loss:  402.872 test_acc:  91.183\n",
      "epoch:  774 train_loss:  261.904 train_acc:  93.279 test_loss:  323.804 test_acc:  92.136\n",
      "epoch:  775 train_loss:  258.734 train_acc:  93.31 test_loss:  329.615 test_acc:  92.265\n",
      "epoch:  776 train_loss:  241.384 train_acc:  93.54 test_loss:  327.364 test_acc:  92.381\n",
      "epoch:  777 train_loss:  270.735 train_acc:  93.144 test_loss:  360.951 test_acc:  91.799\n",
      "epoch:  778 train_loss:  310.305 train_acc:  91.885 test_loss:  403.984 test_acc:  91.148\n",
      "epoch:  779 train_loss:  254.517 train_acc:  93.062 test_loss:  324.172 test_acc:  92.445\n",
      "max acc epoch：779        max acc：92.445\n",
      "epoch:  780 train_loss:  271.289 train_acc:  93.214 test_loss:  374.452 test_acc:  91.598\n",
      "epoch:  781 train_loss:  269.931 train_acc:  93.252 test_loss:  359.029 test_acc:  91.588\n",
      "epoch:  782 train_loss:  251.396 train_acc:  93.409 test_loss:  332.413 test_acc:  92.237\n",
      "epoch:  783 train_loss:  254.902 train_acc:  93.242 test_loss:  326.557 test_acc:  92.156\n",
      "epoch:  784 train_loss:  255.73 train_acc:  93.067 test_loss:  349.085 test_acc:  91.854\n",
      "epoch:  785 train_loss:  250.781 train_acc:  93.102 test_loss:  339.883 test_acc:  91.817\n",
      "epoch:  786 train_loss:  257.942 train_acc:  93.168 test_loss:  350.307 test_acc:  92.076\n",
      "epoch:  787 train_loss:  272.212 train_acc:  92.757 test_loss:  335.168 test_acc:  91.762\n",
      "epoch:  788 train_loss:  325.386 train_acc:  92.932 test_loss:  401.429 test_acc:  91.452\n",
      "epoch:  789 train_loss:  286.731 train_acc:  92.378 test_loss:  380.288 test_acc:  91.074\n",
      "epoch:  790 train_loss:  304.326 train_acc:  92.511 test_loss:  380.064 test_acc:  91.285\n",
      "epoch:  791 train_loss:  264.537 train_acc:  92.941 test_loss:  331.25 test_acc:  91.863\n",
      "epoch:  792 train_loss:  240.629 train_acc:  93.546 test_loss:  332.519 test_acc:  92.212\n",
      "epoch:  793 train_loss:  256.532 train_acc:  92.973 test_loss:  357.878 test_acc:  91.618\n",
      "epoch:  794 train_loss:  274.473 train_acc:  93.141 test_loss:  336.67 test_acc:  91.704\n",
      "epoch:  795 train_loss:  320.76 train_acc:  91.897 test_loss:  413.144 test_acc:  90.763\n",
      "epoch:  796 train_loss:  256.044 train_acc:  93.247 test_loss:  321.864 test_acc:  92.023\n",
      "epoch:  797 train_loss:  266.257 train_acc:  93.106 test_loss:  322.504 test_acc:  91.948\n",
      "epoch:  798 train_loss:  251.05 train_acc:  93.444 test_loss:  313.059 test_acc:  92.358\n",
      "epoch:  799 train_loss:  286.636 train_acc:  92.596 test_loss:  365.771 test_acc:  91.736\n",
      "epoch:  800 train_loss:  266.356 train_acc:  93.423 test_loss:  350.502 test_acc:  91.814\n",
      "epoch:  801 train_loss:  260.528 train_acc:  93.09 test_loss:  332.46 test_acc:  92.015\n",
      "epoch:  802 train_loss:  398.056 train_acc:  90.782 test_loss:  484.808 test_acc:  89.937\n",
      "epoch:  803 train_loss:  261.008 train_acc:  93.247 test_loss:  331.996 test_acc:  92.396\n",
      "epoch:  804 train_loss:  249.733 train_acc:  93.502 test_loss:  327.78 test_acc:  92.153\n",
      "epoch:  805 train_loss:  238.76 train_acc:  93.601 test_loss:  328.241 test_acc:  92.355\n",
      "epoch:  806 train_loss:  240.052 train_acc:  93.44 test_loss:  323.227 test_acc:  92.442\n",
      "epoch:  807 train_loss:  248.866 train_acc:  93.292 test_loss:  320.14 test_acc:  92.157\n",
      "epoch:  808 train_loss:  248.814 train_acc:  93.528 test_loss:  312.948 test_acc:  92.41\n",
      "epoch:  809 train_loss:  272.927 train_acc:  93.283 test_loss:  377.848 test_acc:  91.657\n",
      "epoch:  810 train_loss:  260.955 train_acc:  93.236 test_loss:  339.484 test_acc:  92.106\n",
      "epoch:  811 train_loss:  288.578 train_acc:  92.019 test_loss:  400.203 test_acc:  91.108\n",
      "epoch:  812 train_loss:  257.05 train_acc:  93.345 test_loss:  340.689 test_acc:  91.954\n",
      "epoch:  813 train_loss:  245.339 train_acc:  93.526 test_loss:  339.627 test_acc:  92.352\n",
      "epoch:  814 train_loss:  243.186 train_acc:  93.449 test_loss:  322.295 test_acc:  92.451\n",
      "max acc epoch：814        max acc：92.451\n",
      "epoch:  815 train_loss:  243.601 train_acc:  93.423 test_loss:  333.995 test_acc:  92.385\n",
      "epoch:  816 train_loss:  258.871 train_acc:  93.262 test_loss:  341.383 test_acc:  91.858\n",
      "epoch:  817 train_loss:  353.438 train_acc:  92.388 test_loss:  423.778 test_acc:  91.087\n",
      "epoch:  818 train_loss:  297.939 train_acc:  92.571 test_loss:  417.745 test_acc:  90.863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  819 train_loss:  240.983 train_acc:  93.496 test_loss:  323.23 test_acc:  92.361\n",
      "epoch:  820 train_loss:  235.417 train_acc:  93.493 test_loss:  325.88 test_acc:  92.194\n",
      "epoch:  821 train_loss:  359.389 train_acc:  91.397 test_loss:  441.337 test_acc:  90.838\n",
      "epoch:  822 train_loss:  248.698 train_acc:  93.638 test_loss:  338.68 test_acc:  92.423\n",
      "epoch:  823 train_loss:  247.744 train_acc:  93.427 test_loss:  334.294 test_acc:  92.097\n",
      "epoch:  824 train_loss:  312.031 train_acc:  92.078 test_loss:  405.85 test_acc:  91.231\n",
      "epoch:  825 train_loss:  261.131 train_acc:  93.384 test_loss:  343.566 test_acc:  92.008\n",
      "epoch:  826 train_loss:  236.438 train_acc:  93.533 test_loss:  322.777 test_acc:  92.3\n",
      "epoch:  827 train_loss:  244.09 train_acc:  93.367 test_loss:  334.265 test_acc:  92.053\n",
      "epoch:  828 train_loss:  239.469 train_acc:  93.543 test_loss:  324.262 test_acc:  92.294\n",
      "epoch:  829 train_loss:  254.69 train_acc:  93.544 test_loss:  338.632 test_acc:  92.065\n",
      "epoch:  830 train_loss:  341.928 train_acc:  92.425 test_loss:  473.217 test_acc:  90.603\n",
      "epoch:  831 train_loss:  250.525 train_acc:  93.404 test_loss:  322.567 test_acc:  92.496\n",
      "max acc epoch：831        max acc：92.496\n",
      "epoch:  832 train_loss:  236.574 train_acc:  93.604 test_loss:  327.5 test_acc:  92.192\n",
      "epoch:  833 train_loss:  355.404 train_acc:  92.69 test_loss:  467.25 test_acc:  91.22\n",
      "epoch:  834 train_loss:  301.9 train_acc:  92.035 test_loss:  393.214 test_acc:  91.201\n",
      "epoch:  835 train_loss:  275.776 train_acc:  92.735 test_loss:  347.604 test_acc:  91.868\n",
      "epoch:  836 train_loss:  245.803 train_acc:  93.49 test_loss:  310.466 test_acc:  92.654\n",
      "max acc epoch：836        max acc：92.654\n",
      "epoch:  837 train_loss:  238.267 train_acc:  93.569 test_loss:  323.027 test_acc:  92.282\n",
      "epoch:  838 train_loss:  246.494 train_acc:  93.678 test_loss:  339.497 test_acc:  92.419\n",
      "epoch:  839 train_loss:  299.037 train_acc:  92.446 test_loss:  399.227 test_acc:  91.345\n",
      "epoch:  840 train_loss:  306.534 train_acc:  92.826 test_loss:  413.664 test_acc:  91.725\n",
      "epoch:  841 train_loss:  257.594 train_acc:  92.857 test_loss:  358.659 test_acc:  91.717\n",
      "epoch:  842 train_loss:  264.451 train_acc:  93.419 test_loss:  337.67 test_acc:  92.027\n",
      "epoch:  843 train_loss:  239.063 train_acc:  93.708 test_loss:  342.957 test_acc:  92.375\n",
      "epoch:  844 train_loss:  253.891 train_acc:  93.436 test_loss:  325.74 test_acc:  91.972\n",
      "epoch:  845 train_loss:  258.608 train_acc:  92.863 test_loss:  351.378 test_acc:  92.13\n",
      "epoch:  846 train_loss:  289.821 train_acc:  92.286 test_loss:  363.728 test_acc:  91.551\n",
      "epoch:  847 train_loss:  306.147 train_acc:  92.802 test_loss:  476.024 test_acc:  91.193\n",
      "epoch:  848 train_loss:  243.866 train_acc:  93.617 test_loss:  313.589 test_acc:  92.247\n",
      "epoch:  849 train_loss:  252.361 train_acc:  93.522 test_loss:  300.489 test_acc:  92.602\n",
      "epoch:  850 train_loss:  264.43 train_acc:  93.421 test_loss:  351.827 test_acc:  92.032\n",
      "epoch:  851 train_loss:  242.395 train_acc:  93.571 test_loss:  305.965 test_acc:  92.586\n",
      "epoch:  852 train_loss:  289.635 train_acc:  92.29 test_loss:  373.174 test_acc:  91.541\n",
      "epoch:  853 train_loss:  235.519 train_acc:  93.662 test_loss:  312.908 test_acc:  92.296\n",
      "epoch:  854 train_loss:  254.754 train_acc:  93.602 test_loss:  346.979 test_acc:  92.231\n",
      "epoch:  855 train_loss:  255.773 train_acc:  93.035 test_loss:  368.082 test_acc:  91.581\n",
      "epoch:  856 train_loss:  249.608 train_acc:  93.196 test_loss:  351.088 test_acc:  91.886\n",
      "epoch:  857 train_loss:  288.224 train_acc:  93.105 test_loss:  357.24 test_acc:  91.569\n",
      "epoch:  858 train_loss:  274.907 train_acc:  93.404 test_loss:  353.929 test_acc:  91.886\n",
      "epoch:  859 train_loss:  280.344 train_acc:  92.811 test_loss:  327.344 test_acc:  92.241\n",
      "epoch:  860 train_loss:  334.403 train_acc:  91.359 test_loss:  400.426 test_acc:  90.965\n",
      "epoch:  861 train_loss:  240.746 train_acc:  93.2 test_loss:  315.73 test_acc:  92.036\n",
      "epoch:  862 train_loss:  238.456 train_acc:  93.575 test_loss:  316.259 test_acc:  92.354\n",
      "epoch:  863 train_loss:  243.189 train_acc:  93.521 test_loss:  325.61 test_acc:  92.384\n",
      "epoch:  864 train_loss:  251.734 train_acc:  93.023 test_loss:  338.541 test_acc:  92.008\n",
      "epoch:  865 train_loss:  266.831 train_acc:  93.157 test_loss:  370.476 test_acc:  91.876\n",
      "epoch:  866 train_loss:  242.336 train_acc:  93.48 test_loss:  319.611 test_acc:  92.168\n",
      "epoch:  867 train_loss:  237.355 train_acc:  93.842 test_loss:  316.37 test_acc:  92.713\n",
      "max acc epoch：867        max acc：92.713\n",
      "epoch:  868 train_loss:  275.444 train_acc:  92.549 test_loss:  350.291 test_acc:  91.475\n",
      "epoch:  869 train_loss:  252.414 train_acc:  93.556 test_loss:  323.536 test_acc:  92.235\n",
      "epoch:  870 train_loss:  235.631 train_acc:  93.764 test_loss:  305.347 test_acc:  92.575\n",
      "epoch:  871 train_loss:  247.409 train_acc:  93.498 test_loss:  336.694 test_acc:  92.024\n",
      "epoch:  872 train_loss:  229.101 train_acc:  93.728 test_loss:  298.082 test_acc:  92.762\n",
      "min loss:872\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(224.5639)]\n",
      "1    [tensor(88.8594)]   [tensor(79.9956)]\n",
      "2   [tensor(162.9521)]  [tensor(161.8760)]\n",
      "3   [tensor(185.9385)]  [tensor(209.2689)]\n",
      "4   [tensor(212.8320)]  [tensor(207.8745)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(262.6079)]\n",
      "61  [tensor(127.2646)]  [tensor(124.2330)]\n",
      "62   [tensor(84.7217)]   [tensor(83.3549)]\n",
      "63  [tensor(104.8945)]  [tensor(134.6844)]\n",
      "64  [tensor(156.6016)]  [tensor(185.5936)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "max acc epoch：872        max acc：92.762\n",
      "epoch:  873 train_loss:  249.475 train_acc:  93.072 test_loss:  349.006 test_acc:  92.065\n",
      "epoch:  874 train_loss:  326.6 train_acc:  92.126 test_loss:  414.713 test_acc:  91.26\n",
      "epoch:  875 train_loss:  336.832 train_acc:  92.578 test_loss:  409.651 test_acc:  91.163\n",
      "epoch:  876 train_loss:  242.537 train_acc:  93.678 test_loss:  327.606 test_acc:  92.488\n",
      "epoch:  877 train_loss:  238.048 train_acc:  93.565 test_loss:  307.415 test_acc:  92.505\n",
      "epoch:  878 train_loss:  279.334 train_acc:  92.271 test_loss:  387.491 test_acc:  91.24\n",
      "epoch:  879 train_loss:  314.474 train_acc:  93.035 test_loss:  371.9 test_acc:  91.693\n",
      "epoch:  880 train_loss:  233.466 train_acc:  93.735 test_loss:  318.584 test_acc:  92.452\n",
      "epoch:  881 train_loss:  247.21 train_acc:  93.51 test_loss:  331.473 test_acc:  92.234\n",
      "epoch:  882 train_loss:  249.604 train_acc:  93.413 test_loss:  342.767 test_acc:  92.365\n",
      "epoch:  883 train_loss:  252.915 train_acc:  93.31 test_loss:  330.809 test_acc:  92.096\n",
      "epoch:  884 train_loss:  229.842 train_acc:  93.737 test_loss:  320.753 test_acc:  92.43\n",
      "epoch:  885 train_loss:  232.099 train_acc:  93.73 test_loss:  308.472 test_acc:  92.367\n",
      "epoch:  886 train_loss:  234.049 train_acc:  93.748 test_loss:  304.656 test_acc:  92.386\n",
      "epoch:  887 train_loss:  235.569 train_acc:  93.775 test_loss:  325.412 test_acc:  92.243\n",
      "epoch:  888 train_loss:  243.208 train_acc:  93.352 test_loss:  314.397 test_acc:  92.33\n",
      "epoch:  889 train_loss:  240.248 train_acc:  93.71 test_loss:  305.552 test_acc:  92.605\n",
      "epoch:  890 train_loss:  287.857 train_acc:  93.197 test_loss:  348.097 test_acc:  91.765\n",
      "epoch:  891 train_loss:  227.656 train_acc:  93.667 test_loss:  305.044 test_acc:  92.433\n",
      "epoch:  892 train_loss:  231.808 train_acc:  93.732 test_loss:  307.426 test_acc:  92.441\n",
      "epoch:  893 train_loss:  225.037 train_acc:  93.814 test_loss:  310.62 test_acc:  92.492\n",
      "epoch:  894 train_loss:  262.603 train_acc:  93.099 test_loss:  333.721 test_acc:  91.655\n",
      "epoch:  895 train_loss:  255.757 train_acc:  93.469 test_loss:  358.603 test_acc:  92.181\n",
      "epoch:  896 train_loss:  249.844 train_acc:  93.6 test_loss:  316.06 test_acc:  92.389\n",
      "epoch:  897 train_loss:  320.223 train_acc:  91.52 test_loss:  415.896 test_acc:  90.899\n",
      "epoch:  898 train_loss:  233.158 train_acc:  93.753 test_loss:  318.863 test_acc:  92.271\n",
      "epoch:  899 train_loss:  236.67 train_acc:  93.589 test_loss:  339.216 test_acc:  92.527\n",
      "epoch:  900 train_loss:  283.791 train_acc:  92.688 test_loss:  404.396 test_acc:  91.609\n",
      "epoch:  901 train_loss:  296.137 train_acc:  92.878 test_loss:  358.461 test_acc:  91.52\n",
      "epoch:  902 train_loss:  266.021 train_acc:  93.406 test_loss:  341.714 test_acc:  92.341\n",
      "epoch:  903 train_loss:  271.181 train_acc:  92.762 test_loss:  339.739 test_acc:  91.922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  904 train_loss:  241.386 train_acc:  93.682 test_loss:  327.17 test_acc:  92.254\n",
      "epoch:  905 train_loss:  428.193 train_acc:  90.187 test_loss:  562.503 test_acc:  89.331\n",
      "epoch:  906 train_loss:  289.035 train_acc:  93.065 test_loss:  383.747 test_acc:  91.671\n",
      "epoch:  907 train_loss:  268.166 train_acc:  92.997 test_loss:  371.086 test_acc:  92.11\n",
      "epoch:  908 train_loss:  233.057 train_acc:  93.647 test_loss:  339.367 test_acc:  92.204\n",
      "epoch:  909 train_loss:  250.184 train_acc:  93.544 test_loss:  337.047 test_acc:  91.946\n",
      "epoch:  910 train_loss:  267.526 train_acc:  93.57 test_loss:  365.254 test_acc:  91.928\n",
      "epoch:  911 train_loss:  243.382 train_acc:  93.305 test_loss:  320.77 test_acc:  92.24\n",
      "epoch:  912 train_loss:  268.849 train_acc:  93.289 test_loss:  369.426 test_acc:  92.163\n",
      "epoch:  913 train_loss:  262.059 train_acc:  93.233 test_loss:  373.766 test_acc:  91.998\n",
      "epoch:  914 train_loss:  233.559 train_acc:  93.647 test_loss:  340.051 test_acc:  92.151\n",
      "epoch:  915 train_loss:  243.321 train_acc:  93.129 test_loss:  321.974 test_acc:  92.123\n",
      "epoch:  916 train_loss:  263.551 train_acc:  93.463 test_loss:  351.478 test_acc:  92.189\n",
      "epoch:  917 train_loss:  225.718 train_acc:  93.885 test_loss:  304.061 test_acc:  92.58\n",
      "epoch:  918 train_loss:  255.976 train_acc:  92.863 test_loss:  362.177 test_acc:  91.458\n",
      "epoch:  919 train_loss:  238.693 train_acc:  93.491 test_loss:  328.312 test_acc:  92.179\n",
      "epoch:  920 train_loss:  239.582 train_acc:  93.584 test_loss:  313.594 test_acc:  92.518\n",
      "epoch:  921 train_loss:  391.736 train_acc:  90.385 test_loss:  464.98 test_acc:  90.065\n",
      "epoch:  922 train_loss:  298.659 train_acc:  93.282 test_loss:  377.8 test_acc:  91.659\n",
      "epoch:  923 train_loss:  254.763 train_acc:  93.212 test_loss:  367.079 test_acc:  92.176\n",
      "epoch:  924 train_loss:  274.425 train_acc:  92.933 test_loss:  343.662 test_acc:  92.042\n",
      "epoch:  925 train_loss:  239.205 train_acc:  93.83 test_loss:  320.842 test_acc:  92.397\n",
      "epoch:  926 train_loss:  270.856 train_acc:  93.34 test_loss:  352.332 test_acc:  91.944\n",
      "epoch:  927 train_loss:  262.454 train_acc:  93.399 test_loss:  345.612 test_acc:  91.94\n",
      "epoch:  928 train_loss:  274.848 train_acc:  92.541 test_loss:  357.315 test_acc:  91.758\n",
      "epoch:  929 train_loss:  225.478 train_acc:  93.858 test_loss:  296.112 test_acc:  92.634\n",
      "min loss:929\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(224.8334)]\n",
      "1    [tensor(88.8594)]   [tensor(78.4052)]\n",
      "2   [tensor(162.9521)]  [tensor(161.5108)]\n",
      "3   [tensor(185.9385)]  [tensor(207.2090)]\n",
      "4   [tensor(212.8320)]  [tensor(206.8858)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(260.7607)]\n",
      "61  [tensor(127.2646)]  [tensor(127.7913)]\n",
      "62   [tensor(84.7217)]   [tensor(81.1950)]\n",
      "63  [tensor(104.8945)]  [tensor(137.1473)]\n",
      "64  [tensor(156.6016)]  [tensor(188.3319)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  930 train_loss:  231.88 train_acc:  93.651 test_loss:  321.902 test_acc:  92.65\n",
      "epoch:  931 train_loss:  235.824 train_acc:  93.887 test_loss:  322.168 test_acc:  92.416\n",
      "epoch:  932 train_loss:  236.133 train_acc:  93.502 test_loss:  312.327 test_acc:  92.551\n",
      "epoch:  933 train_loss:  232.456 train_acc:  93.697 test_loss:  299.319 test_acc:  92.882\n",
      "max acc epoch：933        max acc：92.882\n",
      "epoch:  934 train_loss:  229.852 train_acc:  93.653 test_loss:  305.975 test_acc:  92.207\n",
      "epoch:  935 train_loss:  238.715 train_acc:  93.452 test_loss:  336.644 test_acc:  92.214\n",
      "epoch:  936 train_loss:  233.367 train_acc:  93.667 test_loss:  292.366 test_acc:  92.609\n",
      "min loss:936\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(225.8075)]\n",
      "1    [tensor(88.8594)]   [tensor(79.0880)]\n",
      "2   [tensor(162.9521)]  [tensor(161.9780)]\n",
      "3   [tensor(185.9385)]  [tensor(214.3596)]\n",
      "4   [tensor(212.8320)]  [tensor(209.3000)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(258.0220)]\n",
      "61  [tensor(127.2646)]  [tensor(128.2382)]\n",
      "62   [tensor(84.7217)]   [tensor(81.7649)]\n",
      "63  [tensor(104.8945)]  [tensor(136.2042)]\n",
      "64  [tensor(156.6016)]  [tensor(182.1033)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  937 train_loss:  334.874 train_acc:  91.516 test_loss:  400.265 test_acc:  90.944\n",
      "epoch:  938 train_loss:  228.868 train_acc:  93.658 test_loss:  321.308 test_acc:  92.386\n",
      "epoch:  939 train_loss:  222.513 train_acc:  93.909 test_loss:  309.537 test_acc:  92.589\n",
      "epoch:  940 train_loss:  250.178 train_acc:  93.255 test_loss:  359.137 test_acc:  91.599\n",
      "epoch:  941 train_loss:  323.511 train_acc:  91.993 test_loss:  421.196 test_acc:  91.272\n",
      "epoch:  942 train_loss:  235.766 train_acc:  93.27 test_loss:  317.518 test_acc:  92.14\n",
      "epoch:  943 train_loss:  237.055 train_acc:  93.846 test_loss:  325.351 test_acc:  92.864\n",
      "epoch:  944 train_loss:  254.123 train_acc:  93.415 test_loss:  338.691 test_acc:  92.062\n",
      "epoch:  945 train_loss:  229.91 train_acc:  93.61 test_loss:  315.019 test_acc:  92.771\n",
      "epoch:  946 train_loss:  231.951 train_acc:  93.672 test_loss:  310.313 test_acc:  92.306\n",
      "epoch:  947 train_loss:  249.694 train_acc:  93.523 test_loss:  320.107 test_acc:  92.482\n",
      "epoch:  948 train_loss:  337.348 train_acc:  91.301 test_loss:  422.484 test_acc:  90.638\n",
      "epoch:  949 train_loss:  230.969 train_acc:  93.726 test_loss:  318.554 test_acc:  92.527\n",
      "epoch:  950 train_loss:  251.44 train_acc:  93.522 test_loss:  327.153 test_acc:  92.207\n",
      "epoch:  951 train_loss:  243.193 train_acc:  93.42 test_loss:  348.749 test_acc:  92.135\n",
      "epoch:  952 train_loss:  242.701 train_acc:  93.606 test_loss:  322.59 test_acc:  92.515\n",
      "epoch:  953 train_loss:  252.712 train_acc:  92.878 test_loss:  325.4 test_acc:  91.895\n",
      "epoch:  954 train_loss:  227.248 train_acc:  93.913 test_loss:  311.928 test_acc:  92.86\n",
      "epoch:  955 train_loss:  219.645 train_acc:  93.965 test_loss:  309.434 test_acc:  92.669\n",
      "epoch:  956 train_loss:  254.563 train_acc:  92.998 test_loss:  360.251 test_acc:  91.882\n",
      "epoch:  957 train_loss:  223.818 train_acc:  93.922 test_loss:  304.658 test_acc:  92.513\n",
      "epoch:  958 train_loss:  231.993 train_acc:  93.879 test_loss:  316.714 test_acc:  92.918\n",
      "max acc epoch：958        max acc：92.918\n",
      "epoch:  959 train_loss:  221.827 train_acc:  93.925 test_loss:  303.52 test_acc:  92.475\n",
      "epoch:  960 train_loss:  238.523 train_acc:  93.244 test_loss:  322.975 test_acc:  92.392\n",
      "epoch:  961 train_loss:  251.73 train_acc:  93.605 test_loss:  320.342 test_acc:  92.179\n",
      "epoch:  962 train_loss:  228.662 train_acc:  93.678 test_loss:  281.411 test_acc:  92.908\n",
      "min loss:962\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(225.7865)]\n",
      "1    [tensor(88.8594)]   [tensor(79.8031)]\n",
      "2   [tensor(162.9521)]  [tensor(161.3900)]\n",
      "3   [tensor(185.9385)]  [tensor(210.7990)]\n",
      "4   [tensor(212.8320)]  [tensor(208.9854)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(261.0126)]\n",
      "61  [tensor(127.2646)]  [tensor(127.9017)]\n",
      "62   [tensor(84.7217)]   [tensor(82.8997)]\n",
      "63  [tensor(104.8945)]  [tensor(133.8580)]\n",
      "64  [tensor(156.6016)]  [tensor(181.0191)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  963 train_loss:  219.428 train_acc:  94.024 test_loss:  316.716 test_acc:  92.583\n",
      "epoch:  964 train_loss:  235.008 train_acc:  93.872 test_loss:  306.082 test_acc:  92.734\n",
      "epoch:  965 train_loss:  243.705 train_acc:  93.669 test_loss:  320.048 test_acc:  92.307\n",
      "epoch:  966 train_loss:  245.086 train_acc:  93.708 test_loss:  390.892 test_acc:  92.326\n",
      "epoch:  967 train_loss:  231.448 train_acc:  93.687 test_loss:  317.29 test_acc:  92.702\n",
      "epoch:  968 train_loss:  275.491 train_acc:  93.387 test_loss:  390.448 test_acc:  91.624\n",
      "epoch:  969 train_loss:  248.567 train_acc:  93.728 test_loss:  323.087 test_acc:  92.336\n",
      "epoch:  970 train_loss:  268.687 train_acc:  93.506 test_loss:  379.564 test_acc:  91.856\n",
      "epoch:  971 train_loss:  249.504 train_acc:  93.62 test_loss:  324.216 test_acc:  92.134\n",
      "epoch:  972 train_loss:  295.845 train_acc:  93.285 test_loss:  368.294 test_acc:  91.96\n",
      "epoch:  973 train_loss:  226.466 train_acc:  93.822 test_loss:  314.742 test_acc:  92.461\n",
      "epoch:  974 train_loss:  270.005 train_acc:  93.618 test_loss:  387.034 test_acc:  92.179\n",
      "epoch:  975 train_loss:  257.325 train_acc:  92.901 test_loss:  346.052 test_acc:  92.002\n",
      "epoch:  976 train_loss:  235.154 train_acc:  93.427 test_loss:  361.395 test_acc:  92.133\n",
      "epoch:  977 train_loss:  223.805 train_acc:  93.859 test_loss:  315.267 test_acc:  92.472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  978 train_loss:  217.033 train_acc:  94.001 test_loss:  291.191 test_acc:  92.923\n",
      "max acc epoch：978        max acc：92.923\n",
      "epoch:  979 train_loss:  345.341 train_acc:  92.599 test_loss:  429.025 test_acc:  91.604\n",
      "epoch:  980 train_loss:  241.12 train_acc:  93.454 test_loss:  350.382 test_acc:  92.065\n",
      "epoch:  981 train_loss:  225.119 train_acc:  93.693 test_loss:  293.657 test_acc:  92.481\n",
      "epoch:  982 train_loss:  225.02 train_acc:  93.678 test_loss:  291.495 test_acc:  92.775\n",
      "epoch:  983 train_loss:  289.688 train_acc:  92.56 test_loss:  383.536 test_acc:  91.631\n",
      "epoch:  984 train_loss:  221.163 train_acc:  94.023 test_loss:  308.058 test_acc:  92.737\n",
      "epoch:  985 train_loss:  219.487 train_acc:  93.863 test_loss:  297.182 test_acc:  92.816\n",
      "epoch:  986 train_loss:  223.993 train_acc:  93.847 test_loss:  310.116 test_acc:  92.561\n",
      "epoch:  987 train_loss:  239.273 train_acc:  93.705 test_loss:  304.483 test_acc:  92.359\n",
      "epoch:  988 train_loss:  218.409 train_acc:  93.923 test_loss:  306.215 test_acc:  92.689\n",
      "epoch:  989 train_loss:  215.44 train_acc:  94.035 test_loss:  298.426 test_acc:  92.793\n",
      "epoch:  990 train_loss:  219.885 train_acc:  93.839 test_loss:  296.827 test_acc:  92.638\n",
      "epoch:  991 train_loss:  216.731 train_acc:  93.734 test_loss:  297.925 test_acc:  92.645\n",
      "epoch:  992 train_loss:  227.12 train_acc:  93.742 test_loss:  306.368 test_acc:  92.582\n",
      "epoch:  993 train_loss:  245.715 train_acc:  93.236 test_loss:  318.058 test_acc:  92.094\n",
      "epoch:  994 train_loss:  290.118 train_acc:  92.657 test_loss:  356.655 test_acc:  92.155\n",
      "epoch:  995 train_loss:  238.528 train_acc:  93.818 test_loss:  298.021 test_acc:  92.755\n",
      "epoch:  996 train_loss:  240.189 train_acc:  93.408 test_loss:  312.832 test_acc:  92.387\n",
      "epoch:  997 train_loss:  230.982 train_acc:  93.843 test_loss:  288.497 test_acc:  92.803\n",
      "epoch:  998 train_loss:  224.986 train_acc:  94.109 test_loss:  304.939 test_acc:  93.023\n",
      "max acc epoch：998        max acc：93.023\n",
      "epoch:  999 train_loss:  251.775 train_acc:  93.075 test_loss:  320.868 test_acc:  92.188\n",
      "epoch:  1000 train_loss:  266.402 train_acc:  93.585 test_loss:  327.441 test_acc:  92.404\n",
      "epoch:  1001 train_loss:  227.902 train_acc:  93.927 test_loss:  293.368 test_acc:  92.772\n",
      "epoch:  1002 train_loss:  257.982 train_acc:  93.155 test_loss:  370.678 test_acc:  92.15\n",
      "epoch:  1003 train_loss:  226.051 train_acc:  93.69 test_loss:  325.132 test_acc:  92.51\n",
      "epoch:  1004 train_loss:  214.084 train_acc:  94.086 test_loss:  291.428 test_acc:  92.913\n",
      "epoch:  1005 train_loss:  221.524 train_acc:  93.828 test_loss:  312.6 test_acc:  92.75\n",
      "epoch:  1006 train_loss:  334.626 train_acc:  91.175 test_loss:  448.646 test_acc:  90.214\n",
      "epoch:  1007 train_loss:  222.418 train_acc:  93.882 test_loss:  315.514 test_acc:  92.547\n",
      "epoch:  1008 train_loss:  241.427 train_acc:  93.535 test_loss:  341.93 test_acc:  92.426\n",
      "epoch:  1009 train_loss:  242.122 train_acc:  93.737 test_loss:  312.645 test_acc:  92.555\n",
      "epoch:  1010 train_loss:  218.576 train_acc:  93.834 test_loss:  297.772 test_acc:  92.885\n",
      "epoch:  1011 train_loss:  223.811 train_acc:  94.04 test_loss:  320.205 test_acc:  92.532\n",
      "epoch:  1012 train_loss:  224.088 train_acc:  93.857 test_loss:  290.638 test_acc:  92.835\n",
      "epoch:  1013 train_loss:  234.45 train_acc:  93.687 test_loss:  304.022 test_acc:  92.413\n",
      "epoch:  1014 train_loss:  269.149 train_acc:  93.039 test_loss:  366.649 test_acc:  92.024\n",
      "epoch:  1015 train_loss:  219.018 train_acc:  93.961 test_loss:  314.651 test_acc:  92.806\n",
      "epoch:  1016 train_loss:  217.643 train_acc:  93.929 test_loss:  312.289 test_acc:  92.674\n",
      "epoch:  1017 train_loss:  223.994 train_acc:  93.864 test_loss:  290.056 test_acc:  92.711\n",
      "epoch:  1018 train_loss:  218.473 train_acc:  93.87 test_loss:  309.758 test_acc:  92.896\n",
      "epoch:  1019 train_loss:  250.708 train_acc:  93.714 test_loss:  323.017 test_acc:  92.357\n",
      "epoch:  1020 train_loss:  240.356 train_acc:  93.41 test_loss:  320.209 test_acc:  92.183\n",
      "epoch:  1021 train_loss:  214.9 train_acc:  94.024 test_loss:  291.497 test_acc:  92.814\n",
      "epoch:  1022 train_loss:  217.271 train_acc:  94.046 test_loss:  289.532 test_acc:  92.729\n",
      "epoch:  1023 train_loss:  246.671 train_acc:  93.767 test_loss:  302.469 test_acc:  92.609\n",
      "epoch:  1024 train_loss:  245.172 train_acc:  93.377 test_loss:  346.583 test_acc:  92.335\n",
      "epoch:  1025 train_loss:  266.955 train_acc:  92.892 test_loss:  373.508 test_acc:  91.877\n",
      "epoch:  1026 train_loss:  260.112 train_acc:  93.534 test_loss:  350.795 test_acc:  92.196\n",
      "epoch:  1027 train_loss:  220.239 train_acc:  93.717 test_loss:  292.93 test_acc:  92.924\n",
      "epoch:  1028 train_loss:  217.402 train_acc:  93.84 test_loss:  309.737 test_acc:  92.886\n",
      "epoch:  1029 train_loss:  224.44 train_acc:  93.777 test_loss:  307.757 test_acc:  92.457\n",
      "epoch:  1030 train_loss:  230.73 train_acc:  93.635 test_loss:  316.327 test_acc:  92.642\n",
      "epoch:  1031 train_loss:  233.242 train_acc:  93.908 test_loss:  313.903 test_acc:  92.683\n",
      "epoch:  1032 train_loss:  211.64 train_acc:  94.022 test_loss:  292.947 test_acc:  92.811\n",
      "epoch:  1033 train_loss:  245.078 train_acc:  93.69 test_loss:  327.254 test_acc:  92.251\n",
      "epoch:  1034 train_loss:  229.421 train_acc:  93.796 test_loss:  301.524 test_acc:  92.544\n",
      "epoch:  1035 train_loss:  241.235 train_acc:  93.298 test_loss:  334.401 test_acc:  91.856\n",
      "epoch:  1036 train_loss:  282.342 train_acc:  93.265 test_loss:  348.285 test_acc:  92.001\n",
      "epoch:  1037 train_loss:  247.538 train_acc:  93.477 test_loss:  343.938 test_acc:  92.233\n",
      "epoch:  1038 train_loss:  214.238 train_acc:  93.884 test_loss:  281.304 test_acc:  92.676\n",
      "min loss:1038\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(226.0631)]\n",
      "1    [tensor(88.8594)]   [tensor(78.6383)]\n",
      "2   [tensor(162.9521)]  [tensor(160.6336)]\n",
      "3   [tensor(185.9385)]  [tensor(207.4027)]\n",
      "4   [tensor(212.8320)]  [tensor(207.5325)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(263.4032)]\n",
      "61  [tensor(127.2646)]  [tensor(128.0756)]\n",
      "62   [tensor(84.7217)]   [tensor(81.6618)]\n",
      "63  [tensor(104.8945)]  [tensor(138.0746)]\n",
      "64  [tensor(156.6016)]  [tensor(185.2852)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  1039 train_loss:  225.917 train_acc:  93.942 test_loss:  316.204 test_acc:  92.29\n",
      "epoch:  1040 train_loss:  227.662 train_acc:  93.623 test_loss:  310.113 test_acc:  92.384\n",
      "epoch:  1041 train_loss:  214.807 train_acc:  93.976 test_loss:  316.804 test_acc:  92.43\n",
      "epoch:  1042 train_loss:  222.348 train_acc:  93.983 test_loss:  285.614 test_acc:  92.673\n",
      "epoch:  1043 train_loss:  224.676 train_acc:  93.91 test_loss:  277.399 test_acc:  92.634\n",
      "min loss:1043\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(226.2235)]\n",
      "1    [tensor(88.8594)]   [tensor(77.8671)]\n",
      "2   [tensor(162.9521)]  [tensor(161.0547)]\n",
      "3   [tensor(185.9385)]  [tensor(209.4412)]\n",
      "4   [tensor(212.8320)]  [tensor(211.1185)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(258.0817)]\n",
      "61  [tensor(127.2646)]  [tensor(129.1540)]\n",
      "62   [tensor(84.7217)]   [tensor(80.3815)]\n",
      "63  [tensor(104.8945)]  [tensor(125.6172)]\n",
      "64  [tensor(156.6016)]  [tensor(174.9199)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  1044 train_loss:  214.405 train_acc:  93.972 test_loss:  300.822 test_acc:  92.693\n",
      "epoch:  1045 train_loss:  239.641 train_acc:  93.729 test_loss:  318.429 test_acc:  92.648\n",
      "epoch:  1046 train_loss:  214.337 train_acc:  94.079 test_loss:  290.488 test_acc:  92.753\n",
      "epoch:  1047 train_loss:  212.198 train_acc:  93.966 test_loss:  289.698 test_acc:  92.862\n",
      "epoch:  1048 train_loss:  210.212 train_acc:  94.133 test_loss:  282.451 test_acc:  93.198\n",
      "max acc epoch：1048        max acc：93.198\n",
      "epoch:  1049 train_loss:  215.342 train_acc:  94.14 test_loss:  308.59 test_acc:  92.94\n",
      "epoch:  1050 train_loss:  209.452 train_acc:  94.102 test_loss:  281.02 test_acc:  92.908\n",
      "epoch:  1051 train_loss:  236.677 train_acc:  93.785 test_loss:  309.233 test_acc:  92.533\n",
      "epoch:  1052 train_loss:  226.689 train_acc:  94.05 test_loss:  301.471 test_acc:  92.829\n",
      "epoch:  1053 train_loss:  288.992 train_acc:  92.138 test_loss:  401.215 test_acc:  91.242\n",
      "epoch:  1054 train_loss:  243.247 train_acc:  93.794 test_loss:  319.189 test_acc:  92.643\n",
      "epoch:  1055 train_loss:  223.399 train_acc:  93.778 test_loss:  302.836 test_acc:  92.092\n",
      "epoch:  1056 train_loss:  258.734 train_acc:  93.27 test_loss:  372.335 test_acc:  92.074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1057 train_loss:  253.184 train_acc:  93.289 test_loss:  345.277 test_acc:  92.235\n",
      "epoch:  1058 train_loss:  224.735 train_acc:  93.533 test_loss:  282.543 test_acc:  92.526\n",
      "epoch:  1059 train_loss:  281.996 train_acc:  93.403 test_loss:  370.762 test_acc:  92.448\n",
      "epoch:  1060 train_loss:  285.137 train_acc:  92.531 test_loss:  364.267 test_acc:  91.617\n",
      "epoch:  1061 train_loss:  397.562 train_acc:  92.352 test_loss:  450.239 test_acc:  91.404\n",
      "epoch:  1062 train_loss:  243.049 train_acc:  92.68 test_loss:  304.535 test_acc:  91.787\n",
      "epoch:  1063 train_loss:  216.312 train_acc:  93.834 test_loss:  324.673 test_acc:  92.507\n",
      "epoch:  1064 train_loss:  235.145 train_acc:  93.51 test_loss:  337.559 test_acc:  92.216\n",
      "epoch:  1065 train_loss:  259.008 train_acc:  92.916 test_loss:  352.85 test_acc:  91.841\n",
      "epoch:  1066 train_loss:  241.76 train_acc:  93.664 test_loss:  338.342 test_acc:  92.498\n",
      "epoch:  1067 train_loss:  222.542 train_acc:  93.926 test_loss:  340.307 test_acc:  92.302\n",
      "epoch:  1068 train_loss:  210.752 train_acc:  94.177 test_loss:  294.073 test_acc:  92.891\n",
      "epoch:  1069 train_loss:  256.301 train_acc:  93.017 test_loss:  344.233 test_acc:  92.024\n",
      "epoch:  1070 train_loss:  233.849 train_acc:  93.405 test_loss:  313.044 test_acc:  92.353\n",
      "epoch:  1071 train_loss:  220.689 train_acc:  93.88 test_loss:  310.801 test_acc:  92.628\n",
      "epoch:  1072 train_loss:  227.054 train_acc:  93.658 test_loss:  298.933 test_acc:  92.41\n",
      "epoch:  1073 train_loss:  226.696 train_acc:  93.83 test_loss:  280.435 test_acc:  92.958\n",
      "epoch:  1074 train_loss:  221.957 train_acc:  94.041 test_loss:  295.948 test_acc:  92.734\n",
      "epoch:  1075 train_loss:  223.648 train_acc:  93.652 test_loss:  293.585 test_acc:  92.883\n",
      "epoch:  1076 train_loss:  244.978 train_acc:  93.824 test_loss:  346.199 test_acc:  92.402\n",
      "epoch:  1077 train_loss:  247.121 train_acc:  93.619 test_loss:  327.322 test_acc:  92.474\n",
      "epoch:  1078 train_loss:  205.693 train_acc:  94.036 test_loss:  287.597 test_acc:  92.953\n",
      "epoch:  1079 train_loss:  208.59 train_acc:  94.096 test_loss:  283.79 test_acc:  92.906\n",
      "epoch:  1080 train_loss:  205.365 train_acc:  94.096 test_loss:  288.942 test_acc:  92.934\n",
      "epoch:  1081 train_loss:  249.978 train_acc:  93.763 test_loss:  317.194 test_acc:  92.383\n",
      "epoch:  1082 train_loss:  218.393 train_acc:  94.051 test_loss:  315.292 test_acc:  92.849\n",
      "epoch:  1083 train_loss:  208.197 train_acc:  94.23 test_loss:  306.957 test_acc:  93.067\n",
      "epoch:  1084 train_loss:  239.815 train_acc:  94.039 test_loss:  336.103 test_acc:  92.781\n",
      "epoch:  1085 train_loss:  205.719 train_acc:  94.205 test_loss:  280.637 test_acc:  92.971\n",
      "epoch:  1086 train_loss:  212.449 train_acc:  93.851 test_loss:  287.788 test_acc:  92.945\n",
      "epoch:  1087 train_loss:  221.76 train_acc:  93.824 test_loss:  313.667 test_acc:  92.619\n",
      "epoch:  1088 train_loss:  218.073 train_acc:  93.716 test_loss:  277.239 test_acc:  92.468\n",
      "min loss:1088\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(233.0430)]\n",
      "1    [tensor(88.8594)]   [tensor(80.8503)]\n",
      "2   [tensor(162.9521)]  [tensor(163.9049)]\n",
      "3   [tensor(185.9385)]  [tensor(213.1315)]\n",
      "4   [tensor(212.8320)]  [tensor(214.3245)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(262.0441)]\n",
      "61  [tensor(127.2646)]  [tensor(128.3417)]\n",
      "62   [tensor(84.7217)]   [tensor(83.4286)]\n",
      "63  [tensor(104.8945)]  [tensor(140.1464)]\n",
      "64  [tensor(156.6016)]  [tensor(186.7319)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  1089 train_loss:  211.286 train_acc:  94.212 test_loss:  301.872 test_acc:  92.863\n",
      "epoch:  1090 train_loss:  214.098 train_acc:  94.079 test_loss:  271.238 test_acc:  93.014\n",
      "min loss:1090\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(227.4286)]\n",
      "1    [tensor(88.8594)]   [tensor(78.4449)]\n",
      "2   [tensor(162.9521)]  [tensor(160.0555)]\n",
      "3   [tensor(185.9385)]  [tensor(209.4830)]\n",
      "4   [tensor(212.8320)]  [tensor(210.9994)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(257.1595)]\n",
      "61  [tensor(127.2646)]  [tensor(129.3865)]\n",
      "62   [tensor(84.7217)]   [tensor(80.9506)]\n",
      "63  [tensor(104.8945)]  [tensor(127.9008)]\n",
      "64  [tensor(156.6016)]  [tensor(177.8062)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  1091 train_loss:  225.399 train_acc:  93.426 test_loss:  297.211 test_acc:  92.596\n",
      "epoch:  1092 train_loss:  237.221 train_acc:  93.259 test_loss:  322.192 test_acc:  92.209\n",
      "epoch:  1093 train_loss:  237.64 train_acc:  94.049 test_loss:  309.377 test_acc:  92.917\n",
      "epoch:  1094 train_loss:  224.138 train_acc:  93.991 test_loss:  305.165 test_acc:  92.765\n",
      "epoch:  1095 train_loss:  209.103 train_acc:  94.286 test_loss:  303.821 test_acc:  93.112\n",
      "epoch:  1096 train_loss:  207.182 train_acc:  94.184 test_loss:  293.898 test_acc:  92.982\n",
      "epoch:  1097 train_loss:  251.948 train_acc:  93.293 test_loss:  353.252 test_acc:  92.201\n",
      "epoch:  1098 train_loss:  311.192 train_acc:  92.134 test_loss:  391.862 test_acc:  91.261\n",
      "epoch:  1099 train_loss:  227.163 train_acc:  93.75 test_loss:  287.968 test_acc:  92.574\n",
      "epoch:  1100 train_loss:  257.07 train_acc:  93.302 test_loss:  377.105 test_acc:  92.069\n",
      "epoch:  1101 train_loss:  213.595 train_acc:  94.164 test_loss:  294.166 test_acc:  93.032\n",
      "epoch:  1102 train_loss:  224.415 train_acc:  93.614 test_loss:  288.364 test_acc:  92.399\n",
      "epoch:  1103 train_loss:  213.643 train_acc:  94.113 test_loss:  292.279 test_acc:  92.6\n",
      "epoch:  1104 train_loss:  209.14 train_acc:  93.96 test_loss:  274.236 test_acc:  92.591\n",
      "epoch:  1105 train_loss:  215.114 train_acc:  93.909 test_loss:  314.386 test_acc:  92.689\n",
      "epoch:  1106 train_loss:  227.566 train_acc:  94.035 test_loss:  295.216 test_acc:  92.893\n",
      "epoch:  1107 train_loss:  236.387 train_acc:  93.312 test_loss:  317.731 test_acc:  92.218\n",
      "epoch:  1108 train_loss:  225.379 train_acc:  93.889 test_loss:  278.775 test_acc:  92.844\n",
      "epoch:  1109 train_loss:  220.199 train_acc:  93.791 test_loss:  326.604 test_acc:  92.404\n",
      "epoch:  1110 train_loss:  234.01 train_acc:  93.56 test_loss:  319.57 test_acc:  92.382\n",
      "epoch:  1111 train_loss:  244.464 train_acc:  93.347 test_loss:  323.476 test_acc:  92.111\n",
      "epoch:  1112 train_loss:  245.575 train_acc:  93.405 test_loss:  302.703 test_acc:  92.435\n",
      "epoch:  1113 train_loss:  218.887 train_acc:  93.928 test_loss:  285.483 test_acc:  92.604\n",
      "epoch:  1114 train_loss:  224.593 train_acc:  93.982 test_loss:  302.176 test_acc:  93.039\n",
      "epoch:  1115 train_loss:  215.667 train_acc:  94.008 test_loss:  320.518 test_acc:  92.746\n",
      "epoch:  1116 train_loss:  226.942 train_acc:  93.388 test_loss:  306.053 test_acc:  92.229\n",
      "epoch:  1117 train_loss:  220.578 train_acc:  94.071 test_loss:  291.37 test_acc:  92.965\n",
      "epoch:  1118 train_loss:  222.162 train_acc:  93.847 test_loss:  280.452 test_acc:  92.751\n",
      "epoch:  1119 train_loss:  234.993 train_acc:  93.311 test_loss:  348.145 test_acc:  92.02\n",
      "epoch:  1120 train_loss:  216.73 train_acc:  94.146 test_loss:  289.615 test_acc:  92.967\n",
      "epoch:  1121 train_loss:  210.326 train_acc:  94.191 test_loss:  276.003 test_acc:  93.16\n",
      "epoch:  1122 train_loss:  271.026 train_acc:  92.35 test_loss:  335.942 test_acc:  91.516\n",
      "epoch:  1123 train_loss:  248.427 train_acc:  93.038 test_loss:  320.436 test_acc:  92.15\n",
      "epoch:  1124 train_loss:  260.627 train_acc:  93.445 test_loss:  331.184 test_acc:  92.624\n",
      "epoch:  1125 train_loss:  224.487 train_acc:  93.93 test_loss:  310.926 test_acc:  92.438\n",
      "epoch:  1126 train_loss:  236.721 train_acc:  93.319 test_loss:  336.024 test_acc:  92.001\n",
      "epoch:  1127 train_loss:  203.344 train_acc:  94.17 test_loss:  278.68 test_acc:  93.003\n",
      "epoch:  1128 train_loss:  267.903 train_acc:  93.663 test_loss:  352.77 test_acc:  92.357\n",
      "epoch:  1129 train_loss:  255.067 train_acc:  92.804 test_loss:  345.877 test_acc:  91.885\n",
      "epoch:  1130 train_loss:  214.196 train_acc:  94.099 test_loss:  286.08 test_acc:  92.866\n",
      "epoch:  1131 train_loss:  228.159 train_acc:  93.838 test_loss:  317.826 test_acc:  92.518\n",
      "epoch:  1132 train_loss:  205.66 train_acc:  94.075 test_loss:  273.379 test_acc:  93.024\n",
      "epoch:  1133 train_loss:  312.768 train_acc:  93.177 test_loss:  369.384 test_acc:  92.135\n",
      "epoch:  1134 train_loss:  241.401 train_acc:  93.289 test_loss:  334.272 test_acc:  92.2\n",
      "epoch:  1135 train_loss:  221.282 train_acc:  93.667 test_loss:  291.185 test_acc:  92.656\n",
      "epoch:  1136 train_loss:  208.174 train_acc:  94.067 test_loss:  283.322 test_acc:  93.078\n",
      "epoch:  1137 train_loss:  257.928 train_acc:  92.876 test_loss:  356.519 test_acc:  91.681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1138 train_loss:  234.876 train_acc:  93.699 test_loss:  316.099 test_acc:  92.643\n",
      "epoch:  1139 train_loss:  221.309 train_acc:  93.711 test_loss:  315.32 test_acc:  92.768\n",
      "epoch:  1140 train_loss:  206.447 train_acc:  94.098 test_loss:  288.81 test_acc:  92.888\n",
      "epoch:  1141 train_loss:  219.745 train_acc:  94.148 test_loss:  302.78 test_acc:  92.983\n",
      "epoch:  1142 train_loss:  240.38 train_acc:  92.988 test_loss:  343.717 test_acc:  91.94\n",
      "epoch:  1143 train_loss:  272.419 train_acc:  93.596 test_loss:  351.003 test_acc:  92.019\n",
      "epoch:  1144 train_loss:  213.689 train_acc:  93.898 test_loss:  292.376 test_acc:  92.789\n",
      "epoch:  1145 train_loss:  248.154 train_acc:  93.868 test_loss:  308.315 test_acc:  92.8\n",
      "epoch:  1146 train_loss:  223.122 train_acc:  93.579 test_loss:  269.95 test_acc:  92.446\n",
      "min loss:1146\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(232.5605)]\n",
      "1    [tensor(88.8594)]   [tensor(80.3099)]\n",
      "2   [tensor(162.9521)]  [tensor(166.0210)]\n",
      "3   [tensor(185.9385)]  [tensor(212.3404)]\n",
      "4   [tensor(212.8320)]  [tensor(216.9774)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(257.3080)]\n",
      "61  [tensor(127.2646)]  [tensor(131.6645)]\n",
      "62   [tensor(84.7217)]   [tensor(82.8198)]\n",
      "63  [tensor(104.8945)]  [tensor(134.1274)]\n",
      "64  [tensor(156.6016)]  [tensor(182.5474)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  1147 train_loss:  201.899 train_acc:  94.163 test_loss:  273.182 test_acc:  93.032\n",
      "epoch:  1148 train_loss:  226.394 train_acc:  94.15 test_loss:  285.808 test_acc:  92.958\n",
      "epoch:  1149 train_loss:  218.291 train_acc:  93.768 test_loss:  282.145 test_acc:  92.649\n",
      "epoch:  1150 train_loss:  259.836 train_acc:  92.803 test_loss:  345.04 test_acc:  91.88\n",
      "epoch:  1151 train_loss:  219.857 train_acc:  94.083 test_loss:  293.0 test_acc:  92.933\n",
      "epoch:  1152 train_loss:  225.216 train_acc:  93.542 test_loss:  302.646 test_acc:  92.446\n",
      "epoch:  1153 train_loss:  206.595 train_acc:  94.219 test_loss:  277.652 test_acc:  93.005\n",
      "epoch:  1154 train_loss:  232.061 train_acc:  93.906 test_loss:  298.447 test_acc:  92.952\n",
      "epoch:  1155 train_loss:  218.272 train_acc:  93.629 test_loss:  262.205 test_acc:  92.738\n",
      "min loss:1155\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(228.9161)]\n",
      "1    [tensor(88.8594)]   [tensor(79.3667)]\n",
      "2   [tensor(162.9521)]  [tensor(163.2090)]\n",
      "3   [tensor(185.9385)]  [tensor(212.3033)]\n",
      "4   [tensor(212.8320)]  [tensor(211.3413)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(257.0070)]\n",
      "61  [tensor(127.2646)]  [tensor(131.1068)]\n",
      "62   [tensor(84.7217)]   [tensor(81.8175)]\n",
      "63  [tensor(104.8945)]  [tensor(129.6777)]\n",
      "64  [tensor(156.6016)]  [tensor(177.3358)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  1156 train_loss:  214.519 train_acc:  93.546 test_loss:  290.953 test_acc:  92.805\n",
      "epoch:  1157 train_loss:  264.673 train_acc:  93.425 test_loss:  330.37 test_acc:  92.335\n",
      "epoch:  1158 train_loss:  202.979 train_acc:  94.175 test_loss:  280.15 test_acc:  93.072\n",
      "epoch:  1159 train_loss:  204.829 train_acc:  94.143 test_loss:  299.503 test_acc:  93.104\n",
      "epoch:  1160 train_loss:  230.411 train_acc:  94.064 test_loss:  290.529 test_acc:  93.08\n",
      "epoch:  1161 train_loss:  207.672 train_acc:  94.142 test_loss:  268.219 test_acc:  92.951\n",
      "epoch:  1162 train_loss:  248.448 train_acc:  93.805 test_loss:  319.854 test_acc:  92.546\n",
      "epoch:  1163 train_loss:  204.72 train_acc:  94.238 test_loss:  266.993 test_acc:  93.205\n",
      "max acc epoch：1163        max acc：93.205\n",
      "epoch:  1164 train_loss:  217.779 train_acc:  94.043 test_loss:  287.126 test_acc:  92.679\n",
      "epoch:  1165 train_loss:  228.336 train_acc:  93.789 test_loss:  307.041 test_acc:  92.795\n",
      "epoch:  1166 train_loss:  233.82 train_acc:  93.661 test_loss:  305.783 test_acc:  92.625\n",
      "epoch:  1167 train_loss:  227.161 train_acc:  93.893 test_loss:  292.203 test_acc:  92.676\n",
      "epoch:  1168 train_loss:  203.946 train_acc:  94.153 test_loss:  272.485 test_acc:  92.962\n",
      "epoch:  1169 train_loss:  199.256 train_acc:  94.132 test_loss:  280.619 test_acc:  93.019\n",
      "epoch:  1170 train_loss:  201.309 train_acc:  94.18 test_loss:  271.898 test_acc:  93.212\n",
      "max acc epoch：1170        max acc：93.212\n",
      "epoch:  1171 train_loss:  203.251 train_acc:  94.193 test_loss:  296.026 test_acc:  92.8\n",
      "epoch:  1172 train_loss:  223.109 train_acc:  94.027 test_loss:  289.917 test_acc:  92.803\n",
      "epoch:  1173 train_loss:  199.221 train_acc:  94.055 test_loss:  257.98 test_acc:  93.096\n",
      "min loss:1173\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(228.4290)]\n",
      "1    [tensor(88.8594)]   [tensor(80.1860)]\n",
      "2   [tensor(162.9521)]  [tensor(160.8618)]\n",
      "3   [tensor(185.9385)]  [tensor(211.8394)]\n",
      "4   [tensor(212.8320)]  [tensor(209.3562)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(260.6259)]\n",
      "61  [tensor(127.2646)]  [tensor(130.2131)]\n",
      "62   [tensor(84.7217)]   [tensor(82.9029)]\n",
      "63  [tensor(104.8945)]  [tensor(128.7555)]\n",
      "64  [tensor(156.6016)]  [tensor(176.8523)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  1174 train_loss:  198.406 train_acc:  94.322 test_loss:  274.467 test_acc:  93.267\n",
      "max acc epoch：1174        max acc：93.267\n",
      "epoch:  1175 train_loss:  287.163 train_acc:  93.433 test_loss:  372.579 test_acc:  92.488\n",
      "epoch:  1176 train_loss:  219.568 train_acc:  94.006 test_loss:  319.834 test_acc:  92.555\n",
      "epoch:  1177 train_loss:  209.529 train_acc:  94.127 test_loss:  260.467 test_acc:  93.205\n",
      "epoch:  1178 train_loss:  216.846 train_acc:  93.692 test_loss:  282.29 test_acc:  92.568\n",
      "epoch:  1179 train_loss:  241.845 train_acc:  93.699 test_loss:  306.081 test_acc:  92.89\n",
      "epoch:  1180 train_loss:  218.746 train_acc:  93.798 test_loss:  341.556 test_acc:  92.532\n",
      "epoch:  1181 train_loss:  219.885 train_acc:  93.902 test_loss:  344.585 test_acc:  92.19\n",
      "epoch:  1182 train_loss:  275.517 train_acc:  93.332 test_loss:  345.583 test_acc:  92.386\n",
      "epoch:  1183 train_loss:  201.145 train_acc:  93.926 test_loss:  283.149 test_acc:  92.776\n",
      "epoch:  1184 train_loss:  213.895 train_acc:  93.871 test_loss:  293.229 test_acc:  92.851\n",
      "epoch:  1185 train_loss:  203.882 train_acc:  93.998 test_loss:  277.119 test_acc:  92.709\n",
      "epoch:  1186 train_loss:  223.38 train_acc:  94.065 test_loss:  293.01 test_acc:  92.932\n",
      "epoch:  1187 train_loss:  193.665 train_acc:  94.404 test_loss:  265.703 test_acc:  93.284\n",
      "max acc epoch：1187        max acc：93.284\n",
      "epoch:  1188 train_loss:  206.188 train_acc:  94.2 test_loss:  284.897 test_acc:  92.799\n",
      "epoch:  1189 train_loss:  208.027 train_acc:  93.989 test_loss:  292.851 test_acc:  92.734\n",
      "epoch:  1190 train_loss:  210.32 train_acc:  94.031 test_loss:  274.636 test_acc:  93.11\n",
      "epoch:  1191 train_loss:  203.451 train_acc:  94.394 test_loss:  287.688 test_acc:  93.414\n",
      "max acc epoch：1191        max acc：93.414\n",
      "epoch:  1192 train_loss:  210.768 train_acc:  93.881 test_loss:  304.59 test_acc:  92.713\n",
      "epoch:  1193 train_loss:  206.899 train_acc:  93.662 test_loss:  301.194 test_acc:  92.401\n",
      "epoch:  1194 train_loss:  228.175 train_acc:  93.8 test_loss:  288.155 test_acc:  92.912\n",
      "epoch:  1195 train_loss:  198.848 train_acc:  94.202 test_loss:  269.363 test_acc:  93.185\n",
      "epoch:  1196 train_loss:  279.472 train_acc:  93.437 test_loss:  395.504 test_acc:  92.166\n",
      "epoch:  1197 train_loss:  219.751 train_acc:  93.745 test_loss:  291.308 test_acc:  92.954\n",
      "epoch:  1198 train_loss:  230.551 train_acc:  93.442 test_loss:  303.249 test_acc:  92.4\n",
      "epoch:  1199 train_loss:  240.305 train_acc:  92.876 test_loss:  347.075 test_acc:  92.005\n",
      "epoch:  1200 train_loss:  203.057 train_acc:  94.1 test_loss:  273.097 test_acc:  93.091\n",
      "epoch:  1201 train_loss:  204.944 train_acc:  93.858 test_loss:  288.279 test_acc:  92.835\n",
      "epoch:  1202 train_loss:  215.42 train_acc:  93.475 test_loss:  290.667 test_acc:  92.62\n",
      "epoch:  1203 train_loss:  209.546 train_acc:  94.111 test_loss:  293.204 test_acc:  92.98\n",
      "epoch:  1204 train_loss:  218.774 train_acc:  93.583 test_loss:  324.027 test_acc:  92.653\n",
      "epoch:  1205 train_loss:  206.364 train_acc:  94.055 test_loss:  265.57 test_acc:  92.918\n",
      "epoch:  1206 train_loss:  221.977 train_acc:  93.797 test_loss:  322.752 test_acc:  92.437\n",
      "epoch:  1207 train_loss:  206.912 train_acc:  93.858 test_loss:  293.272 test_acc:  92.684\n",
      "epoch:  1208 train_loss:  206.846 train_acc:  93.848 test_loss:  293.784 test_acc:  92.672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1209 train_loss:  210.011 train_acc:  94.019 test_loss:  266.968 test_acc:  93.125\n",
      "epoch:  1210 train_loss:  202.548 train_acc:  94.269 test_loss:  264.568 test_acc:  93.133\n",
      "epoch:  1211 train_loss:  202.149 train_acc:  94.101 test_loss:  271.818 test_acc:  93.016\n",
      "epoch:  1212 train_loss:  217.464 train_acc:  93.6 test_loss:  302.293 test_acc:  92.295\n",
      "epoch:  1213 train_loss:  223.415 train_acc:  93.467 test_loss:  306.839 test_acc:  92.265\n",
      "epoch:  1214 train_loss:  239.799 train_acc:  93.3 test_loss:  313.542 test_acc:  92.419\n",
      "epoch:  1215 train_loss:  203.033 train_acc:  94.19 test_loss:  273.476 test_acc:  93.007\n",
      "epoch:  1216 train_loss:  208.949 train_acc:  93.923 test_loss:  303.623 test_acc:  92.889\n",
      "epoch:  1217 train_loss:  202.48 train_acc:  94.189 test_loss:  298.241 test_acc:  93.137\n",
      "epoch:  1218 train_loss:  192.171 train_acc:  94.372 test_loss:  245.459 test_acc:  93.357\n",
      "min loss:1218\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(226.5457)]\n",
      "1    [tensor(88.8594)]   [tensor(78.5804)]\n",
      "2   [tensor(162.9521)]  [tensor(157.8883)]\n",
      "3   [tensor(185.9385)]  [tensor(206.9660)]\n",
      "4   [tensor(212.8320)]  [tensor(208.3569)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(256.6499)]\n",
      "61  [tensor(127.2646)]  [tensor(127.7660)]\n",
      "62   [tensor(84.7217)]   [tensor(81.3229)]\n",
      "63  [tensor(104.8945)]  [tensor(128.5488)]\n",
      "64  [tensor(156.6016)]  [tensor(177.5017)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  1219 train_loss:  217.209 train_acc:  93.644 test_loss:  284.548 test_acc:  92.523\n",
      "epoch:  1220 train_loss:  219.338 train_acc:  93.799 test_loss:  307.994 test_acc:  92.802\n",
      "epoch:  1221 train_loss:  223.651 train_acc:  93.219 test_loss:  301.882 test_acc:  92.04\n",
      "epoch:  1222 train_loss:  203.763 train_acc:  94.209 test_loss:  288.842 test_acc:  93.025\n",
      "epoch:  1223 train_loss:  209.663 train_acc:  93.948 test_loss:  274.271 test_acc:  92.983\n",
      "epoch:  1224 train_loss:  263.391 train_acc:  92.954 test_loss:  345.483 test_acc:  92.032\n",
      "epoch:  1225 train_loss:  214.02 train_acc:  94.185 test_loss:  292.669 test_acc:  93.085\n",
      "epoch:  1226 train_loss:  213.875 train_acc:  94.231 test_loss:  286.265 test_acc:  93.117\n",
      "epoch:  1227 train_loss:  273.837 train_acc:  92.752 test_loss:  347.564 test_acc:  91.913\n",
      "epoch:  1228 train_loss:  216.655 train_acc:  93.66 test_loss:  284.249 test_acc:  92.711\n",
      "epoch:  1229 train_loss:  216.867 train_acc:  94.087 test_loss:  276.7 test_acc:  92.837\n",
      "epoch:  1230 train_loss:  195.929 train_acc:  94.371 test_loss:  264.144 test_acc:  93.355\n",
      "epoch:  1231 train_loss:  257.574 train_acc:  92.944 test_loss:  347.962 test_acc:  91.974\n",
      "epoch:  1232 train_loss:  241.182 train_acc:  93.892 test_loss:  330.85 test_acc:  92.907\n",
      "epoch:  1233 train_loss:  198.73 train_acc:  94.259 test_loss:  287.186 test_acc:  93.215\n",
      "epoch:  1234 train_loss:  211.444 train_acc:  93.964 test_loss:  277.915 test_acc:  93.079\n",
      "epoch:  1235 train_loss:  226.722 train_acc:  94.079 test_loss:  301.031 test_acc:  92.959\n",
      "epoch:  1236 train_loss:  195.552 train_acc:  94.391 test_loss:  275.379 test_acc:  93.267\n",
      "epoch:  1237 train_loss:  199.862 train_acc:  94.122 test_loss:  270.897 test_acc:  92.881\n",
      "epoch:  1238 train_loss:  211.042 train_acc:  93.954 test_loss:  277.771 test_acc:  93.018\n",
      "epoch:  1239 train_loss:  198.576 train_acc:  94.296 test_loss:  262.539 test_acc:  93.126\n",
      "epoch:  1240 train_loss:  199.018 train_acc:  94.287 test_loss:  277.991 test_acc:  92.95\n",
      "epoch:  1241 train_loss:  237.971 train_acc:  93.734 test_loss:  280.458 test_acc:  92.731\n",
      "epoch:  1242 train_loss:  219.196 train_acc:  94.111 test_loss:  305.112 test_acc:  92.785\n",
      "epoch:  1243 train_loss:  257.197 train_acc:  92.612 test_loss:  326.881 test_acc:  91.788\n",
      "epoch:  1244 train_loss:  196.039 train_acc:  93.984 test_loss:  258.897 test_acc:  93.138\n",
      "epoch:  1245 train_loss:  208.551 train_acc:  94.262 test_loss:  278.352 test_acc:  93.175\n",
      "epoch:  1246 train_loss:  207.704 train_acc:  93.892 test_loss:  288.294 test_acc:  92.802\n",
      "epoch:  1247 train_loss:  206.553 train_acc:  94.024 test_loss:  283.973 test_acc:  92.787\n",
      "epoch:  1248 train_loss:  220.976 train_acc:  94.041 test_loss:  283.821 test_acc:  92.913\n",
      "epoch:  1249 train_loss:  201.265 train_acc:  94.312 test_loss:  261.087 test_acc:  93.386\n",
      "epoch:  1250 train_loss:  272.018 train_acc:  93.653 test_loss:  350.477 test_acc:  92.365\n",
      "epoch:  1251 train_loss:  194.712 train_acc:  94.248 test_loss:  259.326 test_acc:  93.359\n",
      "epoch:  1252 train_loss:  205.579 train_acc:  94.315 test_loss:  288.344 test_acc:  93.206\n",
      "epoch:  1253 train_loss:  200.561 train_acc:  94.293 test_loss:  276.214 test_acc:  92.99\n",
      "epoch:  1254 train_loss:  192.895 train_acc:  94.404 test_loss:  264.015 test_acc:  93.366\n",
      "epoch:  1255 train_loss:  201.054 train_acc:  94.054 test_loss:  248.597 test_acc:  93.281\n",
      "epoch:  1256 train_loss:  191.55 train_acc:  94.352 test_loss:  262.947 test_acc:  93.09\n",
      "epoch:  1257 train_loss:  266.81 train_acc:  93.783 test_loss:  426.816 test_acc:  91.849\n",
      "epoch:  1258 train_loss:  197.283 train_acc:  94.188 test_loss:  258.687 test_acc:  93.202\n",
      "epoch:  1259 train_loss:  221.322 train_acc:  94.141 test_loss:  326.779 test_acc:  92.589\n",
      "epoch:  1260 train_loss:  206.579 train_acc:  94.288 test_loss:  261.546 test_acc:  93.242\n",
      "epoch:  1261 train_loss:  198.9 train_acc:  94.349 test_loss:  275.598 test_acc:  93.337\n",
      "epoch:  1262 train_loss:  195.954 train_acc:  94.279 test_loss:  268.481 test_acc:  93.321\n",
      "epoch:  1263 train_loss:  213.958 train_acc:  94.243 test_loss:  281.748 test_acc:  93.162\n",
      "epoch:  1264 train_loss:  200.273 train_acc:  94.426 test_loss:  281.312 test_acc:  93.416\n",
      "max acc epoch：1264        max acc：93.416\n",
      "epoch:  1265 train_loss:  196.184 train_acc:  94.326 test_loss:  267.651 test_acc:  93.256\n",
      "epoch:  1266 train_loss:  233.527 train_acc:  93.722 test_loss:  342.101 test_acc:  92.45\n",
      "epoch:  1267 train_loss:  226.4 train_acc:  93.114 test_loss:  293.499 test_acc:  92.339\n",
      "epoch:  1268 train_loss:  201.476 train_acc:  94.144 test_loss:  303.109 test_acc:  92.828\n",
      "epoch:  1269 train_loss:  250.043 train_acc:  93.148 test_loss:  327.085 test_acc:  92.113\n",
      "epoch:  1270 train_loss:  194.79 train_acc:  94.317 test_loss:  254.023 test_acc:  93.373\n",
      "epoch:  1271 train_loss:  242.087 train_acc:  93.859 test_loss:  295.79 test_acc:  92.824\n",
      "epoch:  1272 train_loss:  279.272 train_acc:  93.627 test_loss:  349.782 test_acc:  92.301\n",
      "epoch:  1273 train_loss:  218.987 train_acc:  93.824 test_loss:  292.362 test_acc:  92.904\n",
      "epoch:  1274 train_loss:  209.149 train_acc:  94.361 test_loss:  300.37 test_acc:  93.235\n",
      "epoch:  1275 train_loss:  222.316 train_acc:  94.165 test_loss:  287.253 test_acc:  92.999\n",
      "epoch:  1276 train_loss:  199.222 train_acc:  94.374 test_loss:  271.605 test_acc:  93.471\n",
      "max acc epoch：1276        max acc：93.471\n",
      "epoch:  1277 train_loss:  205.03 train_acc:  94.009 test_loss:  282.929 test_acc:  92.615\n",
      "epoch:  1278 train_loss:  244.592 train_acc:  93.316 test_loss:  323.728 test_acc:  92.464\n",
      "epoch:  1279 train_loss:  204.143 train_acc:  94.234 test_loss:  300.407 test_acc:  93.111\n",
      "epoch:  1280 train_loss:  189.142 train_acc:  94.285 test_loss:  249.732 test_acc:  93.314\n",
      "epoch:  1281 train_loss:  191.107 train_acc:  94.345 test_loss:  262.831 test_acc:  93.263\n",
      "epoch:  1282 train_loss:  261.387 train_acc:  93.543 test_loss:  352.731 test_acc:  92.219\n",
      "epoch:  1283 train_loss:  216.774 train_acc:  93.815 test_loss:  279.373 test_acc:  92.877\n",
      "epoch:  1284 train_loss:  197.575 train_acc:  94.057 test_loss:  260.762 test_acc:  92.753\n",
      "epoch:  1285 train_loss:  207.554 train_acc:  93.732 test_loss:  272.324 test_acc:  92.841\n",
      "epoch:  1286 train_loss:  196.538 train_acc:  94.018 test_loss:  273.579 test_acc:  92.815\n",
      "epoch:  1287 train_loss:  188.026 train_acc:  94.393 test_loss:  250.227 test_acc:  93.497\n",
      "max acc epoch：1287        max acc：93.497\n",
      "epoch:  1288 train_loss:  203.117 train_acc:  94.264 test_loss:  272.916 test_acc:  93.073\n",
      "epoch:  1289 train_loss:  200.34 train_acc:  94.329 test_loss:  263.211 test_acc:  93.42\n",
      "epoch:  1290 train_loss:  196.537 train_acc:  94.36 test_loss:  266.709 test_acc:  93.338\n",
      "epoch:  1291 train_loss:  225.623 train_acc:  93.929 test_loss:  315.931 test_acc:  92.551\n",
      "epoch:  1292 train_loss:  218.922 train_acc:  94.226 test_loss:  284.473 test_acc:  92.932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1293 train_loss:  196.917 train_acc:  94.279 test_loss:  269.577 test_acc:  93.076\n",
      "epoch:  1294 train_loss:  209.057 train_acc:  93.775 test_loss:  283.486 test_acc:  92.807\n",
      "epoch:  1295 train_loss:  206.76 train_acc:  94.138 test_loss:  311.239 test_acc:  93.133\n",
      "epoch:  1296 train_loss:  191.154 train_acc:  94.32 test_loss:  246.054 test_acc:  93.383\n",
      "epoch:  1297 train_loss:  217.097 train_acc:  94.019 test_loss:  310.733 test_acc:  92.689\n",
      "epoch:  1298 train_loss:  226.329 train_acc:  93.778 test_loss:  296.531 test_acc:  92.8\n",
      "epoch:  1299 train_loss:  202.425 train_acc:  94.294 test_loss:  316.78 test_acc:  92.956\n",
      "epoch:  1300 train_loss:  199.561 train_acc:  94.258 test_loss:  287.84 test_acc:  92.987\n",
      "epoch:  1301 train_loss:  213.066 train_acc:  93.906 test_loss:  261.69 test_acc:  92.822\n",
      "epoch:  1302 train_loss:  202.27 train_acc:  94.042 test_loss:  260.074 test_acc:  93.233\n",
      "epoch:  1303 train_loss:  229.659 train_acc:  93.55 test_loss:  301.884 test_acc:  92.509\n",
      "epoch:  1304 train_loss:  188.361 train_acc:  94.349 test_loss:  240.34 test_acc:  93.372\n",
      "min loss:1304\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(230.4196)]\n",
      "1    [tensor(88.8594)]   [tensor(80.4638)]\n",
      "2   [tensor(162.9521)]  [tensor(159.2131)]\n",
      "3   [tensor(185.9385)]  [tensor(211.2794)]\n",
      "4   [tensor(212.8320)]  [tensor(212.6542)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(256.8090)]\n",
      "61  [tensor(127.2646)]  [tensor(128.9042)]\n",
      "62   [tensor(84.7217)]   [tensor(83.4508)]\n",
      "63  [tensor(104.8945)]  [tensor(129.2133)]\n",
      "64  [tensor(156.6016)]  [tensor(177.3081)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  1305 train_loss:  193.216 train_acc:  94.463 test_loss:  269.49 test_acc:  93.258\n",
      "epoch:  1306 train_loss:  206.849 train_acc:  93.973 test_loss:  275.78 test_acc:  92.898\n",
      "epoch:  1307 train_loss:  201.21 train_acc:  93.885 test_loss:  286.042 test_acc:  92.94\n",
      "epoch:  1308 train_loss:  187.171 train_acc:  94.498 test_loss:  257.784 test_acc:  93.242\n",
      "epoch:  1309 train_loss:  197.51 train_acc:  94.149 test_loss:  281.777 test_acc:  92.931\n",
      "epoch:  1310 train_loss:  184.9 train_acc:  94.472 test_loss:  245.576 test_acc:  93.493\n",
      "epoch:  1311 train_loss:  213.886 train_acc:  94.17 test_loss:  284.205 test_acc:  93.126\n",
      "epoch:  1312 train_loss:  252.534 train_acc:  93.298 test_loss:  326.996 test_acc:  92.427\n",
      "epoch:  1313 train_loss:  204.704 train_acc:  94.075 test_loss:  280.499 test_acc:  92.888\n",
      "epoch:  1314 train_loss:  201.132 train_acc:  94.071 test_loss:  284.228 test_acc:  92.872\n",
      "epoch:  1315 train_loss:  203.46 train_acc:  94.12 test_loss:  267.695 test_acc:  93.208\n",
      "epoch:  1316 train_loss:  228.094 train_acc:  93.338 test_loss:  283.989 test_acc:  92.338\n",
      "epoch:  1317 train_loss:  260.098 train_acc:  93.682 test_loss:  327.944 test_acc:  92.825\n",
      "epoch:  1318 train_loss:  192.88 train_acc:  94.401 test_loss:  287.835 test_acc:  93.264\n",
      "epoch:  1319 train_loss:  187.142 train_acc:  94.448 test_loss:  241.004 test_acc:  93.433\n",
      "epoch:  1320 train_loss:  193.016 train_acc:  94.33 test_loss:  260.148 test_acc:  93.22\n",
      "epoch:  1321 train_loss:  188.311 train_acc:  94.402 test_loss:  266.755 test_acc:  93.317\n",
      "epoch:  1322 train_loss:  253.307 train_acc:  93.781 test_loss:  327.64 test_acc:  92.947\n",
      "epoch:  1323 train_loss:  219.587 train_acc:  93.753 test_loss:  296.297 test_acc:  92.472\n",
      "epoch:  1324 train_loss:  218.623 train_acc:  94.138 test_loss:  280.703 test_acc:  92.951\n",
      "epoch:  1325 train_loss:  234.66 train_acc:  93.179 test_loss:  315.694 test_acc:  92.248\n",
      "epoch:  1326 train_loss:  309.637 train_acc:  91.777 test_loss:  415.125 test_acc:  91.13\n",
      "epoch:  1327 train_loss:  189.186 train_acc:  94.417 test_loss:  246.598 test_acc:  93.296\n",
      "epoch:  1328 train_loss:  194.449 train_acc:  94.377 test_loss:  274.809 test_acc:  93.172\n",
      "epoch:  1329 train_loss:  247.95 train_acc:  93.869 test_loss:  333.887 test_acc:  92.626\n",
      "epoch:  1330 train_loss:  204.635 train_acc:  93.992 test_loss:  271.954 test_acc:  92.718\n",
      "epoch:  1331 train_loss:  203.703 train_acc:  94.246 test_loss:  253.468 test_acc:  93.163\n",
      "epoch:  1332 train_loss:  193.195 train_acc:  94.059 test_loss:  247.459 test_acc:  93.151\n",
      "epoch:  1333 train_loss:  226.001 train_acc:  93.55 test_loss:  285.833 test_acc:  92.502\n",
      "epoch:  1334 train_loss:  192.453 train_acc:  94.432 test_loss:  281.384 test_acc:  93.154\n",
      "epoch:  1335 train_loss:  190.291 train_acc:  94.241 test_loss:  274.501 test_acc:  93.254\n",
      "epoch:  1336 train_loss:  192.449 train_acc:  94.398 test_loss:  241.52 test_acc:  93.325\n",
      "epoch:  1337 train_loss:  201.332 train_acc:  94.121 test_loss:  270.497 test_acc:  92.952\n",
      "epoch:  1338 train_loss:  185.508 train_acc:  94.412 test_loss:  256.204 test_acc:  93.29\n",
      "epoch:  1339 train_loss:  225.378 train_acc:  93.858 test_loss:  300.477 test_acc:  92.977\n",
      "epoch:  1340 train_loss:  209.809 train_acc:  93.871 test_loss:  262.501 test_acc:  93.07\n",
      "epoch:  1341 train_loss:  195.617 train_acc:  94.44 test_loss:  264.712 test_acc:  93.373\n",
      "epoch:  1342 train_loss:  195.402 train_acc:  94.362 test_loss:  253.786 test_acc:  93.304\n",
      "epoch:  1343 train_loss:  196.012 train_acc:  94.125 test_loss:  263.563 test_acc:  93.037\n",
      "epoch:  1344 train_loss:  185.365 train_acc:  94.325 test_loss:  237.831 test_acc:  93.439\n",
      "min loss:1344\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(229.7303)]\n",
      "1    [tensor(88.8594)]   [tensor(80.4504)]\n",
      "2   [tensor(162.9521)]  [tensor(158.6739)]\n",
      "3   [tensor(185.9385)]  [tensor(213.7693)]\n",
      "4   [tensor(212.8320)]  [tensor(211.4411)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(259.6401)]\n",
      "61  [tensor(127.2646)]  [tensor(128.0793)]\n",
      "62   [tensor(84.7217)]   [tensor(83.2737)]\n",
      "63  [tensor(104.8945)]  [tensor(127.6210)]\n",
      "64  [tensor(156.6016)]  [tensor(174.6201)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  1345 train_loss:  202.753 train_acc:  94.273 test_loss:  287.676 test_acc:  93.113\n",
      "epoch:  1346 train_loss:  182.977 train_acc:  94.492 test_loss:  240.476 test_acc:  93.336\n",
      "epoch:  1347 train_loss:  200.734 train_acc:  94.269 test_loss:  285.757 test_acc:  93.06\n",
      "epoch:  1348 train_loss:  205.945 train_acc:  94.096 test_loss:  269.29 test_acc:  93.293\n",
      "epoch:  1349 train_loss:  222.09 train_acc:  93.441 test_loss:  275.929 test_acc:  92.826\n",
      "epoch:  1350 train_loss:  201.838 train_acc:  94.341 test_loss:  238.92 test_acc:  93.538\n",
      "max acc epoch：1350        max acc：93.538\n",
      "epoch:  1351 train_loss:  193.248 train_acc:  94.32 test_loss:  278.064 test_acc:  93.08\n",
      "epoch:  1352 train_loss:  200.942 train_acc:  93.813 test_loss:  273.174 test_acc:  92.573\n",
      "epoch:  1353 train_loss:  206.59 train_acc:  94.069 test_loss:  301.424 test_acc:  92.801\n",
      "epoch:  1354 train_loss:  219.586 train_acc:  93.613 test_loss:  300.426 test_acc:  92.609\n",
      "epoch:  1355 train_loss:  203.461 train_acc:  94.327 test_loss:  267.95 test_acc:  93.134\n",
      "epoch:  1356 train_loss:  190.206 train_acc:  94.273 test_loss:  248.585 test_acc:  93.325\n",
      "epoch:  1357 train_loss:  218.343 train_acc:  93.718 test_loss:  279.952 test_acc:  92.909\n",
      "epoch:  1358 train_loss:  234.745 train_acc:  93.354 test_loss:  344.516 test_acc:  92.001\n",
      "epoch:  1359 train_loss:  205.043 train_acc:  94.006 test_loss:  254.398 test_acc:  93.059\n",
      "epoch:  1360 train_loss:  209.659 train_acc:  93.805 test_loss:  289.176 test_acc:  92.944\n",
      "epoch:  1361 train_loss:  232.019 train_acc:  93.814 test_loss:  281.676 test_acc:  92.696\n",
      "epoch:  1362 train_loss:  193.017 train_acc:  94.338 test_loss:  264.407 test_acc:  93.182\n",
      "epoch:  1363 train_loss:  190.511 train_acc:  94.169 test_loss:  244.732 test_acc:  92.926\n",
      "epoch:  1364 train_loss:  182.337 train_acc:  94.543 test_loss:  243.735 test_acc:  93.559\n",
      "max acc epoch：1364        max acc：93.559\n",
      "epoch:  1365 train_loss:  205.433 train_acc:  93.831 test_loss:  288.67 test_acc:  92.7\n",
      "epoch:  1366 train_loss:  212.962 train_acc:  94.213 test_loss:  290.339 test_acc:  92.857\n",
      "epoch:  1367 train_loss:  199.819 train_acc:  93.982 test_loss:  278.286 test_acc:  92.99\n",
      "epoch:  1368 train_loss:  194.449 train_acc:  94.084 test_loss:  274.393 test_acc:  93.27\n",
      "epoch:  1369 train_loss:  201.557 train_acc:  93.786 test_loss:  260.541 test_acc:  92.951\n",
      "epoch:  1370 train_loss:  219.618 train_acc:  93.683 test_loss:  293.214 test_acc:  92.448\n",
      "epoch:  1371 train_loss:  190.632 train_acc:  94.175 test_loss:  236.098 test_acc:  93.226\n",
      "min loss:1371\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(229.6969)]\n",
      "1    [tensor(88.8594)]   [tensor(80.6650)]\n",
      "2   [tensor(162.9521)]  [tensor(161.4880)]\n",
      "3   [tensor(185.9385)]  [tensor(212.9287)]\n",
      "4   [tensor(212.8320)]  [tensor(207.4798)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(261.0762)]\n",
      "61  [tensor(127.2646)]  [tensor(130.2732)]\n",
      "62   [tensor(84.7217)]   [tensor(83.7755)]\n",
      "63  [tensor(104.8945)]  [tensor(131.7530)]\n",
      "64  [tensor(156.6016)]  [tensor(179.2979)]\n",
      "\n",
      "[65 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1372 train_loss:  187.963 train_acc:  94.211 test_loss:  266.121 test_acc:  93.129\n",
      "epoch:  1373 train_loss:  184.969 train_acc:  94.551 test_loss:  246.065 test_acc:  93.623\n",
      "max acc epoch：1373        max acc：93.623\n",
      "epoch:  1374 train_loss:  203.864 train_acc:  93.82 test_loss:  276.783 test_acc:  92.893\n",
      "epoch:  1375 train_loss:  209.279 train_acc:  94.02 test_loss:  273.881 test_acc:  92.925\n",
      "epoch:  1376 train_loss:  220.002 train_acc:  93.535 test_loss:  287.366 test_acc:  92.724\n",
      "epoch:  1377 train_loss:  187.976 train_acc:  94.09 test_loss:  252.585 test_acc:  93.187\n",
      "epoch:  1378 train_loss:  210.624 train_acc:  93.658 test_loss:  303.74 test_acc:  92.678\n",
      "epoch:  1379 train_loss:  199.057 train_acc:  94.112 test_loss:  269.516 test_acc:  93.151\n",
      "epoch:  1380 train_loss:  229.565 train_acc:  93.876 test_loss:  295.146 test_acc:  92.966\n",
      "epoch:  1381 train_loss:  248.143 train_acc:  93.358 test_loss:  338.608 test_acc:  92.364\n",
      "epoch:  1382 train_loss:  209.772 train_acc:  93.472 test_loss:  265.051 test_acc:  92.474\n",
      "epoch:  1383 train_loss:  191.839 train_acc:  94.389 test_loss:  258.0 test_acc:  93.358\n",
      "epoch:  1384 train_loss:  195.012 train_acc:  94.196 test_loss:  268.61 test_acc:  92.72\n",
      "epoch:  1385 train_loss:  211.109 train_acc:  93.883 test_loss:  296.83 test_acc:  92.529\n",
      "epoch:  1386 train_loss:  199.215 train_acc:  94.255 test_loss:  253.533 test_acc:  93.393\n",
      "epoch:  1387 train_loss:  221.67 train_acc:  93.889 test_loss:  273.943 test_acc:  93.063\n",
      "epoch:  1388 train_loss:  205.153 train_acc:  94.257 test_loss:  270.045 test_acc:  93.188\n",
      "epoch:  1389 train_loss:  206.281 train_acc:  93.738 test_loss:  287.079 test_acc:  92.722\n",
      "epoch:  1390 train_loss:  195.33 train_acc:  93.829 test_loss:  276.003 test_acc:  92.832\n",
      "epoch:  1391 train_loss:  182.606 train_acc:  94.391 test_loss:  253.552 test_acc:  93.42\n",
      "epoch:  1392 train_loss:  187.087 train_acc:  94.297 test_loss:  257.871 test_acc:  93.247\n",
      "epoch:  1393 train_loss:  195.587 train_acc:  93.973 test_loss:  266.556 test_acc:  93.308\n",
      "epoch:  1394 train_loss:  210.342 train_acc:  94.241 test_loss:  290.246 test_acc:  93.079\n",
      "epoch:  1395 train_loss:  181.071 train_acc:  94.457 test_loss:  249.892 test_acc:  93.471\n",
      "epoch:  1396 train_loss:  280.126 train_acc:  93.688 test_loss:  356.54 test_acc:  92.862\n",
      "epoch:  1397 train_loss:  216.378 train_acc:  93.558 test_loss:  289.221 test_acc:  92.539\n",
      "epoch:  1398 train_loss:  193.204 train_acc:  94.361 test_loss:  237.69 test_acc:  93.156\n",
      "epoch:  1399 train_loss:  184.998 train_acc:  94.415 test_loss:  244.423 test_acc:  93.728\n",
      "max acc epoch：1399        max acc：93.728\n",
      "epoch:  1400 train_loss:  186.561 train_acc:  94.579 test_loss:  267.103 test_acc:  93.226\n",
      "epoch:  1401 train_loss:  207.609 train_acc:  94.253 test_loss:  257.689 test_acc:  93.307\n",
      "epoch:  1402 train_loss:  203.275 train_acc:  94.31 test_loss:  270.321 test_acc:  93.074\n",
      "epoch:  1403 train_loss:  182.753 train_acc:  94.494 test_loss:  254.111 test_acc:  93.521\n",
      "epoch:  1404 train_loss:  189.978 train_acc:  94.506 test_loss:  252.039 test_acc:  93.522\n",
      "epoch:  1405 train_loss:  207.627 train_acc:  93.969 test_loss:  290.157 test_acc:  93.025\n",
      "epoch:  1406 train_loss:  209.097 train_acc:  93.768 test_loss:  284.629 test_acc:  93.093\n",
      "epoch:  1407 train_loss:  198.54 train_acc:  94.401 test_loss:  307.132 test_acc:  93.025\n",
      "epoch:  1408 train_loss:  221.593 train_acc:  93.405 test_loss:  297.853 test_acc:  92.878\n",
      "epoch:  1409 train_loss:  200.21 train_acc:  94.251 test_loss:  261.321 test_acc:  93.242\n",
      "epoch:  1410 train_loss:  193.993 train_acc:  94.387 test_loss:  278.027 test_acc:  93.111\n",
      "epoch:  1411 train_loss:  255.025 train_acc:  92.695 test_loss:  328.852 test_acc:  92.034\n",
      "epoch:  1412 train_loss:  212.093 train_acc:  94.264 test_loss:  280.428 test_acc:  92.954\n",
      "epoch:  1413 train_loss:  197.324 train_acc:  93.678 test_loss:  237.461 test_acc:  93.036\n",
      "epoch:  1414 train_loss:  188.411 train_acc:  94.235 test_loss:  231.749 test_acc:  93.363\n",
      "min loss:1414\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(230.9539)]\n",
      "1    [tensor(88.8594)]   [tensor(80.6731)]\n",
      "2   [tensor(162.9521)]  [tensor(162.9299)]\n",
      "3   [tensor(185.9385)]  [tensor(208.5625)]\n",
      "4   [tensor(212.8320)]  [tensor(209.9345)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(256.9760)]\n",
      "61  [tensor(127.2646)]  [tensor(130.6130)]\n",
      "62   [tensor(84.7217)]   [tensor(82.7628)]\n",
      "63  [tensor(104.8945)]  [tensor(133.5958)]\n",
      "64  [tensor(156.6016)]  [tensor(176.7283)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  1415 train_loss:  189.676 train_acc:  94.139 test_loss:  251.855 test_acc:  93.073\n",
      "epoch:  1416 train_loss:  226.045 train_acc:  94.077 test_loss:  269.694 test_acc:  92.904\n",
      "epoch:  1417 train_loss:  182.575 train_acc:  94.463 test_loss:  235.485 test_acc:  93.329\n",
      "epoch:  1418 train_loss:  184.783 train_acc:  94.518 test_loss:  246.676 test_acc:  93.392\n",
      "epoch:  1419 train_loss:  253.901 train_acc:  92.997 test_loss:  293.125 test_acc:  92.179\n",
      "epoch:  1420 train_loss:  224.242 train_acc:  93.826 test_loss:  296.829 test_acc:  92.804\n",
      "epoch:  1421 train_loss:  192.771 train_acc:  94.33 test_loss:  252.846 test_acc:  93.385\n",
      "epoch:  1422 train_loss:  244.649 train_acc:  92.916 test_loss:  322.806 test_acc:  92.085\n",
      "epoch:  1423 train_loss:  206.876 train_acc:  94.208 test_loss:  278.054 test_acc:  93.235\n",
      "epoch:  1424 train_loss:  206.147 train_acc:  94.408 test_loss:  305.841 test_acc:  93.087\n",
      "epoch:  1425 train_loss:  188.107 train_acc:  94.455 test_loss:  272.366 test_acc:  93.066\n",
      "epoch:  1426 train_loss:  181.05 train_acc:  94.609 test_loss:  227.902 test_acc:  93.567\n",
      "min loss:1426\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(227.3470)]\n",
      "1    [tensor(88.8594)]   [tensor(78.1513)]\n",
      "2   [tensor(162.9521)]  [tensor(157.9228)]\n",
      "3   [tensor(185.9385)]  [tensor(207.7704)]\n",
      "4   [tensor(212.8320)]  [tensor(211.1443)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(259.5469)]\n",
      "61  [tensor(127.2646)]  [tensor(126.5944)]\n",
      "62   [tensor(84.7217)]   [tensor(80.2304)]\n",
      "63  [tensor(104.8945)]  [tensor(128.7193)]\n",
      "64  [tensor(156.6016)]  [tensor(171.7490)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  1427 train_loss:  240.459 train_acc:  93.891 test_loss:  312.283 test_acc:  92.862\n",
      "epoch:  1428 train_loss:  213.414 train_acc:  93.544 test_loss:  293.379 test_acc:  92.838\n",
      "epoch:  1429 train_loss:  195.71 train_acc:  94.28 test_loss:  241.976 test_acc:  93.4\n",
      "epoch:  1430 train_loss:  201.046 train_acc:  94.397 test_loss:  262.597 test_acc:  93.337\n",
      "epoch:  1431 train_loss:  231.697 train_acc:  94.103 test_loss:  280.343 test_acc:  93.206\n",
      "epoch:  1432 train_loss:  181.756 train_acc:  94.492 test_loss:  233.412 test_acc:  93.641\n",
      "epoch:  1433 train_loss:  179.001 train_acc:  94.596 test_loss:  250.988 test_acc:  93.45\n",
      "epoch:  1434 train_loss:  212.959 train_acc:  93.786 test_loss:  281.892 test_acc:  92.725\n",
      "epoch:  1435 train_loss:  202.772 train_acc:  94.317 test_loss:  243.785 test_acc:  93.365\n",
      "epoch:  1436 train_loss:  176.387 train_acc:  94.62 test_loss:  218.31 test_acc:  93.793\n",
      "min loss:1436\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(227.5233)]\n",
      "1    [tensor(88.8594)]   [tensor(79.0517)]\n",
      "2   [tensor(162.9521)]  [tensor(161.2214)]\n",
      "3   [tensor(185.9385)]  [tensor(207.0800)]\n",
      "4   [tensor(212.8320)]  [tensor(211.8609)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(257.6028)]\n",
      "61  [tensor(127.2646)]  [tensor(128.7078)]\n",
      "62   [tensor(84.7217)]   [tensor(81.7053)]\n",
      "63  [tensor(104.8945)]  [tensor(128.3255)]\n",
      "64  [tensor(156.6016)]  [tensor(173.1732)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "max acc epoch：1436        max acc：93.793\n",
      "epoch:  1437 train_loss:  178.262 train_acc:  94.682 test_loss:  244.118 test_acc:  93.541\n",
      "epoch:  1438 train_loss:  177.425 train_acc:  94.666 test_loss:  229.765 test_acc:  93.756\n",
      "epoch:  1439 train_loss:  200.481 train_acc:  94.256 test_loss:  243.487 test_acc:  93.338\n",
      "epoch:  1440 train_loss:  201.126 train_acc:  94.003 test_loss:  251.292 test_acc:  93.093\n",
      "epoch:  1441 train_loss:  255.295 train_acc:  93.041 test_loss:  336.341 test_acc:  92.235\n",
      "epoch:  1442 train_loss:  190.731 train_acc:  94.002 test_loss:  264.606 test_acc:  93.142\n",
      "epoch:  1443 train_loss:  199.325 train_acc:  94.111 test_loss:  253.638 test_acc:  93.289\n",
      "epoch:  1444 train_loss:  180.988 train_acc:  94.69 test_loss:  260.19 test_acc:  93.481\n",
      "epoch:  1445 train_loss:  198.58 train_acc:  94.164 test_loss:  247.699 test_acc:  93.443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1446 train_loss:  216.762 train_acc:  94.22 test_loss:  274.566 test_acc:  93.008\n",
      "epoch:  1447 train_loss:  180.939 train_acc:  94.492 test_loss:  241.265 test_acc:  93.487\n",
      "epoch:  1448 train_loss:  183.127 train_acc:  94.572 test_loss:  225.372 test_acc:  93.55\n",
      "epoch:  1449 train_loss:  207.541 train_acc:  94.383 test_loss:  269.163 test_acc:  93.143\n",
      "epoch:  1450 train_loss:  181.569 train_acc:  94.437 test_loss:  236.503 test_acc:  93.822\n",
      "max acc epoch：1450        max acc：93.822\n",
      "epoch:  1451 train_loss:  182.07 train_acc:  94.351 test_loss:  240.446 test_acc:  93.426\n",
      "epoch:  1452 train_loss:  190.018 train_acc:  94.339 test_loss:  278.918 test_acc:  93.276\n",
      "epoch:  1453 train_loss:  228.232 train_acc:  93.862 test_loss:  322.038 test_acc:  92.337\n",
      "epoch:  1454 train_loss:  266.462 train_acc:  93.449 test_loss:  366.708 test_acc:  92.762\n",
      "epoch:  1455 train_loss:  247.995 train_acc:  93.444 test_loss:  315.222 test_acc:  92.409\n",
      "epoch:  1456 train_loss:  195.711 train_acc:  94.295 test_loss:  254.571 test_acc:  93.562\n",
      "epoch:  1457 train_loss:  219.754 train_acc:  93.96 test_loss:  330.538 test_acc:  92.675\n",
      "epoch:  1458 train_loss:  193.563 train_acc:  94.116 test_loss:  262.85 test_acc:  93.08\n",
      "epoch:  1459 train_loss:  214.023 train_acc:  94.273 test_loss:  290.413 test_acc:  92.824\n",
      "epoch:  1460 train_loss:  173.629 train_acc:  94.654 test_loss:  221.845 test_acc:  93.735\n",
      "epoch:  1461 train_loss:  184.895 train_acc:  94.315 test_loss:  235.147 test_acc:  93.398\n",
      "epoch:  1462 train_loss:  198.539 train_acc:  94.317 test_loss:  279.731 test_acc:  93.44\n",
      "epoch:  1463 train_loss:  188.644 train_acc:  94.438 test_loss:  257.591 test_acc:  93.455\n",
      "epoch:  1464 train_loss:  198.128 train_acc:  93.907 test_loss:  281.417 test_acc:  92.93\n",
      "epoch:  1465 train_loss:  254.321 train_acc:  92.88 test_loss:  272.997 test_acc:  92.552\n",
      "epoch:  1466 train_loss:  203.653 train_acc:  94.172 test_loss:  258.401 test_acc:  93.327\n",
      "epoch:  1467 train_loss:  219.443 train_acc:  93.513 test_loss:  263.574 test_acc:  92.88\n",
      "epoch:  1468 train_loss:  203.494 train_acc:  94.413 test_loss:  261.541 test_acc:  93.288\n",
      "epoch:  1469 train_loss:  220.401 train_acc:  93.593 test_loss:  304.487 test_acc:  92.412\n",
      "epoch:  1470 train_loss:  180.138 train_acc:  94.559 test_loss:  239.008 test_acc:  93.278\n",
      "epoch:  1471 train_loss:  179.033 train_acc:  94.642 test_loss:  228.221 test_acc:  93.734\n",
      "epoch:  1472 train_loss:  227.275 train_acc:  94.175 test_loss:  279.742 test_acc:  93.081\n",
      "epoch:  1473 train_loss:  226.938 train_acc:  93.514 test_loss:  293.681 test_acc:  92.6\n",
      "epoch:  1474 train_loss:  174.188 train_acc:  94.647 test_loss:  233.839 test_acc:  93.734\n",
      "epoch:  1475 train_loss:  184.305 train_acc:  94.606 test_loss:  251.162 test_acc:  93.476\n",
      "epoch:  1476 train_loss:  218.367 train_acc:  94.354 test_loss:  302.503 test_acc:  93.318\n",
      "epoch:  1477 train_loss:  200.215 train_acc:  94.015 test_loss:  259.72 test_acc:  92.944\n",
      "epoch:  1478 train_loss:  183.302 train_acc:  94.299 test_loss:  248.876 test_acc:  93.459\n",
      "epoch:  1479 train_loss:  204.866 train_acc:  94.317 test_loss:  298.204 test_acc:  92.811\n",
      "epoch:  1480 train_loss:  189.419 train_acc:  93.959 test_loss:  244.97 test_acc:  93.045\n",
      "epoch:  1481 train_loss:  217.219 train_acc:  93.38 test_loss:  299.952 test_acc:  92.338\n",
      "epoch:  1482 train_loss:  242.306 train_acc:  93.732 test_loss:  324.243 test_acc:  92.895\n",
      "epoch:  1483 train_loss:  202.472 train_acc:  94.016 test_loss:  264.146 test_acc:  93.229\n",
      "epoch:  1484 train_loss:  205.584 train_acc:  93.857 test_loss:  280.362 test_acc:  92.85\n",
      "epoch:  1485 train_loss:  258.518 train_acc:  93.729 test_loss:  326.033 test_acc:  92.732\n",
      "epoch:  1486 train_loss:  196.617 train_acc:  94.042 test_loss:  277.274 test_acc:  92.984\n",
      "epoch:  1487 train_loss:  174.978 train_acc:  94.652 test_loss:  231.256 test_acc:  93.723\n",
      "epoch:  1488 train_loss:  175.865 train_acc:  94.609 test_loss:  231.966 test_acc:  93.725\n",
      "epoch:  1489 train_loss:  240.94 train_acc:  93.364 test_loss:  296.073 test_acc:  92.624\n",
      "epoch:  1490 train_loss:  249.983 train_acc:  93.66 test_loss:  299.48 test_acc:  92.677\n",
      "epoch:  1491 train_loss:  225.761 train_acc:  93.892 test_loss:  270.856 test_acc:  92.882\n",
      "epoch:  1492 train_loss:  200.964 train_acc:  94.324 test_loss:  301.32 test_acc:  92.755\n",
      "epoch:  1493 train_loss:  202.281 train_acc:  94.371 test_loss:  290.72 test_acc:  93.414\n",
      "epoch:  1494 train_loss:  183.257 train_acc:  94.292 test_loss:  243.995 test_acc:  93.405\n",
      "epoch:  1495 train_loss:  200.413 train_acc:  94.41 test_loss:  237.927 test_acc:  93.515\n",
      "epoch:  1496 train_loss:  180.358 train_acc:  94.658 test_loss:  221.576 test_acc:  93.538\n",
      "epoch:  1497 train_loss:  181.674 train_acc:  94.419 test_loss:  230.43 test_acc:  93.647\n",
      "epoch:  1498 train_loss:  193.145 train_acc:  94.41 test_loss:  227.514 test_acc:  93.519\n",
      "epoch:  1499 train_loss:  182.699 train_acc:  94.195 test_loss:  235.489 test_acc:  93.449\n",
      "epoch:  1500 train_loss:  184.141 train_acc:  94.653 test_loss:  254.677 test_acc:  93.455\n",
      "epoch:  1501 train_loss:  173.501 train_acc:  94.701 test_loss:  228.169 test_acc:  93.827\n",
      "max acc epoch：1501        max acc：93.827\n",
      "epoch:  1502 train_loss:  228.593 train_acc:  93.825 test_loss:  303.665 test_acc:  92.888\n",
      "epoch:  1503 train_loss:  179.214 train_acc:  94.565 test_loss:  239.355 test_acc:  93.449\n",
      "epoch:  1504 train_loss:  182.61 train_acc:  94.411 test_loss:  238.721 test_acc:  93.419\n",
      "epoch:  1505 train_loss:  199.926 train_acc:  94.409 test_loss:  251.262 test_acc:  93.163\n",
      "epoch:  1506 train_loss:  176.427 train_acc:  94.415 test_loss:  236.896 test_acc:  93.322\n",
      "epoch:  1507 train_loss:  181.347 train_acc:  94.568 test_loss:  238.142 test_acc:  93.679\n",
      "epoch:  1508 train_loss:  188.646 train_acc:  94.411 test_loss:  267.12 test_acc:  92.955\n",
      "epoch:  1509 train_loss:  202.463 train_acc:  94.415 test_loss:  249.778 test_acc:  93.569\n",
      "epoch:  1510 train_loss:  190.306 train_acc:  94.547 test_loss:  266.985 test_acc:  93.305\n",
      "epoch:  1511 train_loss:  178.812 train_acc:  94.445 test_loss:  210.924 test_acc:  93.771\n",
      "min loss:1511\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(230.0878)]\n",
      "1    [tensor(88.8594)]   [tensor(80.9367)]\n",
      "2   [tensor(162.9521)]  [tensor(166.1684)]\n",
      "3   [tensor(185.9385)]  [tensor(207.5222)]\n",
      "4   [tensor(212.8320)]  [tensor(215.8230)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(254.6463)]\n",
      "61  [tensor(127.2646)]  [tensor(128.1169)]\n",
      "62   [tensor(84.7217)]   [tensor(83.9089)]\n",
      "63  [tensor(104.8945)]  [tensor(129.1125)]\n",
      "64  [tensor(156.6016)]  [tensor(175.5622)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  1512 train_loss:  184.225 train_acc:  94.651 test_loss:  232.602 test_acc:  93.702\n",
      "epoch:  1513 train_loss:  172.509 train_acc:  94.709 test_loss:  229.104 test_acc:  93.853\n",
      "max acc epoch：1513        max acc：93.853\n",
      "epoch:  1514 train_loss:  173.801 train_acc:  94.721 test_loss:  224.702 test_acc:  93.646\n",
      "epoch:  1515 train_loss:  178.057 train_acc:  94.586 test_loss:  237.397 test_acc:  93.824\n",
      "epoch:  1516 train_loss:  198.606 train_acc:  94.502 test_loss:  282.534 test_acc:  93.351\n",
      "epoch:  1517 train_loss:  200.337 train_acc:  94.069 test_loss:  260.044 test_acc:  93.407\n",
      "epoch:  1518 train_loss:  186.775 train_acc:  94.373 test_loss:  239.924 test_acc:  93.47\n",
      "epoch:  1519 train_loss:  212.898 train_acc:  94.212 test_loss:  265.093 test_acc:  93.135\n",
      "epoch:  1520 train_loss:  176.315 train_acc:  94.716 test_loss:  221.743 test_acc:  93.702\n",
      "epoch:  1521 train_loss:  201.903 train_acc:  94.278 test_loss:  262.667 test_acc:  92.952\n",
      "epoch:  1522 train_loss:  179.183 train_acc:  94.612 test_loss:  256.493 test_acc:  93.463\n",
      "epoch:  1523 train_loss:  209.672 train_acc:  93.913 test_loss:  270.01 test_acc:  93.237\n",
      "epoch:  1524 train_loss:  176.528 train_acc:  94.6 test_loss:  233.907 test_acc:  93.639\n",
      "epoch:  1525 train_loss:  177.174 train_acc:  94.724 test_loss:  242.569 test_acc:  93.73\n",
      "epoch:  1526 train_loss:  200.977 train_acc:  94.401 test_loss:  284.318 test_acc:  93.198\n",
      "epoch:  1527 train_loss:  177.605 train_acc:  94.561 test_loss:  241.922 test_acc:  93.818\n",
      "epoch:  1528 train_loss:  187.478 train_acc:  94.328 test_loss:  248.84 test_acc:  93.655\n",
      "epoch:  1529 train_loss:  192.088 train_acc:  94.406 test_loss:  273.787 test_acc:  93.006\n",
      "epoch:  1530 train_loss:  212.466 train_acc:  93.976 test_loss:  254.838 test_acc:  93.286\n",
      "epoch:  1531 train_loss:  209.541 train_acc:  94.296 test_loss:  305.492 test_acc:  93.085\n",
      "epoch:  1532 train_loss:  282.25 train_acc:  93.557 test_loss:  355.506 test_acc:  92.618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1533 train_loss:  191.517 train_acc:  93.94 test_loss:  262.466 test_acc:  93.105\n",
      "epoch:  1534 train_loss:  276.075 train_acc:  93.678 test_loss:  302.727 test_acc:  92.641\n",
      "epoch:  1535 train_loss:  176.487 train_acc:  94.621 test_loss:  215.568 test_acc:  93.759\n",
      "epoch:  1536 train_loss:  188.865 train_acc:  94.54 test_loss:  247.715 test_acc:  93.334\n",
      "epoch:  1537 train_loss:  219.795 train_acc:  93.858 test_loss:  279.275 test_acc:  92.781\n",
      "epoch:  1538 train_loss:  211.723 train_acc:  93.411 test_loss:  263.918 test_acc:  92.7\n",
      "epoch:  1539 train_loss:  210.047 train_acc:  93.774 test_loss:  263.782 test_acc:  92.881\n",
      "epoch:  1540 train_loss:  173.738 train_acc:  94.68 test_loss:  232.219 test_acc:  93.437\n",
      "epoch:  1541 train_loss:  175.873 train_acc:  94.527 test_loss:  238.291 test_acc:  93.576\n",
      "epoch:  1542 train_loss:  201.88 train_acc:  94.49 test_loss:  309.368 test_acc:  92.905\n",
      "epoch:  1543 train_loss:  204.227 train_acc:  93.607 test_loss:  281.837 test_acc:  93.101\n",
      "epoch:  1544 train_loss:  191.216 train_acc:  94.527 test_loss:  274.098 test_acc:  93.233\n",
      "epoch:  1545 train_loss:  175.519 train_acc:  94.765 test_loss:  239.946 test_acc:  93.681\n",
      "epoch:  1546 train_loss:  177.442 train_acc:  94.134 test_loss:  218.053 test_acc:  93.419\n",
      "epoch:  1547 train_loss:  202.442 train_acc:  94.454 test_loss:  237.289 test_acc:  93.438\n",
      "epoch:  1548 train_loss:  171.27 train_acc:  94.616 test_loss:  217.1 test_acc:  93.813\n",
      "epoch:  1549 train_loss:  322.864 train_acc:  92.753 test_loss:  398.974 test_acc:  92.407\n",
      "epoch:  1550 train_loss:  210.435 train_acc:  93.659 test_loss:  242.587 test_acc:  93.342\n",
      "epoch:  1551 train_loss:  190.183 train_acc:  94.54 test_loss:  257.429 test_acc:  93.459\n",
      "epoch:  1552 train_loss:  198.792 train_acc:  93.99 test_loss:  257.04 test_acc:  93.059\n",
      "epoch:  1553 train_loss:  190.722 train_acc:  94.349 test_loss:  223.934 test_acc:  93.573\n",
      "epoch:  1554 train_loss:  176.441 train_acc:  94.574 test_loss:  230.46 test_acc:  93.373\n",
      "epoch:  1555 train_loss:  184.483 train_acc:  94.528 test_loss:  230.363 test_acc:  93.788\n",
      "epoch:  1556 train_loss:  204.803 train_acc:  93.844 test_loss:  243.463 test_acc:  93.113\n",
      "epoch:  1557 train_loss:  224.286 train_acc:  93.65 test_loss:  267.334 test_acc:  93.182\n",
      "epoch:  1558 train_loss:  177.982 train_acc:  94.688 test_loss:  232.784 test_acc:  93.751\n",
      "epoch:  1559 train_loss:  191.366 train_acc:  94.357 test_loss:  264.36 test_acc:  93.414\n",
      "epoch:  1560 train_loss:  181.426 train_acc:  94.659 test_loss:  223.956 test_acc:  93.705\n",
      "epoch:  1561 train_loss:  176.533 train_acc:  94.467 test_loss:  228.424 test_acc:  93.6\n",
      "epoch:  1562 train_loss:  175.855 train_acc:  94.65 test_loss:  223.413 test_acc:  93.862\n",
      "max acc epoch：1562        max acc：93.862\n",
      "epoch:  1563 train_loss:  186.653 train_acc:  94.645 test_loss:  269.139 test_acc:  93.419\n",
      "epoch:  1564 train_loss:  193.351 train_acc:  94.626 test_loss:  272.263 test_acc:  93.541\n",
      "epoch:  1565 train_loss:  240.092 train_acc:  93.808 test_loss:  293.651 test_acc:  93.107\n",
      "epoch:  1566 train_loss:  210.83 train_acc:  94.048 test_loss:  325.139 test_acc:  92.733\n",
      "epoch:  1567 train_loss:  189.438 train_acc:  94.443 test_loss:  277.447 test_acc:  93.599\n",
      "epoch:  1568 train_loss:  179.054 train_acc:  94.585 test_loss:  253.598 test_acc:  93.667\n",
      "epoch:  1569 train_loss:  187.809 train_acc:  94.243 test_loss:  229.385 test_acc:  93.522\n",
      "epoch:  1570 train_loss:  182.351 train_acc:  94.419 test_loss:  246.081 test_acc:  93.449\n",
      "epoch:  1571 train_loss:  197.234 train_acc:  93.959 test_loss:  241.897 test_acc:  93.243\n",
      "epoch:  1572 train_loss:  217.816 train_acc:  93.972 test_loss:  282.94 test_acc:  93.371\n",
      "epoch:  1573 train_loss:  173.656 train_acc:  94.655 test_loss:  218.498 test_acc:  94.124\n",
      "max acc epoch：1573        max acc：94.124\n",
      "epoch:  1574 train_loss:  183.608 train_acc:  94.538 test_loss:  248.667 test_acc:  93.452\n",
      "epoch:  1575 train_loss:  263.962 train_acc:  93.318 test_loss:  314.631 test_acc:  92.675\n",
      "epoch:  1576 train_loss:  178.002 train_acc:  94.687 test_loss:  214.73 test_acc:  93.871\n",
      "epoch:  1577 train_loss:  178.787 train_acc:  94.684 test_loss:  249.538 test_acc:  93.531\n",
      "epoch:  1578 train_loss:  193.399 train_acc:  94.453 test_loss:  250.893 test_acc:  93.573\n",
      "epoch:  1579 train_loss:  217.005 train_acc:  93.868 test_loss:  291.578 test_acc:  92.835\n",
      "epoch:  1580 train_loss:  175.326 train_acc:  94.746 test_loss:  243.549 test_acc:  93.657\n",
      "epoch:  1581 train_loss:  179.015 train_acc:  94.405 test_loss:  219.147 test_acc:  93.685\n",
      "epoch:  1582 train_loss:  232.945 train_acc:  94.201 test_loss:  292.044 test_acc:  92.989\n",
      "epoch:  1583 train_loss:  191.252 train_acc:  94.036 test_loss:  239.61 test_acc:  92.934\n",
      "epoch:  1584 train_loss:  221.359 train_acc:  93.938 test_loss:  304.631 test_acc:  93.0\n",
      "epoch:  1585 train_loss:  175.44 train_acc:  94.482 test_loss:  218.007 test_acc:  93.792\n",
      "epoch:  1586 train_loss:  179.496 train_acc:  94.556 test_loss:  230.514 test_acc:  93.933\n",
      "epoch:  1587 train_loss:  169.042 train_acc:  94.775 test_loss:  233.17 test_acc:  93.873\n",
      "epoch:  1588 train_loss:  179.019 train_acc:  94.584 test_loss:  248.03 test_acc:  93.498\n",
      "epoch:  1589 train_loss:  284.783 train_acc:  92.165 test_loss:  321.472 test_acc:  91.532\n",
      "epoch:  1590 train_loss:  244.328 train_acc:  93.205 test_loss:  295.987 test_acc:  92.542\n",
      "epoch:  1591 train_loss:  182.662 train_acc:  94.555 test_loss:  245.13 test_acc:  93.652\n",
      "epoch:  1592 train_loss:  185.932 train_acc:  94.035 test_loss:  233.518 test_acc:  93.207\n",
      "epoch:  1593 train_loss:  172.31 train_acc:  94.758 test_loss:  231.82 test_acc:  94.025\n",
      "epoch:  1594 train_loss:  171.262 train_acc:  94.783 test_loss:  231.715 test_acc:  93.647\n",
      "epoch:  1595 train_loss:  202.692 train_acc:  94.251 test_loss:  301.046 test_acc:  93.352\n",
      "epoch:  1596 train_loss:  170.626 train_acc:  94.675 test_loss:  223.102 test_acc:  93.772\n",
      "epoch:  1597 train_loss:  172.471 train_acc:  94.682 test_loss:  209.371 test_acc:  93.868\n",
      "min loss:1597\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(225.3068)]\n",
      "1    [tensor(88.8594)]   [tensor(79.3809)]\n",
      "2   [tensor(162.9521)]  [tensor(157.5073)]\n",
      "3   [tensor(185.9385)]  [tensor(212.4997)]\n",
      "4   [tensor(212.8320)]  [tensor(207.2774)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(253.0963)]\n",
      "61  [tensor(127.2646)]  [tensor(130.3452)]\n",
      "62   [tensor(84.7217)]   [tensor(81.7343)]\n",
      "63  [tensor(104.8945)]  [tensor(127.7754)]\n",
      "64  [tensor(156.6016)]  [tensor(172.4201)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  1598 train_loss:  197.421 train_acc:  93.766 test_loss:  237.089 test_acc:  93.09\n",
      "epoch:  1599 train_loss:  189.057 train_acc:  94.53 test_loss:  223.827 test_acc:  93.585\n",
      "epoch:  1600 train_loss:  191.319 train_acc:  93.997 test_loss:  231.751 test_acc:  93.163\n",
      "epoch:  1601 train_loss:  184.485 train_acc:  93.89 test_loss:  228.739 test_acc:  93.173\n",
      "epoch:  1602 train_loss:  171.833 train_acc:  94.621 test_loss:  226.841 test_acc:  93.844\n",
      "epoch:  1603 train_loss:  210.815 train_acc:  93.436 test_loss:  259.038 test_acc:  92.677\n",
      "epoch:  1604 train_loss:  184.135 train_acc:  94.547 test_loss:  222.439 test_acc:  93.742\n",
      "epoch:  1605 train_loss:  192.957 train_acc:  94.278 test_loss:  214.235 test_acc:  93.692\n",
      "epoch:  1606 train_loss:  171.278 train_acc:  94.606 test_loss:  211.918 test_acc:  93.778\n",
      "epoch:  1607 train_loss:  175.853 train_acc:  94.699 test_loss:  214.448 test_acc:  93.798\n",
      "epoch:  1608 train_loss:  231.825 train_acc:  93.783 test_loss:  277.282 test_acc:  92.894\n",
      "epoch:  1609 train_loss:  194.217 train_acc:  94.466 test_loss:  240.386 test_acc:  93.707\n",
      "epoch:  1610 train_loss:  182.192 train_acc:  94.52 test_loss:  252.676 test_acc:  93.609\n",
      "epoch:  1611 train_loss:  217.232 train_acc:  93.833 test_loss:  253.626 test_acc:  92.966\n",
      "epoch:  1612 train_loss:  253.78 train_acc:  93.874 test_loss:  326.99 test_acc:  92.891\n",
      "epoch:  1613 train_loss:  173.101 train_acc:  94.414 test_loss:  210.742 test_acc:  93.552\n",
      "epoch:  1614 train_loss:  178.816 train_acc:  94.584 test_loss:  253.894 test_acc:  93.321\n",
      "epoch:  1615 train_loss:  185.133 train_acc:  94.221 test_loss:  229.989 test_acc:  93.236\n",
      "epoch:  1616 train_loss:  183.407 train_acc:  94.521 test_loss:  273.42 test_acc:  93.448\n",
      "epoch:  1617 train_loss:  188.158 train_acc:  94.551 test_loss:  274.973 test_acc:  93.362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1618 train_loss:  244.572 train_acc:  92.896 test_loss:  300.277 test_acc:  92.228\n",
      "epoch:  1619 train_loss:  185.369 train_acc:  94.263 test_loss:  229.299 test_acc:  93.324\n",
      "epoch:  1620 train_loss:  246.012 train_acc:  94.041 test_loss:  294.72 test_acc:  92.912\n",
      "epoch:  1621 train_loss:  173.685 train_acc:  94.716 test_loss:  228.066 test_acc:  93.68\n",
      "epoch:  1622 train_loss:  232.11 train_acc:  94.263 test_loss:  281.388 test_acc:  93.3\n",
      "epoch:  1623 train_loss:  193.027 train_acc:  93.916 test_loss:  255.004 test_acc:  93.061\n",
      "epoch:  1624 train_loss:  174.15 train_acc:  94.621 test_loss:  208.984 test_acc:  93.98\n",
      "min loss:1624\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(227.3425)]\n",
      "1    [tensor(88.8594)]   [tensor(81.0650)]\n",
      "2   [tensor(162.9521)]  [tensor(160.3832)]\n",
      "3   [tensor(185.9385)]  [tensor(208.8810)]\n",
      "4   [tensor(212.8320)]  [tensor(214.7274)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(257.3008)]\n",
      "61  [tensor(127.2646)]  [tensor(130.0498)]\n",
      "62   [tensor(84.7217)]   [tensor(83.6006)]\n",
      "63  [tensor(104.8945)]  [tensor(129.0855)]\n",
      "64  [tensor(156.6016)]  [tensor(174.1383)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  1625 train_loss:  169.124 train_acc:  94.757 test_loss:  222.72 test_acc:  93.745\n",
      "epoch:  1626 train_loss:  212.411 train_acc:  94.169 test_loss:  274.895 test_acc:  93.453\n",
      "epoch:  1627 train_loss:  176.007 train_acc:  94.801 test_loss:  230.999 test_acc:  93.761\n",
      "epoch:  1628 train_loss:  175.255 train_acc:  94.749 test_loss:  219.581 test_acc:  94.005\n",
      "epoch:  1629 train_loss:  174.541 train_acc:  94.64 test_loss:  211.183 test_acc:  93.614\n",
      "epoch:  1630 train_loss:  179.449 train_acc:  94.677 test_loss:  246.54 test_acc:  93.674\n",
      "epoch:  1631 train_loss:  207.397 train_acc:  94.151 test_loss:  253.328 test_acc:  93.24\n",
      "epoch:  1632 train_loss:  169.749 train_acc:  94.604 test_loss:  205.633 test_acc:  93.763\n",
      "min loss:1632\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(227.8611)]\n",
      "1    [tensor(88.8594)]   [tensor(81.3534)]\n",
      "2   [tensor(162.9521)]  [tensor(163.0952)]\n",
      "3   [tensor(185.9385)]  [tensor(205.3038)]\n",
      "4   [tensor(212.8320)]  [tensor(214.3892)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(253.6509)]\n",
      "61  [tensor(127.2646)]  [tensor(128.3385)]\n",
      "62   [tensor(84.7217)]   [tensor(83.5989)]\n",
      "63  [tensor(104.8945)]  [tensor(135.9366)]\n",
      "64  [tensor(156.6016)]  [tensor(177.0580)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  1633 train_loss:  176.314 train_acc:  94.731 test_loss:  220.852 test_acc:  93.781\n",
      "epoch:  1634 train_loss:  206.535 train_acc:  93.673 test_loss:  270.592 test_acc:  93.039\n",
      "epoch:  1635 train_loss:  220.392 train_acc:  93.607 test_loss:  269.102 test_acc:  92.847\n",
      "epoch:  1636 train_loss:  202.12 train_acc:  93.676 test_loss:  235.67 test_acc:  92.914\n",
      "epoch:  1637 train_loss:  213.587 train_acc:  93.655 test_loss:  274.58 test_acc:  92.821\n",
      "epoch:  1638 train_loss:  169.422 train_acc:  94.709 test_loss:  215.652 test_acc:  93.799\n",
      "epoch:  1639 train_loss:  174.297 train_acc:  94.713 test_loss:  223.83 test_acc:  93.495\n",
      "epoch:  1640 train_loss:  212.346 train_acc:  94.208 test_loss:  285.25 test_acc:  93.129\n",
      "epoch:  1641 train_loss:  200.008 train_acc:  93.674 test_loss:  243.098 test_acc:  93.13\n",
      "epoch:  1642 train_loss:  213.2 train_acc:  94.272 test_loss:  275.315 test_acc:  92.972\n",
      "epoch:  1643 train_loss:  183.974 train_acc:  94.389 test_loss:  220.64 test_acc:  93.801\n",
      "epoch:  1644 train_loss:  192.198 train_acc:  94.597 test_loss:  238.685 test_acc:  93.408\n",
      "epoch:  1645 train_loss:  181.24 train_acc:  94.759 test_loss:  242.524 test_acc:  93.745\n",
      "epoch:  1646 train_loss:  177.034 train_acc:  94.503 test_loss:  243.332 test_acc:  93.49\n",
      "epoch:  1647 train_loss:  169.034 train_acc:  94.758 test_loss:  220.744 test_acc:  93.803\n",
      "epoch:  1648 train_loss:  218.429 train_acc:  94.021 test_loss:  269.773 test_acc:  93.283\n",
      "epoch:  1649 train_loss:  167.827 train_acc:  94.755 test_loss:  204.546 test_acc:  94.138\n",
      "min loss:1649\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(225.0402)]\n",
      "1    [tensor(88.8594)]   [tensor(79.6014)]\n",
      "2   [tensor(162.9521)]  [tensor(163.8797)]\n",
      "3   [tensor(185.9385)]  [tensor(209.8357)]\n",
      "4   [tensor(212.8320)]  [tensor(209.9006)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(259.7090)]\n",
      "61  [tensor(127.2646)]  [tensor(128.4653)]\n",
      "62   [tensor(84.7217)]   [tensor(82.2936)]\n",
      "63  [tensor(104.8945)]  [tensor(126.0220)]\n",
      "64  [tensor(156.6016)]  [tensor(168.5521)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "max acc epoch：1649        max acc：94.138\n",
      "epoch:  1650 train_loss:  175.48 train_acc:  94.604 test_loss:  215.783 test_acc:  93.715\n",
      "epoch:  1651 train_loss:  182.094 train_acc:  94.438 test_loss:  222.756 test_acc:  93.507\n",
      "epoch:  1652 train_loss:  263.147 train_acc:  92.135 test_loss:  286.963 test_acc:  91.951\n",
      "epoch:  1653 train_loss:  190.167 train_acc:  94.087 test_loss:  234.049 test_acc:  93.009\n",
      "epoch:  1654 train_loss:  171.918 train_acc:  94.777 test_loss:  212.518 test_acc:  93.697\n",
      "epoch:  1655 train_loss:  186.969 train_acc:  94.64 test_loss:  223.472 test_acc:  93.612\n",
      "epoch:  1656 train_loss:  183.606 train_acc:  94.259 test_loss:  214.139 test_acc:  93.705\n",
      "epoch:  1657 train_loss:  182.531 train_acc:  94.242 test_loss:  234.735 test_acc:  93.468\n",
      "epoch:  1658 train_loss:  208.882 train_acc:  94.347 test_loss:  246.991 test_acc:  93.177\n",
      "epoch:  1659 train_loss:  197.017 train_acc:  94.504 test_loss:  256.161 test_acc:  93.434\n",
      "epoch:  1660 train_loss:  170.868 train_acc:  94.689 test_loss:  202.376 test_acc:  94.027\n",
      "min loss:1660\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(229.2006)]\n",
      "1    [tensor(88.8594)]   [tensor(80.9698)]\n",
      "2   [tensor(162.9521)]  [tensor(163.6667)]\n",
      "3   [tensor(185.9385)]  [tensor(208.6384)]\n",
      "4   [tensor(212.8320)]  [tensor(214.5847)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(253.9528)]\n",
      "61  [tensor(127.2646)]  [tensor(130.3638)]\n",
      "62   [tensor(84.7217)]   [tensor(83.4130)]\n",
      "63  [tensor(104.8945)]  [tensor(126.0509)]\n",
      "64  [tensor(156.6016)]  [tensor(170.6358)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  1661 train_loss:  182.941 train_acc:  94.392 test_loss:  245.793 test_acc:  93.253\n",
      "epoch:  1662 train_loss:  169.103 train_acc:  94.561 test_loss:  206.673 test_acc:  93.767\n",
      "epoch:  1663 train_loss:  191.924 train_acc:  93.86 test_loss:  227.062 test_acc:  93.18\n",
      "epoch:  1664 train_loss:  197.455 train_acc:  94.52 test_loss:  269.232 test_acc:  93.318\n",
      "epoch:  1665 train_loss:  185.795 train_acc:  94.233 test_loss:  238.934 test_acc:  93.469\n",
      "epoch:  1666 train_loss:  180.992 train_acc:  94.439 test_loss:  252.767 test_acc:  93.402\n",
      "epoch:  1667 train_loss:  165.97 train_acc:  94.903 test_loss:  205.558 test_acc:  94.153\n",
      "max acc epoch：1667        max acc：94.153\n",
      "epoch:  1668 train_loss:  175.213 train_acc:  94.691 test_loss:  207.148 test_acc:  93.812\n",
      "epoch:  1669 train_loss:  179.247 train_acc:  94.644 test_loss:  204.723 test_acc:  94.155\n",
      "max acc epoch：1669        max acc：94.155\n",
      "epoch:  1670 train_loss:  227.18 train_acc:  94.182 test_loss:  304.031 test_acc:  92.811\n",
      "epoch:  1671 train_loss:  197.24 train_acc:  94.564 test_loss:  276.256 test_acc:  93.61\n",
      "epoch:  1672 train_loss:  199.291 train_acc:  94.009 test_loss:  238.37 test_acc:  93.589\n",
      "epoch:  1673 train_loss:  191.534 train_acc:  94.079 test_loss:  219.112 test_acc:  93.172\n",
      "epoch:  1674 train_loss:  163.499 train_acc:  94.874 test_loss:  197.215 test_acc:  94.147\n",
      "min loss:1674\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(228.1310)]\n",
      "1    [tensor(88.8594)]   [tensor(80.2235)]\n",
      "2   [tensor(162.9521)]  [tensor(160.5206)]\n",
      "3   [tensor(185.9385)]  [tensor(207.5489)]\n",
      "4   [tensor(212.8320)]  [tensor(211.9807)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(251.7190)]\n",
      "61  [tensor(127.2646)]  [tensor(128.9686)]\n",
      "62   [tensor(84.7217)]   [tensor(81.7587)]\n",
      "63  [tensor(104.8945)]  [tensor(127.8800)]\n",
      "64  [tensor(156.6016)]  [tensor(170.0062)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  1675 train_loss:  184.404 train_acc:  94.519 test_loss:  218.792 test_acc:  93.652\n",
      "epoch:  1676 train_loss:  182.822 train_acc:  94.336 test_loss:  231.547 test_acc:  93.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1677 train_loss:  209.198 train_acc:  94.194 test_loss:  246.216 test_acc:  93.179\n",
      "epoch:  1678 train_loss:  176.486 train_acc:  94.522 test_loss:  232.303 test_acc:  93.649\n",
      "epoch:  1679 train_loss:  207.26 train_acc:  93.678 test_loss:  263.921 test_acc:  93.014\n",
      "epoch:  1680 train_loss:  175.016 train_acc:  94.669 test_loss:  235.672 test_acc:  93.813\n",
      "epoch:  1681 train_loss:  254.547 train_acc:  93.599 test_loss:  268.108 test_acc:  92.832\n",
      "epoch:  1682 train_loss:  189.656 train_acc:  94.507 test_loss:  227.486 test_acc:  93.661\n",
      "epoch:  1683 train_loss:  176.098 train_acc:  94.564 test_loss:  238.192 test_acc:  93.366\n",
      "epoch:  1684 train_loss:  183.969 train_acc:  94.357 test_loss:  213.094 test_acc:  93.626\n",
      "epoch:  1685 train_loss:  257.449 train_acc:  93.852 test_loss:  362.81 test_acc:  92.583\n",
      "epoch:  1686 train_loss:  185.478 train_acc:  94.712 test_loss:  242.146 test_acc:  93.784\n",
      "epoch:  1687 train_loss:  183.678 train_acc:  94.173 test_loss:  224.762 test_acc:  93.328\n",
      "epoch:  1688 train_loss:  210.684 train_acc:  94.444 test_loss:  265.188 test_acc:  93.195\n",
      "epoch:  1689 train_loss:  180.66 train_acc:  94.446 test_loss:  218.707 test_acc:  93.559\n",
      "epoch:  1690 train_loss:  178.002 train_acc:  94.359 test_loss:  232.25 test_acc:  93.469\n",
      "epoch:  1691 train_loss:  213.773 train_acc:  93.439 test_loss:  274.305 test_acc:  92.658\n",
      "epoch:  1692 train_loss:  192.566 train_acc:  94.094 test_loss:  269.195 test_acc:  92.84\n",
      "epoch:  1693 train_loss:  176.699 train_acc:  94.689 test_loss:  205.894 test_acc:  93.864\n",
      "epoch:  1694 train_loss:  173.391 train_acc:  94.779 test_loss:  225.139 test_acc:  94.049\n",
      "epoch:  1695 train_loss:  187.916 train_acc:  94.395 test_loss:  247.445 test_acc:  93.699\n",
      "epoch:  1696 train_loss:  198.391 train_acc:  94.646 test_loss:  268.937 test_acc:  93.575\n",
      "epoch:  1697 train_loss:  171.056 train_acc:  94.677 test_loss:  208.918 test_acc:  94.044\n",
      "epoch:  1698 train_loss:  172.297 train_acc:  94.584 test_loss:  231.842 test_acc:  93.556\n",
      "epoch:  1699 train_loss:  173.288 train_acc:  94.673 test_loss:  234.441 test_acc:  93.619\n",
      "epoch:  1700 train_loss:  173.058 train_acc:  94.739 test_loss:  235.662 test_acc:  93.774\n",
      "epoch:  1701 train_loss:  176.515 train_acc:  94.263 test_loss:  220.166 test_acc:  93.539\n",
      "epoch:  1702 train_loss:  162.406 train_acc:  94.863 test_loss:  208.417 test_acc:  93.986\n",
      "epoch:  1703 train_loss:  168.085 train_acc:  94.874 test_loss:  194.314 test_acc:  94.244\n",
      "min loss:1703\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(224.8452)]\n",
      "1    [tensor(88.8594)]   [tensor(79.2295)]\n",
      "2   [tensor(162.9521)]  [tensor(157.6093)]\n",
      "3   [tensor(185.9385)]  [tensor(206.8553)]\n",
      "4   [tensor(212.8320)]  [tensor(209.1402)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(252.3201)]\n",
      "61  [tensor(127.2646)]  [tensor(127.7571)]\n",
      "62   [tensor(84.7217)]   [tensor(81.6636)]\n",
      "63  [tensor(104.8945)]  [tensor(123.6313)]\n",
      "64  [tensor(156.6016)]  [tensor(167.6768)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "max acc epoch：1703        max acc：94.244\n",
      "epoch:  1704 train_loss:  173.676 train_acc:  94.576 test_loss:  228.325 test_acc:  93.974\n",
      "epoch:  1705 train_loss:  192.6 train_acc:  94.601 test_loss:  259.321 test_acc:  93.596\n",
      "epoch:  1706 train_loss:  219.213 train_acc:  93.699 test_loss:  291.125 test_acc:  92.661\n",
      "epoch:  1707 train_loss:  166.185 train_acc:  94.778 test_loss:  214.036 test_acc:  93.747\n",
      "epoch:  1708 train_loss:  195.064 train_acc:  94.504 test_loss:  253.684 test_acc:  93.56\n",
      "epoch:  1709 train_loss:  184.204 train_acc:  94.623 test_loss:  251.342 test_acc:  93.619\n",
      "epoch:  1710 train_loss:  178.99 train_acc:  94.221 test_loss:  210.278 test_acc:  93.485\n",
      "epoch:  1711 train_loss:  199.144 train_acc:  93.947 test_loss:  234.148 test_acc:  93.445\n",
      "epoch:  1712 train_loss:  224.071 train_acc:  94.108 test_loss:  253.013 test_acc:  93.528\n",
      "epoch:  1713 train_loss:  241.243 train_acc:  94.008 test_loss:  301.968 test_acc:  93.138\n",
      "epoch:  1714 train_loss:  204.629 train_acc:  94.314 test_loss:  296.73 test_acc:  93.173\n",
      "epoch:  1715 train_loss:  175.311 train_acc:  94.746 test_loss:  242.969 test_acc:  93.782\n",
      "epoch:  1716 train_loss:  177.39 train_acc:  94.805 test_loss:  235.827 test_acc:  93.894\n",
      "epoch:  1717 train_loss:  172.025 train_acc:  94.843 test_loss:  238.05 test_acc:  93.952\n",
      "epoch:  1718 train_loss:  185.651 train_acc:  94.37 test_loss:  228.449 test_acc:  93.596\n",
      "epoch:  1719 train_loss:  169.632 train_acc:  94.632 test_loss:  199.079 test_acc:  93.921\n",
      "epoch:  1720 train_loss:  213.265 train_acc:  93.582 test_loss:  257.405 test_acc:  92.76\n",
      "epoch:  1721 train_loss:  172.281 train_acc:  94.661 test_loss:  195.308 test_acc:  93.792\n",
      "epoch:  1722 train_loss:  187.153 train_acc:  94.199 test_loss:  216.085 test_acc:  93.684\n",
      "epoch:  1723 train_loss:  226.92 train_acc:  93.654 test_loss:  271.666 test_acc:  93.083\n",
      "epoch:  1724 train_loss:  214.209 train_acc:  94.324 test_loss:  301.033 test_acc:  92.937\n",
      "epoch:  1725 train_loss:  162.658 train_acc:  94.878 test_loss:  211.025 test_acc:  93.976\n",
      "epoch:  1726 train_loss:  182.434 train_acc:  94.774 test_loss:  233.44 test_acc:  93.876\n",
      "epoch:  1727 train_loss:  187.919 train_acc:  94.554 test_loss:  229.973 test_acc:  93.502\n",
      "epoch:  1728 train_loss:  177.677 train_acc:  94.481 test_loss:  210.201 test_acc:  93.715\n",
      "epoch:  1729 train_loss:  165.028 train_acc:  94.724 test_loss:  216.674 test_acc:  94.141\n",
      "epoch:  1730 train_loss:  190.582 train_acc:  94.568 test_loss:  295.391 test_acc:  92.929\n",
      "epoch:  1731 train_loss:  236.253 train_acc:  93.314 test_loss:  309.047 test_acc:  92.538\n",
      "epoch:  1732 train_loss:  171.692 train_acc:  94.546 test_loss:  208.268 test_acc:  93.851\n",
      "epoch:  1733 train_loss:  195.3 train_acc:  93.857 test_loss:  218.319 test_acc:  93.49\n",
      "epoch:  1734 train_loss:  188.414 train_acc:  94.491 test_loss:  224.768 test_acc:  93.664\n",
      "epoch:  1735 train_loss:  185.964 train_acc:  94.502 test_loss:  209.682 test_acc:  93.726\n",
      "epoch:  1736 train_loss:  192.493 train_acc:  94.498 test_loss:  278.031 test_acc:  93.528\n",
      "epoch:  1737 train_loss:  171.857 train_acc:  94.837 test_loss:  221.69 test_acc:  93.737\n",
      "epoch:  1738 train_loss:  180.1 train_acc:  94.6 test_loss:  254.874 test_acc:  93.735\n",
      "epoch:  1739 train_loss:  178.599 train_acc:  94.457 test_loss:  236.837 test_acc:  93.416\n",
      "epoch:  1740 train_loss:  224.242 train_acc:  93.585 test_loss:  256.325 test_acc:  92.929\n",
      "epoch:  1741 train_loss:  199.378 train_acc:  93.686 test_loss:  219.822 test_acc:  93.397\n",
      "epoch:  1742 train_loss:  161.768 train_acc:  94.826 test_loss:  202.762 test_acc:  94.116\n",
      "epoch:  1743 train_loss:  297.181 train_acc:  91.755 test_loss:  330.313 test_acc:  91.296\n",
      "epoch:  1744 train_loss:  217.061 train_acc:  94.263 test_loss:  260.584 test_acc:  93.355\n",
      "epoch:  1745 train_loss:  164.45 train_acc:  94.798 test_loss:  215.673 test_acc:  94.09\n",
      "epoch:  1746 train_loss:  232.605 train_acc:  93.254 test_loss:  283.104 test_acc:  92.547\n",
      "epoch:  1747 train_loss:  175.583 train_acc:  94.314 test_loss:  219.212 test_acc:  93.614\n",
      "epoch:  1748 train_loss:  169.12 train_acc:  94.664 test_loss:  217.508 test_acc:  93.707\n",
      "epoch:  1749 train_loss:  169.203 train_acc:  94.803 test_loss:  201.645 test_acc:  94.153\n",
      "epoch:  1750 train_loss:  169.636 train_acc:  94.597 test_loss:  232.559 test_acc:  93.623\n",
      "epoch:  1751 train_loss:  162.641 train_acc:  94.757 test_loss:  205.171 test_acc:  94.091\n",
      "epoch:  1752 train_loss:  172.166 train_acc:  94.74 test_loss:  209.413 test_acc:  93.711\n",
      "epoch:  1753 train_loss:  191.261 train_acc:  94.684 test_loss:  260.898 test_acc:  93.694\n",
      "epoch:  1754 train_loss:  168.222 train_acc:  94.646 test_loss:  225.024 test_acc:  93.886\n",
      "epoch:  1755 train_loss:  196.833 train_acc:  94.084 test_loss:  259.917 test_acc:  93.22\n",
      "epoch:  1756 train_loss:  166.107 train_acc:  94.731 test_loss:  211.999 test_acc:  93.984\n",
      "epoch:  1757 train_loss:  163.314 train_acc:  94.762 test_loss:  206.037 test_acc:  93.873\n",
      "epoch:  1758 train_loss:  220.716 train_acc:  93.367 test_loss:  267.864 test_acc:  92.703\n",
      "epoch:  1759 train_loss:  169.452 train_acc:  94.555 test_loss:  213.984 test_acc:  93.625\n",
      "epoch:  1760 train_loss:  170.072 train_acc:  94.425 test_loss:  198.222 test_acc:  93.701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1761 train_loss:  167.729 train_acc:  94.846 test_loss:  217.989 test_acc:  93.982\n",
      "epoch:  1762 train_loss:  210.928 train_acc:  94.372 test_loss:  265.467 test_acc:  93.377\n",
      "epoch:  1763 train_loss:  225.552 train_acc:  94.353 test_loss:  291.598 test_acc:  93.498\n",
      "epoch:  1764 train_loss:  166.934 train_acc:  94.737 test_loss:  228.18 test_acc:  93.783\n",
      "epoch:  1765 train_loss:  220.411 train_acc:  94.199 test_loss:  272.435 test_acc:  93.147\n",
      "epoch:  1766 train_loss:  175.849 train_acc:  94.517 test_loss:  240.439 test_acc:  93.403\n",
      "epoch:  1767 train_loss:  193.261 train_acc:  94.553 test_loss:  269.863 test_acc:  93.47\n",
      "epoch:  1768 train_loss:  181.858 train_acc:  94.743 test_loss:  232.859 test_acc:  93.715\n",
      "epoch:  1769 train_loss:  180.436 train_acc:  94.716 test_loss:  223.696 test_acc:  93.687\n",
      "epoch:  1770 train_loss:  200.552 train_acc:  93.737 test_loss:  215.521 test_acc:  93.322\n",
      "epoch:  1771 train_loss:  176.926 train_acc:  94.304 test_loss:  216.454 test_acc:  93.785\n",
      "epoch:  1772 train_loss:  177.66 train_acc:  94.73 test_loss:  234.274 test_acc:  94.002\n",
      "epoch:  1773 train_loss:  241.408 train_acc:  93.073 test_loss:  296.849 test_acc:  92.577\n",
      "epoch:  1774 train_loss:  175.418 train_acc:  94.576 test_loss:  205.458 test_acc:  93.698\n",
      "epoch:  1775 train_loss:  236.935 train_acc:  93.372 test_loss:  280.456 test_acc:  92.832\n",
      "epoch:  1776 train_loss:  201.192 train_acc:  93.802 test_loss:  233.784 test_acc:  93.377\n",
      "epoch:  1777 train_loss:  244.016 train_acc:  94.013 test_loss:  287.151 test_acc:  93.105\n",
      "epoch:  1778 train_loss:  177.977 train_acc:  94.303 test_loss:  213.54 test_acc:  93.472\n",
      "epoch:  1779 train_loss:  222.591 train_acc:  94.187 test_loss:  274.133 test_acc:  93.418\n",
      "epoch:  1780 train_loss:  168.952 train_acc:  94.6 test_loss:  206.9 test_acc:  93.842\n",
      "epoch:  1781 train_loss:  197.717 train_acc:  94.406 test_loss:  278.593 test_acc:  93.162\n",
      "epoch:  1782 train_loss:  166.793 train_acc:  94.876 test_loss:  230.114 test_acc:  93.964\n",
      "epoch:  1783 train_loss:  170.742 train_acc:  94.791 test_loss:  201.444 test_acc:  93.86\n",
      "epoch:  1784 train_loss:  195.181 train_acc:  94.521 test_loss:  228.5 test_acc:  93.668\n",
      "epoch:  1785 train_loss:  185.622 train_acc:  94.701 test_loss:  235.997 test_acc:  93.747\n",
      "epoch:  1786 train_loss:  181.94 train_acc:  94.71 test_loss:  243.083 test_acc:  93.615\n",
      "epoch:  1787 train_loss:  190.215 train_acc:  93.891 test_loss:  259.834 test_acc:  93.141\n",
      "epoch:  1788 train_loss:  200.553 train_acc:  94.314 test_loss:  242.476 test_acc:  93.535\n",
      "epoch:  1789 train_loss:  165.619 train_acc:  94.916 test_loss:  222.343 test_acc:  94.149\n",
      "epoch:  1790 train_loss:  194.156 train_acc:  94.522 test_loss:  242.752 test_acc:  93.735\n",
      "epoch:  1791 train_loss:  244.619 train_acc:  93.891 test_loss:  304.439 test_acc:  92.877\n",
      "epoch:  1792 train_loss:  171.613 train_acc:  94.608 test_loss:  219.049 test_acc:  93.492\n",
      "epoch:  1793 train_loss:  201.477 train_acc:  93.8 test_loss:  250.747 test_acc:  93.16\n",
      "epoch:  1794 train_loss:  171.693 train_acc:  94.59 test_loss:  216.293 test_acc:  93.783\n",
      "epoch:  1795 train_loss:  165.576 train_acc:  94.716 test_loss:  222.783 test_acc:  93.755\n",
      "epoch:  1796 train_loss:  203.409 train_acc:  94.055 test_loss:  284.485 test_acc:  93.16\n",
      "epoch:  1797 train_loss:  196.806 train_acc:  94.232 test_loss:  245.891 test_acc:  93.103\n",
      "epoch:  1798 train_loss:  169.663 train_acc:  94.53 test_loss:  194.868 test_acc:  93.854\n",
      "epoch:  1799 train_loss:  166.454 train_acc:  94.765 test_loss:  226.698 test_acc:  93.86\n",
      "epoch:  1800 train_loss:  177.983 train_acc:  94.802 test_loss:  238.069 test_acc:  93.913\n",
      "epoch:  1801 train_loss:  240.751 train_acc:  92.728 test_loss:  283.36 test_acc:  92.213\n",
      "epoch:  1802 train_loss:  176.968 train_acc:  94.359 test_loss:  189.977 test_acc:  93.971\n",
      "min loss:1802\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(229.9941)]\n",
      "1    [tensor(88.8594)]   [tensor(82.8161)]\n",
      "2   [tensor(162.9521)]  [tensor(164.2808)]\n",
      "3   [tensor(185.9385)]  [tensor(208.1185)]\n",
      "4   [tensor(212.8320)]  [tensor(215.5694)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(254.8490)]\n",
      "61  [tensor(127.2646)]  [tensor(129.5288)]\n",
      "62   [tensor(84.7217)]   [tensor(85.6471)]\n",
      "63  [tensor(104.8945)]  [tensor(133.1476)]\n",
      "64  [tensor(156.6016)]  [tensor(175.8171)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  1803 train_loss:  191.549 train_acc:  94.528 test_loss:  246.476 test_acc:  93.7\n",
      "epoch:  1804 train_loss:  182.097 train_acc:  94.636 test_loss:  229.607 test_acc:  93.662\n",
      "epoch:  1805 train_loss:  239.761 train_acc:  93.147 test_loss:  285.538 test_acc:  92.831\n",
      "epoch:  1806 train_loss:  162.957 train_acc:  94.837 test_loss:  190.98 test_acc:  94.173\n",
      "epoch:  1807 train_loss:  197.536 train_acc:  93.942 test_loss:  224.471 test_acc:  93.248\n",
      "epoch:  1808 train_loss:  165.762 train_acc:  94.876 test_loss:  195.432 test_acc:  94.161\n",
      "epoch:  1809 train_loss:  167.784 train_acc:  94.729 test_loss:  213.297 test_acc:  93.718\n",
      "epoch:  1810 train_loss:  177.232 train_acc:  94.293 test_loss:  215.053 test_acc:  93.673\n",
      "epoch:  1811 train_loss:  163.709 train_acc:  94.694 test_loss:  198.882 test_acc:  94.044\n",
      "epoch:  1812 train_loss:  160.243 train_acc:  94.927 test_loss:  191.704 test_acc:  94.138\n",
      "epoch:  1813 train_loss:  171.504 train_acc:  94.731 test_loss:  206.51 test_acc:  93.984\n",
      "epoch:  1814 train_loss:  167.81 train_acc:  94.899 test_loss:  215.752 test_acc:  94.027\n",
      "epoch:  1815 train_loss:  172.887 train_acc:  94.525 test_loss:  214.754 test_acc:  93.643\n",
      "epoch:  1816 train_loss:  169.911 train_acc:  94.505 test_loss:  207.529 test_acc:  93.782\n",
      "epoch:  1817 train_loss:  164.726 train_acc:  94.712 test_loss:  186.647 test_acc:  94.063\n",
      "min loss:1817\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(228.5707)]\n",
      "1    [tensor(88.8594)]   [tensor(80.5571)]\n",
      "2   [tensor(162.9521)]  [tensor(159.6833)]\n",
      "3   [tensor(185.9385)]  [tensor(207.7175)]\n",
      "4   [tensor(212.8320)]  [tensor(208.4085)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(251.3123)]\n",
      "61  [tensor(127.2646)]  [tensor(127.2486)]\n",
      "62   [tensor(84.7217)]   [tensor(81.9035)]\n",
      "63  [tensor(104.8945)]  [tensor(130.5865)]\n",
      "64  [tensor(156.6016)]  [tensor(170.2699)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  1818 train_loss:  367.023 train_acc:  90.489 test_loss:  426.475 test_acc:  90.296\n",
      "epoch:  1819 train_loss:  189.917 train_acc:  94.357 test_loss:  229.614 test_acc:  93.596\n",
      "epoch:  1820 train_loss:  175.077 train_acc:  94.806 test_loss:  235.47 test_acc:  93.84\n",
      "epoch:  1821 train_loss:  166.48 train_acc:  94.7 test_loss:  214.248 test_acc:  94.169\n",
      "epoch:  1822 train_loss:  209.02 train_acc:  94.224 test_loss:  272.848 test_acc:  93.089\n",
      "epoch:  1823 train_loss:  232.281 train_acc:  93.936 test_loss:  261.374 test_acc:  93.052\n",
      "epoch:  1824 train_loss:  210.385 train_acc:  94.1 test_loss:  284.573 test_acc:  93.449\n",
      "epoch:  1825 train_loss:  170.004 train_acc:  94.582 test_loss:  223.768 test_acc:  93.607\n",
      "epoch:  1826 train_loss:  172.656 train_acc:  94.611 test_loss:  227.169 test_acc:  93.727\n",
      "epoch:  1827 train_loss:  176.445 train_acc:  94.735 test_loss:  246.031 test_acc:  93.841\n",
      "epoch:  1828 train_loss:  174.955 train_acc:  94.275 test_loss:  219.758 test_acc:  93.402\n",
      "epoch:  1829 train_loss:  172.014 train_acc:  94.851 test_loss:  228.623 test_acc:  93.788\n",
      "epoch:  1830 train_loss:  189.483 train_acc:  94.468 test_loss:  276.781 test_acc:  93.438\n",
      "epoch:  1831 train_loss:  171.089 train_acc:  94.449 test_loss:  218.332 test_acc:  93.701\n",
      "epoch:  1832 train_loss:  165.565 train_acc:  94.781 test_loss:  215.719 test_acc:  93.741\n",
      "epoch:  1833 train_loss:  178.042 train_acc:  94.488 test_loss:  225.314 test_acc:  93.431\n",
      "epoch:  1834 train_loss:  233.624 train_acc:  92.996 test_loss:  280.74 test_acc:  92.518\n",
      "epoch:  1835 train_loss:  235.182 train_acc:  93.847 test_loss:  262.028 test_acc:  92.972\n",
      "epoch:  1836 train_loss:  171.983 train_acc:  94.694 test_loss:  247.232 test_acc:  93.456\n",
      "epoch:  1837 train_loss:  190.951 train_acc:  94.252 test_loss:  241.863 test_acc:  93.464\n",
      "epoch:  1838 train_loss:  188.145 train_acc:  94.59 test_loss:  250.709 test_acc:  93.712\n",
      "epoch:  1839 train_loss:  161.36 train_acc:  94.892 test_loss:  199.447 test_acc:  94.165\n",
      "epoch:  1840 train_loss:  178.292 train_acc:  94.526 test_loss:  207.308 test_acc:  93.872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1841 train_loss:  161.325 train_acc:  94.9 test_loss:  192.3 test_acc:  93.915\n",
      "epoch:  1842 train_loss:  192.709 train_acc:  93.658 test_loss:  238.657 test_acc:  92.98\n",
      "epoch:  1843 train_loss:  170.01 train_acc:  94.573 test_loss:  211.896 test_acc:  93.906\n",
      "epoch:  1844 train_loss:  179.942 train_acc:  94.267 test_loss:  253.119 test_acc:  93.268\n",
      "epoch:  1845 train_loss:  162.311 train_acc:  94.928 test_loss:  188.571 test_acc:  94.257\n",
      "max acc epoch：1845        max acc：94.257\n",
      "epoch:  1846 train_loss:  162.071 train_acc:  94.9 test_loss:  214.37 test_acc:  94.011\n",
      "epoch:  1847 train_loss:  247.247 train_acc:  92.796 test_loss:  287.745 test_acc:  92.573\n",
      "epoch:  1848 train_loss:  160.676 train_acc:  94.807 test_loss:  206.371 test_acc:  94.016\n",
      "epoch:  1849 train_loss:  173.783 train_acc:  94.542 test_loss:  212.88 test_acc:  93.943\n",
      "epoch:  1850 train_loss:  217.5 train_acc:  94.369 test_loss:  262.01 test_acc:  93.367\n",
      "epoch:  1851 train_loss:  164.15 train_acc:  94.856 test_loss:  212.686 test_acc:  93.974\n",
      "epoch:  1852 train_loss:  189.397 train_acc:  94.142 test_loss:  215.036 test_acc:  93.542\n",
      "epoch:  1853 train_loss:  169.067 train_acc:  94.578 test_loss:  203.956 test_acc:  93.961\n",
      "epoch:  1854 train_loss:  175.582 train_acc:  94.753 test_loss:  227.193 test_acc:  93.734\n",
      "epoch:  1855 train_loss:  175.13 train_acc:  94.757 test_loss:  221.082 test_acc:  93.89\n",
      "epoch:  1856 train_loss:  183.834 train_acc:  94.089 test_loss:  236.042 test_acc:  93.275\n",
      "epoch:  1857 train_loss:  176.068 train_acc:  94.135 test_loss:  210.072 test_acc:  93.656\n",
      "epoch:  1858 train_loss:  218.898 train_acc:  94.038 test_loss:  346.168 test_acc:  92.556\n",
      "epoch:  1859 train_loss:  173.325 train_acc:  94.649 test_loss:  236.405 test_acc:  93.9\n",
      "epoch:  1860 train_loss:  306.844 train_acc:  93.207 test_loss:  376.879 test_acc:  92.256\n",
      "epoch:  1861 train_loss:  169.797 train_acc:  94.776 test_loss:  233.735 test_acc:  93.67\n",
      "epoch:  1862 train_loss:  203.007 train_acc:  94.428 test_loss:  238.453 test_acc:  93.624\n",
      "epoch:  1863 train_loss:  166.666 train_acc:  94.839 test_loss:  208.616 test_acc:  93.821\n",
      "epoch:  1864 train_loss:  161.308 train_acc:  94.945 test_loss:  198.169 test_acc:  94.289\n",
      "max acc epoch：1864        max acc：94.289\n",
      "epoch:  1865 train_loss:  202.16 train_acc:  94.459 test_loss:  243.944 test_acc:  93.442\n",
      "epoch:  1866 train_loss:  176.05 train_acc:  94.264 test_loss:  203.469 test_acc:  93.68\n",
      "epoch:  1867 train_loss:  186.288 train_acc:  94.148 test_loss:  268.722 test_acc:  92.865\n",
      "epoch:  1868 train_loss:  183.834 train_acc:  94.473 test_loss:  223.143 test_acc:  93.768\n",
      "epoch:  1869 train_loss:  174.865 train_acc:  94.836 test_loss:  239.641 test_acc:  93.78\n",
      "epoch:  1870 train_loss:  181.972 train_acc:  94.45 test_loss:  265.354 test_acc:  93.276\n",
      "epoch:  1871 train_loss:  167.126 train_acc:  94.424 test_loss:  206.013 test_acc:  93.636\n",
      "epoch:  1872 train_loss:  193.959 train_acc:  94.203 test_loss:  222.296 test_acc:  93.32\n",
      "epoch:  1873 train_loss:  168.171 train_acc:  94.659 test_loss:  225.012 test_acc:  93.771\n",
      "epoch:  1874 train_loss:  161.177 train_acc:  94.8 test_loss:  202.08 test_acc:  94.018\n",
      "epoch:  1875 train_loss:  214.888 train_acc:  93.093 test_loss:  245.537 test_acc:  92.571\n",
      "epoch:  1876 train_loss:  170.06 train_acc:  94.482 test_loss:  221.368 test_acc:  93.689\n",
      "epoch:  1877 train_loss:  170.684 train_acc:  94.876 test_loss:  217.314 test_acc:  93.961\n",
      "epoch:  1878 train_loss:  178.997 train_acc:  94.685 test_loss:  243.073 test_acc:  93.608\n",
      "epoch:  1879 train_loss:  172.2 train_acc:  94.735 test_loss:  230.305 test_acc:  93.611\n",
      "epoch:  1880 train_loss:  166.039 train_acc:  94.7 test_loss:  212.158 test_acc:  94.148\n",
      "epoch:  1881 train_loss:  200.884 train_acc:  94.343 test_loss:  266.221 test_acc:  93.433\n",
      "epoch:  1882 train_loss:  164.907 train_acc:  94.856 test_loss:  214.115 test_acc:  93.82\n",
      "epoch:  1883 train_loss:  183.751 train_acc:  94.562 test_loss:  255.631 test_acc:  93.682\n",
      "epoch:  1884 train_loss:  224.621 train_acc:  94.109 test_loss:  284.859 test_acc:  93.107\n",
      "epoch:  1885 train_loss:  187.791 train_acc:  94.499 test_loss:  273.346 test_acc:  93.419\n",
      "epoch:  1886 train_loss:  232.255 train_acc:  92.93 test_loss:  305.575 test_acc:  92.385\n",
      "epoch:  1887 train_loss:  170.804 train_acc:  94.611 test_loss:  225.588 test_acc:  93.75\n",
      "epoch:  1888 train_loss:  157.208 train_acc:  94.929 test_loss:  190.946 test_acc:  94.115\n",
      "epoch:  1889 train_loss:  180.469 train_acc:  94.308 test_loss:  227.288 test_acc:  93.414\n",
      "epoch:  1890 train_loss:  169.091 train_acc:  94.904 test_loss:  201.395 test_acc:  93.968\n",
      "epoch:  1891 train_loss:  177.425 train_acc:  94.733 test_loss:  215.375 test_acc:  93.8\n",
      "epoch:  1892 train_loss:  189.979 train_acc:  94.66 test_loss:  241.595 test_acc:  93.699\n",
      "epoch:  1893 train_loss:  180.053 train_acc:  94.615 test_loss:  209.629 test_acc:  93.765\n",
      "epoch:  1894 train_loss:  162.842 train_acc:  94.722 test_loss:  221.83 test_acc:  93.604\n",
      "epoch:  1895 train_loss:  190.935 train_acc:  93.541 test_loss:  222.511 test_acc:  93.204\n",
      "epoch:  1896 train_loss:  157.885 train_acc:  94.838 test_loss:  202.753 test_acc:  94.078\n",
      "epoch:  1897 train_loss:  179.687 train_acc:  94.031 test_loss:  211.614 test_acc:  93.529\n",
      "epoch:  1898 train_loss:  164.309 train_acc:  94.806 test_loss:  197.991 test_acc:  94.125\n",
      "epoch:  1899 train_loss:  234.683 train_acc:  94.097 test_loss:  313.653 test_acc:  92.952\n",
      "epoch:  1900 train_loss:  169.517 train_acc:  94.63 test_loss:  231.29 test_acc:  93.597\n",
      "epoch:  1901 train_loss:  168.695 train_acc:  94.883 test_loss:  226.723 test_acc:  94.071\n",
      "epoch:  1902 train_loss:  187.761 train_acc:  94.631 test_loss:  188.77 test_acc:  94.201\n",
      "epoch:  1903 train_loss:  175.801 train_acc:  94.487 test_loss:  214.447 test_acc:  93.956\n",
      "epoch:  1904 train_loss:  206.788 train_acc:  94.272 test_loss:  267.208 test_acc:  93.204\n",
      "epoch:  1905 train_loss:  280.978 train_acc:  91.92 test_loss:  362.359 test_acc:  91.417\n",
      "epoch:  1906 train_loss:  165.021 train_acc:  94.582 test_loss:  207.608 test_acc:  93.856\n",
      "epoch:  1907 train_loss:  207.228 train_acc:  94.286 test_loss:  264.272 test_acc:  93.409\n",
      "epoch:  1908 train_loss:  194.973 train_acc:  94.302 test_loss:  261.711 test_acc:  93.35\n",
      "epoch:  1909 train_loss:  177.553 train_acc:  94.618 test_loss:  229.2 test_acc:  93.655\n",
      "epoch:  1910 train_loss:  166.518 train_acc:  94.81 test_loss:  218.078 test_acc:  93.952\n",
      "epoch:  1911 train_loss:  170.627 train_acc:  94.464 test_loss:  231.421 test_acc:  93.636\n",
      "epoch:  1912 train_loss:  162.677 train_acc:  94.818 test_loss:  194.844 test_acc:  93.949\n",
      "epoch:  1913 train_loss:  176.129 train_acc:  94.715 test_loss:  231.407 test_acc:  93.729\n",
      "epoch:  1914 train_loss:  178.557 train_acc:  94.196 test_loss:  198.109 test_acc:  93.928\n",
      "epoch:  1915 train_loss:  166.379 train_acc:  94.706 test_loss:  209.779 test_acc:  93.583\n",
      "epoch:  1916 train_loss:  183.398 train_acc:  94.617 test_loss:  233.191 test_acc:  93.907\n",
      "epoch:  1917 train_loss:  170.915 train_acc:  94.555 test_loss:  203.338 test_acc:  93.811\n",
      "epoch:  1918 train_loss:  181.003 train_acc:  94.585 test_loss:  227.644 test_acc:  93.82\n",
      "epoch:  1919 train_loss:  182.482 train_acc:  94.236 test_loss:  224.45 test_acc:  93.536\n",
      "epoch:  1920 train_loss:  188.19 train_acc:  94.644 test_loss:  276.585 test_acc:  93.686\n",
      "epoch:  1921 train_loss:  163.494 train_acc:  94.853 test_loss:  201.086 test_acc:  93.972\n",
      "epoch:  1922 train_loss:  194.49 train_acc:  94.552 test_loss:  254.844 test_acc:  93.571\n",
      "epoch:  1923 train_loss:  161.534 train_acc:  94.772 test_loss:  193.953 test_acc:  94.068\n",
      "epoch:  1924 train_loss:  166.896 train_acc:  94.737 test_loss:  230.789 test_acc:  93.612\n",
      "epoch:  1925 train_loss:  179.614 train_acc:  94.704 test_loss:  205.933 test_acc:  93.945\n",
      "epoch:  1926 train_loss:  163.656 train_acc:  94.895 test_loss:  209.377 test_acc:  94.048\n",
      "epoch:  1927 train_loss:  187.43 train_acc:  94.008 test_loss:  222.76 test_acc:  93.626\n",
      "epoch:  1928 train_loss:  177.542 train_acc:  94.387 test_loss:  227.819 test_acc:  93.59\n",
      "epoch:  1929 train_loss:  175.82 train_acc:  94.675 test_loss:  247.102 test_acc:  93.748\n",
      "epoch:  1930 train_loss:  159.934 train_acc:  94.938 test_loss:  204.96 test_acc:  93.854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1931 train_loss:  160.466 train_acc:  94.95 test_loss:  192.569 test_acc:  94.134\n",
      "epoch:  1932 train_loss:  221.572 train_acc:  94.136 test_loss:  272.705 test_acc:  93.463\n",
      "epoch:  1933 train_loss:  163.596 train_acc:  94.635 test_loss:  190.082 test_acc:  94.087\n",
      "epoch:  1934 train_loss:  186.68 train_acc:  94.113 test_loss:  226.152 test_acc:  93.412\n",
      "epoch:  1935 train_loss:  160.864 train_acc:  94.905 test_loss:  199.755 test_acc:  94.405\n",
      "max acc epoch：1935        max acc：94.405\n",
      "epoch:  1936 train_loss:  176.245 train_acc:  94.599 test_loss:  242.871 test_acc:  93.803\n",
      "epoch:  1937 train_loss:  215.709 train_acc:  93.13 test_loss:  246.247 test_acc:  92.893\n",
      "epoch:  1938 train_loss:  203.957 train_acc:  94.468 test_loss:  244.95 test_acc:  93.642\n",
      "epoch:  1939 train_loss:  165.506 train_acc:  94.791 test_loss:  220.934 test_acc:  93.97\n",
      "epoch:  1940 train_loss:  165.759 train_acc:  94.806 test_loss:  193.287 test_acc:  94.105\n",
      "epoch:  1941 train_loss:  190.574 train_acc:  94.514 test_loss:  242.496 test_acc:  93.97\n",
      "epoch:  1942 train_loss:  166.552 train_acc:  94.794 test_loss:  208.829 test_acc:  94.088\n",
      "epoch:  1943 train_loss:  156.322 train_acc:  94.947 test_loss:  191.73 test_acc:  94.145\n",
      "epoch:  1944 train_loss:  165.501 train_acc:  94.639 test_loss:  199.016 test_acc:  94.055\n",
      "epoch:  1945 train_loss:  225.465 train_acc:  93.185 test_loss:  270.715 test_acc:  92.799\n",
      "epoch:  1946 train_loss:  165.23 train_acc:  94.929 test_loss:  186.74 test_acc:  94.438\n",
      "max acc epoch：1946        max acc：94.438\n",
      "epoch:  1947 train_loss:  158.25 train_acc:  94.898 test_loss:  190.028 test_acc:  94.258\n",
      "epoch:  1948 train_loss:  167.266 train_acc:  94.818 test_loss:  205.25 test_acc:  94.093\n",
      "epoch:  1949 train_loss:  220.602 train_acc:  93.099 test_loss:  266.45 test_acc:  92.724\n",
      "epoch:  1950 train_loss:  198.797 train_acc:  94.443 test_loss:  245.04 test_acc:  93.431\n",
      "epoch:  1951 train_loss:  155.229 train_acc:  95.044 test_loss:  192.922 test_acc:  94.178\n",
      "epoch:  1952 train_loss:  189.386 train_acc:  94.269 test_loss:  250.819 test_acc:  93.557\n",
      "epoch:  1953 train_loss:  174.51 train_acc:  94.731 test_loss:  194.397 test_acc:  93.816\n",
      "epoch:  1954 train_loss:  182.394 train_acc:  94.539 test_loss:  233.01 test_acc:  93.678\n",
      "epoch:  1955 train_loss:  172.907 train_acc:  94.268 test_loss:  199.637 test_acc:  93.673\n",
      "epoch:  1956 train_loss:  163.129 train_acc:  94.705 test_loss:  198.993 test_acc:  93.919\n",
      "epoch:  1957 train_loss:  275.965 train_acc:  93.338 test_loss:  315.679 test_acc:  92.941\n",
      "epoch:  1958 train_loss:  173.172 train_acc:  94.516 test_loss:  204.842 test_acc:  93.885\n",
      "epoch:  1959 train_loss:  269.2 train_acc:  92.305 test_loss:  307.212 test_acc:  92.16\n",
      "epoch:  1960 train_loss:  274.697 train_acc:  93.608 test_loss:  338.645 test_acc:  93.081\n",
      "epoch:  1961 train_loss:  156.757 train_acc:  94.807 test_loss:  201.285 test_acc:  93.956\n",
      "epoch:  1962 train_loss:  177.646 train_acc:  94.159 test_loss:  213.2 test_acc:  93.909\n",
      "epoch:  1963 train_loss:  168.087 train_acc:  94.753 test_loss:  208.664 test_acc:  93.637\n",
      "epoch:  1964 train_loss:  172.5 train_acc:  94.444 test_loss:  201.829 test_acc:  94.019\n",
      "epoch:  1965 train_loss:  228.717 train_acc:  93.329 test_loss:  235.473 test_acc:  93.337\n",
      "epoch:  1966 train_loss:  167.902 train_acc:  94.625 test_loss:  193.757 test_acc:  93.94\n",
      "epoch:  1967 train_loss:  178.688 train_acc:  94.503 test_loss:  221.889 test_acc:  94.006\n",
      "epoch:  1968 train_loss:  163.519 train_acc:  94.885 test_loss:  213.299 test_acc:  94.143\n",
      "epoch:  1969 train_loss:  185.732 train_acc:  94.081 test_loss:  238.947 test_acc:  93.462\n",
      "epoch:  1970 train_loss:  198.247 train_acc:  94.021 test_loss:  236.914 test_acc:  93.578\n",
      "epoch:  1971 train_loss:  185.889 train_acc:  94.633 test_loss:  225.967 test_acc:  93.626\n",
      "epoch:  1972 train_loss:  181.997 train_acc:  94.72 test_loss:  262.691 test_acc:  93.968\n",
      "epoch:  1973 train_loss:  174.594 train_acc:  94.64 test_loss:  226.143 test_acc:  93.498\n",
      "epoch:  1974 train_loss:  183.909 train_acc:  94.258 test_loss:  236.514 test_acc:  93.31\n",
      "epoch:  1975 train_loss:  203.757 train_acc:  94.335 test_loss:  276.773 test_acc:  93.086\n",
      "epoch:  1976 train_loss:  162.52 train_acc:  94.797 test_loss:  200.68 test_acc:  94.088\n",
      "epoch:  1977 train_loss:  162.165 train_acc:  94.871 test_loss:  194.942 test_acc:  94.243\n",
      "epoch:  1978 train_loss:  156.702 train_acc:  94.842 test_loss:  194.357 test_acc:  94.198\n",
      "epoch:  1979 train_loss:  162.439 train_acc:  94.857 test_loss:  216.191 test_acc:  93.824\n",
      "epoch:  1980 train_loss:  225.665 train_acc:  94.029 test_loss:  286.545 test_acc:  93.256\n",
      "epoch:  1981 train_loss:  185.646 train_acc:  93.91 test_loss:  252.127 test_acc:  93.297\n",
      "epoch:  1982 train_loss:  176.171 train_acc:  94.725 test_loss:  212.794 test_acc:  94.01\n",
      "epoch:  1983 train_loss:  213.862 train_acc:  93.435 test_loss:  254.614 test_acc:  92.929\n",
      "epoch:  1984 train_loss:  192.085 train_acc:  93.991 test_loss:  221.968 test_acc:  93.547\n",
      "epoch:  1985 train_loss:  171.344 train_acc:  94.347 test_loss:  200.597 test_acc:  93.544\n",
      "epoch:  1986 train_loss:  168.331 train_acc:  94.634 test_loss:  214.23 test_acc:  94.213\n",
      "epoch:  1987 train_loss:  171.805 train_acc:  94.599 test_loss:  223.508 test_acc:  93.763\n",
      "epoch:  1988 train_loss:  179.467 train_acc:  93.928 test_loss:  211.318 test_acc:  93.571\n",
      "epoch:  1989 train_loss:  158.595 train_acc:  94.923 test_loss:  201.544 test_acc:  94.157\n",
      "epoch:  1990 train_loss:  251.322 train_acc:  92.845 test_loss:  287.218 test_acc:  92.416\n",
      "epoch:  1991 train_loss:  164.595 train_acc:  94.783 test_loss:  202.556 test_acc:  94.054\n",
      "epoch:  1992 train_loss:  183.004 train_acc:  94.539 test_loss:  211.948 test_acc:  93.744\n",
      "epoch:  1993 train_loss:  167.543 train_acc:  94.87 test_loss:  179.701 test_acc:  94.502\n",
      "min loss:1993\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(224.9865)]\n",
      "1    [tensor(88.8594)]   [tensor(79.9275)]\n",
      "2   [tensor(162.9521)]  [tensor(156.5323)]\n",
      "3   [tensor(185.9385)]  [tensor(205.2385)]\n",
      "4   [tensor(212.8320)]  [tensor(209.3705)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(248.1011)]\n",
      "61  [tensor(127.2646)]  [tensor(128.3323)]\n",
      "62   [tensor(84.7217)]   [tensor(81.5663)]\n",
      "63  [tensor(104.8945)]  [tensor(120.8723)]\n",
      "64  [tensor(156.6016)]  [tensor(163.3623)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "max acc epoch：1993        max acc：94.502\n",
      "epoch:  1994 train_loss:  214.681 train_acc:  93.047 test_loss:  250.195 test_acc:  92.426\n",
      "epoch:  1995 train_loss:  164.552 train_acc:  94.681 test_loss:  200.635 test_acc:  94.302\n",
      "epoch:  1996 train_loss:  192.301 train_acc:  94.421 test_loss:  271.559 test_acc:  93.332\n",
      "epoch:  1997 train_loss:  164.3 train_acc:  94.902 test_loss:  185.951 test_acc:  93.984\n",
      "epoch:  1998 train_loss:  163.867 train_acc:  94.741 test_loss:  219.775 test_acc:  94.114\n",
      "epoch:  1999 train_loss:  167.085 train_acc:  94.424 test_loss:  219.842 test_acc:  93.588\n",
      "epoch:  2000 train_loss:  171.191 train_acc:  94.556 test_loss:  189.102 test_acc:  93.931\n",
      "epoch:  2001 train_loss:  156.596 train_acc:  94.94 test_loss:  186.291 test_acc:  94.229\n",
      "epoch:  2002 train_loss:  194.767 train_acc:  94.2 test_loss:  229.36 test_acc:  93.65\n",
      "epoch:  2003 train_loss:  197.985 train_acc:  93.453 test_loss:  214.713 test_acc:  93.416\n",
      "epoch:  2004 train_loss:  184.012 train_acc:  94.666 test_loss:  228.342 test_acc:  93.757\n",
      "epoch:  2005 train_loss:  248.171 train_acc:  92.814 test_loss:  287.658 test_acc:  92.266\n",
      "epoch:  2006 train_loss:  159.048 train_acc:  94.906 test_loss:  197.974 test_acc:  94.336\n",
      "epoch:  2007 train_loss:  170.497 train_acc:  94.558 test_loss:  220.58 test_acc:  93.718\n",
      "epoch:  2008 train_loss:  164.063 train_acc:  94.522 test_loss:  197.164 test_acc:  93.816\n",
      "epoch:  2009 train_loss:  193.772 train_acc:  94.157 test_loss:  246.976 test_acc:  93.566\n",
      "epoch:  2010 train_loss:  185.004 train_acc:  94.084 test_loss:  225.112 test_acc:  93.415\n",
      "epoch:  2011 train_loss:  207.554 train_acc:  94.494 test_loss:  310.884 test_acc:  93.543\n",
      "epoch:  2012 train_loss:  174.641 train_acc:  94.655 test_loss:  229.515 test_acc:  93.585\n",
      "epoch:  2013 train_loss:  171.99 train_acc:  94.698 test_loss:  224.629 test_acc:  94.079\n",
      "epoch:  2014 train_loss:  178.186 train_acc:  94.241 test_loss:  208.843 test_acc:  93.482\n",
      "epoch:  2015 train_loss:  198.28 train_acc:  93.91 test_loss:  271.628 test_acc:  93.078\n",
      "epoch:  2016 train_loss:  173.878 train_acc:  94.672 test_loss:  226.494 test_acc:  93.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2017 train_loss:  161.252 train_acc:  94.74 test_loss:  201.945 test_acc:  93.897\n",
      "epoch:  2018 train_loss:  165.568 train_acc:  94.86 test_loss:  228.683 test_acc:  93.804\n",
      "epoch:  2019 train_loss:  177.987 train_acc:  94.679 test_loss:  220.809 test_acc:  93.772\n",
      "epoch:  2020 train_loss:  158.657 train_acc:  94.889 test_loss:  207.45 test_acc:  94.198\n",
      "epoch:  2021 train_loss:  158.825 train_acc:  95.027 test_loss:  184.972 test_acc:  94.232\n",
      "epoch:  2022 train_loss:  175.897 train_acc:  94.062 test_loss:  201.602 test_acc:  93.663\n",
      "epoch:  2023 train_loss:  165.821 train_acc:  94.869 test_loss:  191.997 test_acc:  94.147\n",
      "epoch:  2024 train_loss:  165.203 train_acc:  94.431 test_loss:  197.713 test_acc:  93.805\n",
      "epoch:  2025 train_loss:  162.665 train_acc:  94.674 test_loss:  198.717 test_acc:  93.915\n",
      "epoch:  2026 train_loss:  159.602 train_acc:  94.875 test_loss:  195.281 test_acc:  94.273\n",
      "epoch:  2027 train_loss:  205.445 train_acc:  94.394 test_loss:  233.703 test_acc:  93.748\n",
      "epoch:  2028 train_loss:  163.189 train_acc:  94.77 test_loss:  182.918 test_acc:  94.159\n",
      "epoch:  2029 train_loss:  163.112 train_acc:  94.891 test_loss:  196.906 test_acc:  94.105\n",
      "epoch:  2030 train_loss:  209.253 train_acc:  93.181 test_loss:  258.456 test_acc:  92.657\n",
      "epoch:  2031 train_loss:  167.622 train_acc:  94.597 test_loss:  235.972 test_acc:  93.592\n",
      "epoch:  2032 train_loss:  167.954 train_acc:  94.356 test_loss:  203.609 test_acc:  93.79\n",
      "epoch:  2033 train_loss:  209.791 train_acc:  94.323 test_loss:  237.828 test_acc:  93.621\n",
      "epoch:  2034 train_loss:  163.733 train_acc:  94.973 test_loss:  215.997 test_acc:  94.193\n",
      "epoch:  2035 train_loss:  159.39 train_acc:  94.726 test_loss:  196.192 test_acc:  93.989\n",
      "epoch:  2036 train_loss:  166.791 train_acc:  94.46 test_loss:  220.976 test_acc:  93.66\n",
      "epoch:  2037 train_loss:  166.805 train_acc:  94.454 test_loss:  203.065 test_acc:  93.587\n",
      "epoch:  2038 train_loss:  160.027 train_acc:  94.682 test_loss:  197.099 test_acc:  94.011\n",
      "epoch:  2039 train_loss:  154.035 train_acc:  95.033 test_loss:  187.62 test_acc:  94.353\n",
      "epoch:  2040 train_loss:  175.072 train_acc:  94.381 test_loss:  225.485 test_acc:  93.513\n",
      "epoch:  2041 train_loss:  166.712 train_acc:  94.874 test_loss:  217.995 test_acc:  94.26\n",
      "epoch:  2042 train_loss:  156.446 train_acc:  94.901 test_loss:  188.536 test_acc:  94.279\n",
      "epoch:  2043 train_loss:  162.378 train_acc:  94.759 test_loss:  191.736 test_acc:  94.204\n",
      "epoch:  2044 train_loss:  175.959 train_acc:  94.292 test_loss:  219.689 test_acc:  93.563\n",
      "epoch:  2045 train_loss:  190.076 train_acc:  94.395 test_loss:  213.994 test_acc:  94.028\n",
      "epoch:  2046 train_loss:  171.522 train_acc:  94.333 test_loss:  228.686 test_acc:  93.149\n",
      "epoch:  2047 train_loss:  183.022 train_acc:  94.648 test_loss:  247.052 test_acc:  93.866\n",
      "epoch:  2048 train_loss:  156.965 train_acc:  94.872 test_loss:  196.794 test_acc:  93.992\n",
      "epoch:  2049 train_loss:  180.711 train_acc:  94.362 test_loss:  225.851 test_acc:  93.469\n",
      "epoch:  2050 train_loss:  159.86 train_acc:  94.91 test_loss:  195.959 test_acc:  94.443\n",
      "epoch:  2051 train_loss:  155.545 train_acc:  94.839 test_loss:  182.339 test_acc:  94.301\n",
      "epoch:  2052 train_loss:  200.618 train_acc:  94.183 test_loss:  241.221 test_acc:  93.522\n",
      "epoch:  2053 train_loss:  177.07 train_acc:  94.643 test_loss:  253.116 test_acc:  93.839\n",
      "epoch:  2054 train_loss:  167.777 train_acc:  94.601 test_loss:  200.765 test_acc:  94.166\n",
      "epoch:  2055 train_loss:  184.074 train_acc:  94.232 test_loss:  251.799 test_acc:  93.142\n",
      "epoch:  2056 train_loss:  197.039 train_acc:  94.189 test_loss:  258.443 test_acc:  93.283\n",
      "epoch:  2057 train_loss:  158.063 train_acc:  94.713 test_loss:  202.816 test_acc:  93.831\n",
      "epoch:  2058 train_loss:  200.815 train_acc:  93.559 test_loss:  206.44 test_acc:  93.398\n",
      "epoch:  2059 train_loss:  186.067 train_acc:  94.475 test_loss:  223.298 test_acc:  93.858\n",
      "epoch:  2060 train_loss:  168.842 train_acc:  94.903 test_loss:  240.305 test_acc:  94.091\n",
      "epoch:  2061 train_loss:  209.638 train_acc:  93.933 test_loss:  255.515 test_acc:  93.395\n",
      "epoch:  2062 train_loss:  175.038 train_acc:  94.848 test_loss:  248.687 test_acc:  93.95\n",
      "epoch:  2063 train_loss:  193.504 train_acc:  94.553 test_loss:  228.525 test_acc:  93.669\n",
      "epoch:  2064 train_loss:  282.717 train_acc:  91.939 test_loss:  319.965 test_acc:  91.695\n",
      "epoch:  2065 train_loss:  259.622 train_acc:  93.627 test_loss:  296.001 test_acc:  92.93\n",
      "epoch:  2066 train_loss:  174.55 train_acc:  94.467 test_loss:  237.013 test_acc:  93.813\n",
      "epoch:  2067 train_loss:  193.829 train_acc:  94.539 test_loss:  289.727 test_acc:  93.492\n",
      "epoch:  2068 train_loss:  159.096 train_acc:  95.006 test_loss:  213.216 test_acc:  94.034\n",
      "epoch:  2069 train_loss:  178.867 train_acc:  94.461 test_loss:  257.318 test_acc:  93.639\n",
      "epoch:  2070 train_loss:  169.372 train_acc:  94.879 test_loss:  209.25 test_acc:  93.898\n",
      "epoch:  2071 train_loss:  164.186 train_acc:  94.846 test_loss:  210.568 test_acc:  94.104\n",
      "epoch:  2072 train_loss:  166.625 train_acc:  94.487 test_loss:  213.088 test_acc:  93.658\n",
      "epoch:  2073 train_loss:  162.476 train_acc:  94.74 test_loss:  203.834 test_acc:  93.981\n",
      "epoch:  2074 train_loss:  151.746 train_acc:  95.084 test_loss:  179.926 test_acc:  94.416\n",
      "epoch:  2075 train_loss:  202.125 train_acc:  93.793 test_loss:  259.494 test_acc:  93.208\n",
      "epoch:  2076 train_loss:  217.367 train_acc:  94.317 test_loss:  266.432 test_acc:  93.154\n",
      "epoch:  2077 train_loss:  158.164 train_acc:  94.821 test_loss:  210.436 test_acc:  94.139\n",
      "epoch:  2078 train_loss:  179.497 train_acc:  94.485 test_loss:  199.303 test_acc:  93.967\n",
      "epoch:  2079 train_loss:  192.732 train_acc:  94.538 test_loss:  215.18 test_acc:  93.658\n",
      "epoch:  2080 train_loss:  154.877 train_acc:  94.915 test_loss:  191.197 test_acc:  94.17\n",
      "epoch:  2081 train_loss:  169.615 train_acc:  94.824 test_loss:  201.328 test_acc:  94.047\n",
      "epoch:  2082 train_loss:  206.061 train_acc:  94.246 test_loss:  259.562 test_acc:  93.501\n",
      "epoch:  2083 train_loss:  166.183 train_acc:  94.248 test_loss:  194.42 test_acc:  93.759\n",
      "epoch:  2084 train_loss:  205.691 train_acc:  94.338 test_loss:  301.853 test_acc:  93.3\n",
      "epoch:  2085 train_loss:  162.451 train_acc:  94.527 test_loss:  202.72 test_acc:  94.024\n",
      "epoch:  2086 train_loss:  193.268 train_acc:  94.029 test_loss:  270.778 test_acc:  92.877\n",
      "epoch:  2087 train_loss:  259.319 train_acc:  93.431 test_loss:  318.89 test_acc:  93.0\n",
      "epoch:  2088 train_loss:  154.12 train_acc:  94.809 test_loss:  192.857 test_acc:  93.983\n",
      "epoch:  2089 train_loss:  175.832 train_acc:  94.707 test_loss:  215.487 test_acc:  93.854\n",
      "epoch:  2090 train_loss:  216.003 train_acc:  94.358 test_loss:  279.277 test_acc:  93.424\n",
      "epoch:  2091 train_loss:  157.101 train_acc:  94.993 test_loss:  205.412 test_acc:  94.094\n",
      "epoch:  2092 train_loss:  166.591 train_acc:  94.657 test_loss:  200.821 test_acc:  93.935\n",
      "epoch:  2093 train_loss:  183.285 train_acc:  94.558 test_loss:  190.103 test_acc:  94.001\n",
      "epoch:  2094 train_loss:  163.694 train_acc:  94.65 test_loss:  216.627 test_acc:  93.75\n",
      "epoch:  2095 train_loss:  157.037 train_acc:  94.948 test_loss:  198.39 test_acc:  94.061\n",
      "epoch:  2096 train_loss:  176.254 train_acc:  94.791 test_loss:  206.124 test_acc:  94.014\n",
      "epoch:  2097 train_loss:  153.386 train_acc:  94.996 test_loss:  193.273 test_acc:  94.151\n",
      "epoch:  2098 train_loss:  175.992 train_acc:  94.222 test_loss:  209.701 test_acc:  93.573\n",
      "epoch:  2099 train_loss:  169.052 train_acc:  94.412 test_loss:  214.379 test_acc:  93.722\n",
      "epoch:  2100 train_loss:  203.686 train_acc:  93.86 test_loss:  231.975 test_acc:  93.48\n",
      "epoch:  2101 train_loss:  190.843 train_acc:  94.306 test_loss:  206.3 test_acc:  93.893\n",
      "epoch:  2102 train_loss:  179.508 train_acc:  94.346 test_loss:  232.758 test_acc:  93.651\n",
      "epoch:  2103 train_loss:  161.237 train_acc:  94.695 test_loss:  194.184 test_acc:  94.036\n",
      "epoch:  2104 train_loss:  166.655 train_acc:  94.888 test_loss:  201.798 test_acc:  94.341\n",
      "epoch:  2105 train_loss:  174.223 train_acc:  94.711 test_loss:  224.304 test_acc:  93.931\n",
      "epoch:  2106 train_loss:  159.604 train_acc:  94.928 test_loss:  184.68 test_acc:  94.177\n",
      "epoch:  2107 train_loss:  162.032 train_acc:  94.684 test_loss:  191.047 test_acc:  94.018\n",
      "epoch:  2108 train_loss:  163.22 train_acc:  94.732 test_loss:  191.527 test_acc:  94.314\n",
      "epoch:  2109 train_loss:  223.318 train_acc:  93.157 test_loss:  230.144 test_acc:  92.939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2110 train_loss:  217.037 train_acc:  92.968 test_loss:  222.434 test_acc:  92.895\n",
      "epoch:  2111 train_loss:  177.209 train_acc:  94.782 test_loss:  215.972 test_acc:  93.927\n",
      "epoch:  2112 train_loss:  163.408 train_acc:  94.933 test_loss:  202.59 test_acc:  94.145\n",
      "epoch:  2113 train_loss:  155.382 train_acc:  94.959 test_loss:  179.728 test_acc:  94.236\n",
      "epoch:  2114 train_loss:  153.287 train_acc:  95.046 test_loss:  191.92 test_acc:  94.44\n",
      "epoch:  2115 train_loss:  183.305 train_acc:  94.216 test_loss:  248.27 test_acc:  93.558\n",
      "epoch:  2116 train_loss:  154.893 train_acc:  94.812 test_loss:  195.128 test_acc:  93.901\n",
      "epoch:  2117 train_loss:  177.727 train_acc:  94.243 test_loss:  226.101 test_acc:  93.791\n",
      "epoch:  2118 train_loss:  185.368 train_acc:  94.681 test_loss:  213.199 test_acc:  93.958\n",
      "epoch:  2119 train_loss:  173.529 train_acc:  94.431 test_loss:  204.599 test_acc:  93.742\n",
      "epoch:  2120 train_loss:  156.623 train_acc:  94.608 test_loss:  191.91 test_acc:  93.925\n",
      "epoch:  2121 train_loss:  191.552 train_acc:  93.739 test_loss:  206.285 test_acc:  93.343\n",
      "epoch:  2122 train_loss:  154.222 train_acc:  94.974 test_loss:  190.097 test_acc:  94.378\n",
      "epoch:  2123 train_loss:  166.614 train_acc:  94.593 test_loss:  225.256 test_acc:  93.939\n",
      "epoch:  2124 train_loss:  183.385 train_acc:  94.45 test_loss:  219.232 test_acc:  93.942\n",
      "epoch:  2125 train_loss:  156.029 train_acc:  94.951 test_loss:  198.593 test_acc:  94.271\n",
      "epoch:  2126 train_loss:  178.887 train_acc:  94.341 test_loss:  204.863 test_acc:  93.934\n",
      "epoch:  2127 train_loss:  175.299 train_acc:  94.592 test_loss:  200.459 test_acc:  93.757\n",
      "epoch:  2128 train_loss:  160.943 train_acc:  94.919 test_loss:  211.965 test_acc:  94.058\n",
      "epoch:  2129 train_loss:  170.945 train_acc:  94.199 test_loss:  212.349 test_acc:  93.391\n",
      "epoch:  2130 train_loss:  188.21 train_acc:  93.758 test_loss:  227.375 test_acc:  93.254\n",
      "epoch:  2131 train_loss:  155.362 train_acc:  95.062 test_loss:  193.468 test_acc:  94.399\n",
      "epoch:  2132 train_loss:  180.217 train_acc:  94.304 test_loss:  232.954 test_acc:  93.512\n",
      "epoch:  2133 train_loss:  189.009 train_acc:  94.567 test_loss:  249.184 test_acc:  93.597\n",
      "epoch:  2134 train_loss:  156.69 train_acc:  94.885 test_loss:  202.816 test_acc:  94.073\n",
      "epoch:  2135 train_loss:  167.407 train_acc:  94.766 test_loss:  179.07 test_acc:  94.246\n",
      "min loss:2135\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(226.6132)]\n",
      "1    [tensor(88.8594)]   [tensor(79.9694)]\n",
      "2   [tensor(162.9521)]  [tensor(156.5943)]\n",
      "3   [tensor(185.9385)]  [tensor(207.7654)]\n",
      "4   [tensor(212.8320)]  [tensor(205.9541)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(250.3098)]\n",
      "61  [tensor(127.2646)]  [tensor(129.4275)]\n",
      "62   [tensor(84.7217)]   [tensor(81.3813)]\n",
      "63  [tensor(104.8945)]  [tensor(121.7464)]\n",
      "64  [tensor(156.6016)]  [tensor(163.7319)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  2136 train_loss:  168.434 train_acc:  94.166 test_loss:  197.341 test_acc:  93.57\n",
      "epoch:  2137 train_loss:  153.892 train_acc:  94.877 test_loss:  185.661 test_acc:  94.097\n",
      "epoch:  2138 train_loss:  160.687 train_acc:  94.719 test_loss:  199.576 test_acc:  93.862\n",
      "epoch:  2139 train_loss:  154.621 train_acc:  94.992 test_loss:  180.469 test_acc:  94.656\n",
      "max acc epoch：2139        max acc：94.656\n",
      "epoch:  2140 train_loss:  193.541 train_acc:  93.955 test_loss:  231.397 test_acc:  93.298\n",
      "epoch:  2141 train_loss:  188.83 train_acc:  94.517 test_loss:  262.05 test_acc:  93.682\n",
      "epoch:  2142 train_loss:  199.439 train_acc:  93.755 test_loss:  259.578 test_acc:  92.787\n",
      "epoch:  2143 train_loss:  155.613 train_acc:  94.683 test_loss:  183.27 test_acc:  94.183\n",
      "epoch:  2144 train_loss:  167.139 train_acc:  94.668 test_loss:  205.849 test_acc:  94.01\n",
      "epoch:  2145 train_loss:  231.85 train_acc:  94.004 test_loss:  319.801 test_acc:  92.765\n",
      "epoch:  2146 train_loss:  160.416 train_acc:  94.526 test_loss:  182.485 test_acc:  94.086\n",
      "epoch:  2147 train_loss:  155.822 train_acc:  94.939 test_loss:  193.148 test_acc:  94.263\n",
      "epoch:  2148 train_loss:  188.827 train_acc:  94.412 test_loss:  223.306 test_acc:  93.762\n",
      "epoch:  2149 train_loss:  178.122 train_acc:  94.411 test_loss:  226.29 test_acc:  93.564\n",
      "epoch:  2150 train_loss:  183.718 train_acc:  94.637 test_loss:  220.498 test_acc:  93.697\n",
      "epoch:  2151 train_loss:  152.773 train_acc:  95.034 test_loss:  188.8 test_acc:  94.373\n",
      "epoch:  2152 train_loss:  165.883 train_acc:  94.866 test_loss:  183.679 test_acc:  94.22\n",
      "epoch:  2153 train_loss:  172.223 train_acc:  94.726 test_loss:  195.567 test_acc:  94.192\n",
      "epoch:  2154 train_loss:  156.164 train_acc:  94.94 test_loss:  211.941 test_acc:  94.089\n",
      "epoch:  2155 train_loss:  156.428 train_acc:  95.024 test_loss:  191.605 test_acc:  94.361\n",
      "epoch:  2156 train_loss:  151.177 train_acc:  95.079 test_loss:  181.653 test_acc:  94.518\n",
      "epoch:  2157 train_loss:  190.455 train_acc:  94.351 test_loss:  262.342 test_acc:  93.768\n",
      "epoch:  2158 train_loss:  186.037 train_acc:  94.226 test_loss:  248.683 test_acc:  93.309\n",
      "epoch:  2159 train_loss:  179.194 train_acc:  94.574 test_loss:  236.132 test_acc:  93.618\n",
      "epoch:  2160 train_loss:  163.642 train_acc:  94.795 test_loss:  223.527 test_acc:  93.886\n",
      "epoch:  2161 train_loss:  167.16 train_acc:  94.586 test_loss:  186.838 test_acc:  93.782\n",
      "epoch:  2162 train_loss:  152.352 train_acc:  95.015 test_loss:  199.001 test_acc:  94.457\n",
      "epoch:  2163 train_loss:  154.317 train_acc:  95.078 test_loss:  208.966 test_acc:  94.057\n",
      "epoch:  2164 train_loss:  190.636 train_acc:  94.735 test_loss:  238.206 test_acc:  94.071\n",
      "epoch:  2165 train_loss:  149.8 train_acc:  95.076 test_loss:  175.295 test_acc:  94.585\n",
      "min loss:2165\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(230.2258)]\n",
      "1    [tensor(88.8594)]   [tensor(81.8835)]\n",
      "2   [tensor(162.9521)]  [tensor(162.8062)]\n",
      "3   [tensor(185.9385)]  [tensor(202.1286)]\n",
      "4   [tensor(212.8320)]  [tensor(213.1568)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(251.7394)]\n",
      "61  [tensor(127.2646)]  [tensor(127.3882)]\n",
      "62   [tensor(84.7217)]   [tensor(83.4706)]\n",
      "63  [tensor(104.8945)]  [tensor(127.4735)]\n",
      "64  [tensor(156.6016)]  [tensor(169.6291)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  2166 train_loss:  174.07 train_acc:  94.377 test_loss:  217.109 test_acc:  93.824\n",
      "epoch:  2167 train_loss:  260.212 train_acc:  93.178 test_loss:  330.591 test_acc:  92.486\n",
      "epoch:  2168 train_loss:  154.177 train_acc:  94.825 test_loss:  184.988 test_acc:  94.047\n",
      "epoch:  2169 train_loss:  207.747 train_acc:  94.318 test_loss:  255.288 test_acc:  93.66\n",
      "epoch:  2170 train_loss:  166.41 train_acc:  94.797 test_loss:  217.651 test_acc:  93.986\n",
      "epoch:  2171 train_loss:  192.321 train_acc:  94.4 test_loss:  259.012 test_acc:  93.857\n",
      "epoch:  2172 train_loss:  168.922 train_acc:  94.839 test_loss:  229.893 test_acc:  93.69\n",
      "epoch:  2173 train_loss:  166.47 train_acc:  94.52 test_loss:  183.713 test_acc:  94.053\n",
      "epoch:  2174 train_loss:  196.468 train_acc:  94.304 test_loss:  242.761 test_acc:  93.752\n",
      "epoch:  2175 train_loss:  164.6 train_acc:  94.72 test_loss:  225.67 test_acc:  93.869\n",
      "epoch:  2176 train_loss:  174.219 train_acc:  94.412 test_loss:  201.705 test_acc:  94.035\n",
      "epoch:  2177 train_loss:  171.033 train_acc:  94.134 test_loss:  217.219 test_acc:  93.42\n",
      "epoch:  2178 train_loss:  174.912 train_acc:  94.197 test_loss:  188.855 test_acc:  93.68\n",
      "epoch:  2179 train_loss:  152.995 train_acc:  94.832 test_loss:  183.893 test_acc:  94.085\n",
      "epoch:  2180 train_loss:  163.395 train_acc:  94.771 test_loss:  237.577 test_acc:  93.816\n",
      "epoch:  2181 train_loss:  175.707 train_acc:  94.351 test_loss:  194.224 test_acc:  94.128\n",
      "epoch:  2182 train_loss:  195.016 train_acc:  93.757 test_loss:  233.246 test_acc:  93.149\n",
      "epoch:  2183 train_loss:  172.464 train_acc:  94.552 test_loss:  226.97 test_acc:  94.018\n",
      "epoch:  2184 train_loss:  165.454 train_acc:  94.671 test_loss:  206.327 test_acc:  94.237\n",
      "epoch:  2185 train_loss:  153.265 train_acc:  95.041 test_loss:  193.463 test_acc:  94.376\n",
      "epoch:  2186 train_loss:  196.006 train_acc:  93.852 test_loss:  227.104 test_acc:  93.652\n",
      "epoch:  2187 train_loss:  153.458 train_acc:  94.954 test_loss:  172.348 test_acc:  94.546\n",
      "min loss:2187\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(229.3785)]\n",
      "1    [tensor(88.8594)]   [tensor(81.4577)]\n",
      "2   [tensor(162.9521)]  [tensor(163.2882)]\n",
      "3   [tensor(185.9385)]  [tensor(199.8852)]\n",
      "4   [tensor(212.8320)]  [tensor(211.8160)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(248.4201)]\n",
      "61  [tensor(127.2646)]  [tensor(126.4326)]\n",
      "62   [tensor(84.7217)]   [tensor(83.2231)]\n",
      "63  [tensor(104.8945)]  [tensor(125.8329)]\n",
      "64  [tensor(156.6016)]  [tensor(167.7388)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  2188 train_loss:  153.329 train_acc:  95.104 test_loss:  190.051 test_acc:  94.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2189 train_loss:  166.197 train_acc:  94.891 test_loss:  187.159 test_acc:  94.319\n",
      "epoch:  2190 train_loss:  171.662 train_acc:  94.921 test_loss:  236.839 test_acc:  93.959\n",
      "epoch:  2191 train_loss:  156.833 train_acc:  94.923 test_loss:  198.285 test_acc:  94.127\n",
      "epoch:  2192 train_loss:  160.836 train_acc:  94.896 test_loss:  188.735 test_acc:  94.416\n",
      "epoch:  2193 train_loss:  163.03 train_acc:  94.667 test_loss:  216.815 test_acc:  93.791\n",
      "epoch:  2194 train_loss:  165.737 train_acc:  94.826 test_loss:  235.203 test_acc:  93.577\n",
      "epoch:  2195 train_loss:  179.233 train_acc:  94.402 test_loss:  219.053 test_acc:  93.789\n",
      "epoch:  2196 train_loss:  159.703 train_acc:  94.935 test_loss:  201.192 test_acc:  94.211\n",
      "epoch:  2197 train_loss:  168.231 train_acc:  94.789 test_loss:  222.205 test_acc:  94.129\n",
      "epoch:  2198 train_loss:  186.537 train_acc:  93.821 test_loss:  223.533 test_acc:  93.115\n",
      "epoch:  2199 train_loss:  151.903 train_acc:  95.108 test_loss:  194.813 test_acc:  94.192\n",
      "epoch:  2200 train_loss:  157.237 train_acc:  94.789 test_loss:  187.317 test_acc:  94.189\n",
      "epoch:  2201 train_loss:  200.225 train_acc:  94.316 test_loss:  226.061 test_acc:  93.818\n",
      "epoch:  2202 train_loss:  169.001 train_acc:  94.476 test_loss:  192.538 test_acc:  93.983\n",
      "epoch:  2203 train_loss:  193.867 train_acc:  94.491 test_loss:  229.793 test_acc:  93.599\n",
      "epoch:  2204 train_loss:  162.622 train_acc:  94.921 test_loss:  236.952 test_acc:  93.69\n",
      "epoch:  2205 train_loss:  154.578 train_acc:  94.88 test_loss:  200.597 test_acc:  94.122\n",
      "epoch:  2206 train_loss:  151.829 train_acc:  94.979 test_loss:  192.209 test_acc:  94.198\n",
      "epoch:  2207 train_loss:  182.276 train_acc:  94.417 test_loss:  207.134 test_acc:  94.174\n",
      "epoch:  2208 train_loss:  197.563 train_acc:  94.168 test_loss:  198.762 test_acc:  93.674\n",
      "epoch:  2209 train_loss:  203.587 train_acc:  94.0 test_loss:  290.438 test_acc:  92.805\n",
      "epoch:  2210 train_loss:  166.106 train_acc:  94.715 test_loss:  195.38 test_acc:  94.293\n",
      "epoch:  2211 train_loss:  157.838 train_acc:  94.805 test_loss:  185.688 test_acc:  94.246\n",
      "epoch:  2212 train_loss:  213.291 train_acc:  94.278 test_loss:  301.608 test_acc:  92.932\n",
      "epoch:  2213 train_loss:  177.707 train_acc:  94.429 test_loss:  228.19 test_acc:  93.745\n",
      "epoch:  2214 train_loss:  177.193 train_acc:  94.82 test_loss:  251.21 test_acc:  93.617\n",
      "epoch:  2215 train_loss:  152.632 train_acc:  94.963 test_loss:  174.361 test_acc:  94.35\n",
      "epoch:  2216 train_loss:  154.098 train_acc:  95.153 test_loss:  200.72 test_acc:  94.037\n",
      "epoch:  2217 train_loss:  172.897 train_acc:  94.068 test_loss:  206.968 test_acc:  93.443\n",
      "epoch:  2218 train_loss:  156.249 train_acc:  95.053 test_loss:  193.134 test_acc:  94.389\n",
      "epoch:  2219 train_loss:  176.782 train_acc:  94.291 test_loss:  227.48 test_acc:  93.362\n",
      "epoch:  2220 train_loss:  190.645 train_acc:  93.797 test_loss:  272.99 test_acc:  92.891\n",
      "epoch:  2221 train_loss:  199.436 train_acc:  94.644 test_loss:  263.504 test_acc:  93.683\n",
      "epoch:  2222 train_loss:  149.998 train_acc:  95.123 test_loss:  192.407 test_acc:  94.267\n",
      "epoch:  2223 train_loss:  160.579 train_acc:  94.798 test_loss:  180.751 test_acc:  94.411\n",
      "epoch:  2224 train_loss:  219.409 train_acc:  94.283 test_loss:  289.842 test_acc:  93.395\n",
      "epoch:  2225 train_loss:  162.95 train_acc:  94.925 test_loss:  221.334 test_acc:  94.1\n",
      "epoch:  2226 train_loss:  153.45 train_acc:  95.04 test_loss:  205.837 test_acc:  94.216\n",
      "epoch:  2227 train_loss:  153.913 train_acc:  95.026 test_loss:  181.359 test_acc:  94.392\n",
      "epoch:  2228 train_loss:  192.871 train_acc:  93.858 test_loss:  238.543 test_acc:  93.15\n",
      "epoch:  2229 train_loss:  152.821 train_acc:  95.078 test_loss:  204.934 test_acc:  94.251\n",
      "epoch:  2230 train_loss:  154.088 train_acc:  95.138 test_loss:  197.301 test_acc:  94.271\n",
      "epoch:  2231 train_loss:  160.557 train_acc:  94.733 test_loss:  203.346 test_acc:  93.853\n",
      "epoch:  2232 train_loss:  173.088 train_acc:  94.489 test_loss:  177.278 test_acc:  94.281\n",
      "epoch:  2233 train_loss:  156.478 train_acc:  95.047 test_loss:  189.735 test_acc:  94.273\n",
      "epoch:  2234 train_loss:  155.681 train_acc:  94.723 test_loss:  191.678 test_acc:  94.153\n",
      "epoch:  2235 train_loss:  180.279 train_acc:  94.792 test_loss:  232.081 test_acc:  93.752\n",
      "epoch:  2236 train_loss:  164.803 train_acc:  94.575 test_loss:  189.55 test_acc:  94.273\n",
      "epoch:  2237 train_loss:  179.226 train_acc:  94.651 test_loss:  195.934 test_acc:  93.892\n",
      "epoch:  2238 train_loss:  187.814 train_acc:  93.566 test_loss:  221.169 test_acc:  93.306\n",
      "epoch:  2239 train_loss:  155.31 train_acc:  94.783 test_loss:  193.009 test_acc:  93.958\n",
      "epoch:  2240 train_loss:  163.973 train_acc:  94.605 test_loss:  199.219 test_acc:  93.652\n",
      "epoch:  2241 train_loss:  178.154 train_acc:  94.548 test_loss:  231.312 test_acc:  93.622\n",
      "epoch:  2242 train_loss:  150.562 train_acc:  95.083 test_loss:  189.993 test_acc:  94.409\n",
      "epoch:  2243 train_loss:  152.95 train_acc:  95.002 test_loss:  184.71 test_acc:  94.25\n",
      "epoch:  2244 train_loss:  163.225 train_acc:  94.654 test_loss:  207.316 test_acc:  94.269\n",
      "epoch:  2245 train_loss:  154.208 train_acc:  95.121 test_loss:  191.446 test_acc:  94.211\n",
      "epoch:  2246 train_loss:  189.248 train_acc:  94.789 test_loss:  248.534 test_acc:  93.869\n",
      "epoch:  2247 train_loss:  180.747 train_acc:  94.739 test_loss:  251.894 test_acc:  93.75\n",
      "epoch:  2248 train_loss:  155.262 train_acc:  95.089 test_loss:  208.166 test_acc:  94.279\n",
      "epoch:  2249 train_loss:  166.778 train_acc:  94.959 test_loss:  197.546 test_acc:  93.94\n",
      "epoch:  2250 train_loss:  241.885 train_acc:  93.774 test_loss:  309.007 test_acc:  93.184\n",
      "epoch:  2251 train_loss:  208.918 train_acc:  94.367 test_loss:  271.289 test_acc:  93.442\n",
      "epoch:  2252 train_loss:  157.097 train_acc:  95.078 test_loss:  204.497 test_acc:  93.985\n",
      "epoch:  2253 train_loss:  188.404 train_acc:  94.36 test_loss:  233.451 test_acc:  93.487\n",
      "epoch:  2254 train_loss:  157.634 train_acc:  94.812 test_loss:  202.425 test_acc:  94.135\n",
      "epoch:  2255 train_loss:  165.711 train_acc:  94.793 test_loss:  233.924 test_acc:  93.782\n",
      "epoch:  2256 train_loss:  164.842 train_acc:  94.893 test_loss:  219.755 test_acc:  93.987\n",
      "epoch:  2257 train_loss:  153.534 train_acc:  95.047 test_loss:  178.721 test_acc:  94.445\n",
      "epoch:  2258 train_loss:  176.24 train_acc:  94.432 test_loss:  196.812 test_acc:  94.227\n",
      "epoch:  2259 train_loss:  168.547 train_acc:  94.799 test_loss:  223.039 test_acc:  94.002\n",
      "epoch:  2260 train_loss:  153.567 train_acc:  94.948 test_loss:  186.029 test_acc:  94.306\n",
      "epoch:  2261 train_loss:  155.976 train_acc:  94.653 test_loss:  185.219 test_acc:  94.053\n",
      "epoch:  2262 train_loss:  150.036 train_acc:  94.939 test_loss:  179.357 test_acc:  94.472\n",
      "epoch:  2263 train_loss:  160.091 train_acc:  94.706 test_loss:  202.189 test_acc:  93.703\n",
      "epoch:  2264 train_loss:  151.881 train_acc:  94.932 test_loss:  186.548 test_acc:  94.346\n",
      "epoch:  2265 train_loss:  185.013 train_acc:  93.965 test_loss:  226.548 test_acc:  93.388\n",
      "epoch:  2266 train_loss:  149.952 train_acc:  95.066 test_loss:  177.903 test_acc:  94.586\n",
      "epoch:  2267 train_loss:  251.08 train_acc:  92.119 test_loss:  266.459 test_acc:  92.012\n",
      "epoch:  2268 train_loss:  180.788 train_acc:  94.581 test_loss:  219.432 test_acc:  93.845\n",
      "epoch:  2269 train_loss:  159.774 train_acc:  94.777 test_loss:  205.688 test_acc:  94.168\n",
      "epoch:  2270 train_loss:  196.228 train_acc:  94.276 test_loss:  259.17 test_acc:  93.347\n",
      "epoch:  2271 train_loss:  171.123 train_acc:  94.851 test_loss:  226.806 test_acc:  93.963\n",
      "epoch:  2272 train_loss:  175.759 train_acc:  94.709 test_loss:  193.527 test_acc:  94.082\n",
      "epoch:  2273 train_loss:  172.444 train_acc:  94.676 test_loss:  205.025 test_acc:  94.072\n",
      "epoch:  2274 train_loss:  163.179 train_acc:  94.826 test_loss:  209.125 test_acc:  93.946\n",
      "epoch:  2275 train_loss:  153.45 train_acc:  95.172 test_loss:  214.078 test_acc:  94.258\n",
      "epoch:  2276 train_loss:  166.195 train_acc:  94.877 test_loss:  214.437 test_acc:  94.137\n",
      "epoch:  2277 train_loss:  174.211 train_acc:  94.775 test_loss:  231.967 test_acc:  93.853\n",
      "epoch:  2278 train_loss:  191.933 train_acc:  94.368 test_loss:  257.426 test_acc:  93.648\n",
      "epoch:  2279 train_loss:  175.244 train_acc:  94.795 test_loss:  235.872 test_acc:  93.825\n",
      "epoch:  2280 train_loss:  153.653 train_acc:  94.727 test_loss:  190.997 test_acc:  93.937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2281 train_loss:  163.557 train_acc:  94.568 test_loss:  203.995 test_acc:  93.786\n",
      "epoch:  2282 train_loss:  180.257 train_acc:  94.255 test_loss:  210.433 test_acc:  93.76\n",
      "epoch:  2283 train_loss:  217.779 train_acc:  94.008 test_loss:  258.82 test_acc:  93.654\n",
      "epoch:  2284 train_loss:  155.833 train_acc:  95.066 test_loss:  200.424 test_acc:  94.523\n",
      "epoch:  2285 train_loss:  188.668 train_acc:  94.552 test_loss:  229.161 test_acc:  93.79\n",
      "epoch:  2286 train_loss:  152.105 train_acc:  94.852 test_loss:  183.652 test_acc:  93.991\n",
      "epoch:  2287 train_loss:  167.336 train_acc:  94.785 test_loss:  211.452 test_acc:  93.975\n",
      "epoch:  2288 train_loss:  155.088 train_acc:  95.052 test_loss:  172.187 test_acc:  94.352\n",
      "min loss:2288\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(228.7317)]\n",
      "1    [tensor(88.8594)]   [tensor(80.7548)]\n",
      "2   [tensor(162.9521)]  [tensor(160.5777)]\n",
      "3   [tensor(185.9385)]  [tensor(197.8055)]\n",
      "4   [tensor(212.8320)]  [tensor(209.5210)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(247.5323)]\n",
      "61  [tensor(127.2646)]  [tensor(126.7648)]\n",
      "62   [tensor(84.7217)]   [tensor(81.7931)]\n",
      "63  [tensor(104.8945)]  [tensor(131.4180)]\n",
      "64  [tensor(156.6016)]  [tensor(168.5589)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  2289 train_loss:  186.867 train_acc:  94.725 test_loss:  230.612 test_acc:  93.864\n",
      "epoch:  2290 train_loss:  158.957 train_acc:  94.65 test_loss:  180.848 test_acc:  94.07\n",
      "epoch:  2291 train_loss:  159.913 train_acc:  94.705 test_loss:  182.905 test_acc:  94.445\n",
      "epoch:  2292 train_loss:  199.519 train_acc:  94.155 test_loss:  216.539 test_acc:  93.807\n",
      "epoch:  2293 train_loss:  162.308 train_acc:  94.608 test_loss:  197.374 test_acc:  94.085\n",
      "epoch:  2294 train_loss:  156.502 train_acc:  94.762 test_loss:  173.746 test_acc:  94.253\n",
      "epoch:  2295 train_loss:  162.965 train_acc:  94.87 test_loss:  182.872 test_acc:  94.336\n",
      "epoch:  2296 train_loss:  212.913 train_acc:  93.098 test_loss:  223.676 test_acc:  92.912\n",
      "epoch:  2297 train_loss:  154.2 train_acc:  94.959 test_loss:  178.121 test_acc:  94.141\n",
      "epoch:  2298 train_loss:  155.456 train_acc:  94.911 test_loss:  211.045 test_acc:  94.027\n",
      "epoch:  2299 train_loss:  162.162 train_acc:  94.842 test_loss:  213.043 test_acc:  94.087\n",
      "epoch:  2300 train_loss:  155.017 train_acc:  94.964 test_loss:  193.515 test_acc:  94.297\n",
      "epoch:  2301 train_loss:  166.113 train_acc:  94.852 test_loss:  222.13 test_acc:  93.95\n",
      "epoch:  2302 train_loss:  161.368 train_acc:  94.699 test_loss:  210.459 test_acc:  94.033\n",
      "epoch:  2303 train_loss:  160.896 train_acc:  94.89 test_loss:  198.447 test_acc:  93.853\n",
      "epoch:  2304 train_loss:  157.385 train_acc:  94.908 test_loss:  205.441 test_acc:  94.066\n",
      "epoch:  2305 train_loss:  155.344 train_acc:  95.197 test_loss:  205.903 test_acc:  94.238\n",
      "epoch:  2306 train_loss:  156.761 train_acc:  94.914 test_loss:  196.471 test_acc:  94.026\n",
      "epoch:  2307 train_loss:  179.987 train_acc:  94.156 test_loss:  240.186 test_acc:  93.605\n",
      "epoch:  2308 train_loss:  157.811 train_acc:  95.068 test_loss:  206.063 test_acc:  94.37\n",
      "epoch:  2309 train_loss:  166.691 train_acc:  94.933 test_loss:  216.789 test_acc:  93.99\n",
      "epoch:  2310 train_loss:  169.834 train_acc:  94.877 test_loss:  237.852 test_acc:  93.846\n",
      "epoch:  2311 train_loss:  154.348 train_acc:  95.051 test_loss:  189.526 test_acc:  94.378\n",
      "epoch:  2312 train_loss:  154.906 train_acc:  95.044 test_loss:  208.326 test_acc:  94.119\n",
      "epoch:  2313 train_loss:  184.977 train_acc:  94.773 test_loss:  234.571 test_acc:  93.717\n",
      "epoch:  2314 train_loss:  219.913 train_acc:  93.681 test_loss:  257.104 test_acc:  93.456\n",
      "epoch:  2315 train_loss:  152.625 train_acc:  94.951 test_loss:  177.734 test_acc:  94.304\n",
      "epoch:  2316 train_loss:  182.294 train_acc:  94.131 test_loss:  199.928 test_acc:  94.193\n",
      "epoch:  2317 train_loss:  156.75 train_acc:  94.973 test_loss:  176.434 test_acc:  94.345\n",
      "epoch:  2318 train_loss:  184.312 train_acc:  94.423 test_loss:  246.129 test_acc:  93.971\n",
      "epoch:  2319 train_loss:  167.898 train_acc:  94.828 test_loss:  246.839 test_acc:  93.559\n",
      "epoch:  2320 train_loss:  167.608 train_acc:  94.757 test_loss:  196.691 test_acc:  94.137\n",
      "epoch:  2321 train_loss:  152.534 train_acc:  94.911 test_loss:  188.366 test_acc:  93.959\n",
      "epoch:  2322 train_loss:  176.525 train_acc:  94.549 test_loss:  228.889 test_acc:  93.838\n",
      "epoch:  2323 train_loss:  160.723 train_acc:  94.895 test_loss:  213.705 test_acc:  94.154\n",
      "epoch:  2324 train_loss:  175.035 train_acc:  94.533 test_loss:  221.898 test_acc:  93.85\n",
      "epoch:  2325 train_loss:  147.746 train_acc:  95.218 test_loss:  194.425 test_acc:  94.434\n",
      "epoch:  2326 train_loss:  203.326 train_acc:  93.684 test_loss:  217.558 test_acc:  93.394\n",
      "epoch:  2327 train_loss:  157.331 train_acc:  95.031 test_loss:  185.973 test_acc:  94.513\n",
      "epoch:  2328 train_loss:  195.958 train_acc:  94.574 test_loss:  216.015 test_acc:  93.789\n",
      "epoch:  2329 train_loss:  183.312 train_acc:  94.312 test_loss:  272.168 test_acc:  93.483\n",
      "epoch:  2330 train_loss:  179.968 train_acc:  94.5 test_loss:  249.289 test_acc:  93.553\n",
      "epoch:  2331 train_loss:  149.958 train_acc:  95.002 test_loss:  176.292 test_acc:  94.288\n",
      "epoch:  2332 train_loss:  209.819 train_acc:  94.337 test_loss:  285.447 test_acc:  93.708\n",
      "epoch:  2333 train_loss:  151.234 train_acc:  94.994 test_loss:  181.015 test_acc:  94.551\n",
      "epoch:  2334 train_loss:  156.061 train_acc:  94.909 test_loss:  185.34 test_acc:  94.553\n",
      "epoch:  2335 train_loss:  160.705 train_acc:  94.953 test_loss:  188.93 test_acc:  94.258\n",
      "epoch:  2336 train_loss:  156.081 train_acc:  94.814 test_loss:  185.353 test_acc:  94.317\n",
      "epoch:  2337 train_loss:  204.314 train_acc:  94.313 test_loss:  235.291 test_acc:  93.351\n",
      "epoch:  2338 train_loss:  181.28 train_acc:  94.636 test_loss:  229.73 test_acc:  93.725\n",
      "epoch:  2339 train_loss:  265.225 train_acc:  93.082 test_loss:  299.102 test_acc:  92.86\n",
      "epoch:  2340 train_loss:  201.979 train_acc:  94.382 test_loss:  251.045 test_acc:  93.267\n",
      "epoch:  2341 train_loss:  160.283 train_acc:  94.88 test_loss:  218.563 test_acc:  94.081\n",
      "epoch:  2342 train_loss:  155.409 train_acc:  94.961 test_loss:  194.76 test_acc:  94.253\n",
      "epoch:  2343 train_loss:  201.389 train_acc:  94.139 test_loss:  246.108 test_acc:  93.413\n",
      "epoch:  2344 train_loss:  158.116 train_acc:  94.625 test_loss:  191.895 test_acc:  93.914\n",
      "epoch:  2345 train_loss:  188.45 train_acc:  94.508 test_loss:  247.348 test_acc:  93.82\n",
      "epoch:  2346 train_loss:  225.051 train_acc:  92.833 test_loss:  258.001 test_acc:  92.688\n",
      "epoch:  2347 train_loss:  176.006 train_acc:  94.659 test_loss:  208.334 test_acc:  94.205\n",
      "epoch:  2348 train_loss:  166.815 train_acc:  94.228 test_loss:  194.641 test_acc:  93.709\n",
      "epoch:  2349 train_loss:  149.232 train_acc:  95.008 test_loss:  170.817 test_acc:  94.629\n",
      "min loss:2349\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(229.7878)]\n",
      "1    [tensor(88.8594)]   [tensor(81.6317)]\n",
      "2   [tensor(162.9521)]  [tensor(162.9790)]\n",
      "3   [tensor(185.9385)]  [tensor(204.7581)]\n",
      "4   [tensor(212.8320)]  [tensor(211.3906)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(252.3033)]\n",
      "61  [tensor(127.2646)]  [tensor(128.4687)]\n",
      "62   [tensor(84.7217)]   [tensor(83.9619)]\n",
      "63  [tensor(104.8945)]  [tensor(130.8300)]\n",
      "64  [tensor(156.6016)]  [tensor(170.3149)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  2350 train_loss:  154.061 train_acc:  95.089 test_loss:  191.267 test_acc:  94.105\n",
      "epoch:  2351 train_loss:  153.584 train_acc:  95.046 test_loss:  204.656 test_acc:  94.316\n",
      "epoch:  2352 train_loss:  172.731 train_acc:  94.768 test_loss:  230.633 test_acc:  93.924\n",
      "epoch:  2353 train_loss:  160.389 train_acc:  95.03 test_loss:  224.513 test_acc:  94.094\n",
      "epoch:  2354 train_loss:  182.526 train_acc:  94.011 test_loss:  219.307 test_acc:  93.371\n",
      "epoch:  2355 train_loss:  154.555 train_acc:  94.862 test_loss:  183.019 test_acc:  94.115\n",
      "epoch:  2356 train_loss:  163.098 train_acc:  94.875 test_loss:  200.853 test_acc:  94.25\n",
      "epoch:  2357 train_loss:  156.515 train_acc:  94.764 test_loss:  183.868 test_acc:  94.125\n",
      "epoch:  2358 train_loss:  201.815 train_acc:  93.351 test_loss:  188.648 test_acc:  93.397\n",
      "epoch:  2359 train_loss:  166.576 train_acc:  94.61 test_loss:  194.906 test_acc:  94.035\n",
      "epoch:  2360 train_loss:  157.445 train_acc:  95.034 test_loss:  204.844 test_acc:  94.569\n",
      "epoch:  2361 train_loss:  152.192 train_acc:  94.94 test_loss:  202.792 test_acc:  94.093\n",
      "epoch:  2362 train_loss:  149.528 train_acc:  95.066 test_loss:  167.52 test_acc:  94.609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss:2362\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(228.8200)]\n",
      "1    [tensor(88.8594)]   [tensor(80.9206)]\n",
      "2   [tensor(162.9521)]  [tensor(163.2150)]\n",
      "3   [tensor(185.9385)]  [tensor(205.0229)]\n",
      "4   [tensor(212.8320)]  [tensor(209.0432)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(254.7177)]\n",
      "61  [tensor(127.2646)]  [tensor(129.4130)]\n",
      "62   [tensor(84.7217)]   [tensor(82.8118)]\n",
      "63  [tensor(104.8945)]  [tensor(125.7372)]\n",
      "64  [tensor(156.6016)]  [tensor(166.2553)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  2363 train_loss:  173.517 train_acc:  94.839 test_loss:  216.696 test_acc:  93.966\n",
      "epoch:  2364 train_loss:  151.429 train_acc:  95.013 test_loss:  183.015 test_acc:  94.083\n",
      "epoch:  2365 train_loss:  153.993 train_acc:  95.171 test_loss:  201.843 test_acc:  94.345\n",
      "epoch:  2366 train_loss:  153.631 train_acc:  95.017 test_loss:  197.149 test_acc:  94.31\n",
      "epoch:  2367 train_loss:  200.91 train_acc:  94.356 test_loss:  229.746 test_acc:  93.549\n",
      "epoch:  2368 train_loss:  155.991 train_acc:  94.655 test_loss:  194.421 test_acc:  93.993\n",
      "epoch:  2369 train_loss:  176.507 train_acc:  94.869 test_loss:  250.704 test_acc:  93.867\n",
      "epoch:  2370 train_loss:  157.184 train_acc:  95.06 test_loss:  212.76 test_acc:  94.232\n",
      "epoch:  2371 train_loss:  191.394 train_acc:  94.186 test_loss:  244.427 test_acc:  93.427\n",
      "epoch:  2372 train_loss:  200.547 train_acc:  94.04 test_loss:  212.938 test_acc:  93.675\n",
      "epoch:  2373 train_loss:  155.162 train_acc:  94.918 test_loss:  189.772 test_acc:  94.407\n",
      "epoch:  2374 train_loss:  150.159 train_acc:  94.885 test_loss:  172.875 test_acc:  94.315\n",
      "epoch:  2375 train_loss:  164.206 train_acc:  94.843 test_loss:  221.224 test_acc:  94.099\n",
      "epoch:  2376 train_loss:  177.427 train_acc:  94.467 test_loss:  238.891 test_acc:  93.817\n",
      "epoch:  2377 train_loss:  168.963 train_acc:  94.763 test_loss:  196.754 test_acc:  94.059\n",
      "epoch:  2378 train_loss:  158.755 train_acc:  94.708 test_loss:  212.763 test_acc:  93.768\n",
      "epoch:  2379 train_loss:  151.227 train_acc:  95.044 test_loss:  198.891 test_acc:  94.081\n",
      "epoch:  2380 train_loss:  149.3 train_acc:  95.075 test_loss:  198.311 test_acc:  93.989\n",
      "epoch:  2381 train_loss:  163.717 train_acc:  94.393 test_loss:  179.881 test_acc:  94.033\n",
      "epoch:  2382 train_loss:  154.017 train_acc:  94.878 test_loss:  184.482 test_acc:  94.095\n",
      "epoch:  2383 train_loss:  156.286 train_acc:  94.677 test_loss:  186.516 test_acc:  94.202\n",
      "epoch:  2384 train_loss:  187.722 train_acc:  94.278 test_loss:  271.201 test_acc:  93.846\n",
      "epoch:  2385 train_loss:  174.531 train_acc:  94.559 test_loss:  205.631 test_acc:  93.838\n",
      "epoch:  2386 train_loss:  155.371 train_acc:  94.999 test_loss:  204.739 test_acc:  94.196\n",
      "epoch:  2387 train_loss:  169.413 train_acc:  94.684 test_loss:  235.781 test_acc:  94.166\n",
      "epoch:  2388 train_loss:  168.122 train_acc:  94.816 test_loss:  213.46 test_acc:  94.019\n",
      "epoch:  2389 train_loss:  263.117 train_acc:  93.206 test_loss:  419.54 test_acc:  91.888\n",
      "epoch:  2390 train_loss:  152.901 train_acc:  95.109 test_loss:  208.011 test_acc:  94.257\n",
      "epoch:  2391 train_loss:  162.847 train_acc:  94.606 test_loss:  203.951 test_acc:  93.854\n",
      "epoch:  2392 train_loss:  156.524 train_acc:  95.022 test_loss:  184.433 test_acc:  94.391\n",
      "epoch:  2393 train_loss:  168.914 train_acc:  94.707 test_loss:  233.148 test_acc:  94.235\n",
      "epoch:  2394 train_loss:  151.804 train_acc:  95.098 test_loss:  197.881 test_acc:  94.46\n",
      "epoch:  2395 train_loss:  163.225 train_acc:  94.896 test_loss:  195.14 test_acc:  94.028\n",
      "epoch:  2396 train_loss:  160.933 train_acc:  94.742 test_loss:  198.706 test_acc:  94.051\n",
      "epoch:  2397 train_loss:  152.283 train_acc:  95.14 test_loss:  182.993 test_acc:  94.51\n",
      "epoch:  2398 train_loss:  174.17 train_acc:  94.834 test_loss:  254.485 test_acc:  94.053\n",
      "epoch:  2399 train_loss:  178.241 train_acc:  94.821 test_loss:  280.807 test_acc:  93.491\n",
      "epoch:  2400 train_loss:  205.478 train_acc:  94.13 test_loss:  242.875 test_acc:  93.746\n",
      "epoch:  2401 train_loss:  171.053 train_acc:  94.421 test_loss:  195.07 test_acc:  93.781\n",
      "epoch:  2402 train_loss:  178.646 train_acc:  94.787 test_loss:  233.961 test_acc:  93.661\n",
      "epoch:  2403 train_loss:  219.493 train_acc:  94.206 test_loss:  259.151 test_acc:  93.498\n",
      "epoch:  2404 train_loss:  186.703 train_acc:  94.617 test_loss:  232.891 test_acc:  93.773\n",
      "epoch:  2405 train_loss:  170.645 train_acc:  94.326 test_loss:  188.303 test_acc:  94.179\n",
      "epoch:  2406 train_loss:  155.65 train_acc:  94.886 test_loss:  187.865 test_acc:  94.249\n",
      "epoch:  2407 train_loss:  161.471 train_acc:  95.019 test_loss:  207.966 test_acc:  94.156\n",
      "epoch:  2408 train_loss:  192.153 train_acc:  94.251 test_loss:  274.751 test_acc:  93.506\n",
      "epoch:  2409 train_loss:  162.051 train_acc:  95.001 test_loss:  205.329 test_acc:  94.08\n",
      "epoch:  2410 train_loss:  166.513 train_acc:  94.384 test_loss:  182.232 test_acc:  93.933\n",
      "epoch:  2411 train_loss:  178.411 train_acc:  94.042 test_loss:  211.564 test_acc:  93.432\n",
      "epoch:  2412 train_loss:  175.133 train_acc:  94.707 test_loss:  222.843 test_acc:  93.822\n",
      "epoch:  2413 train_loss:  200.871 train_acc:  93.58 test_loss:  217.403 test_acc:  93.464\n",
      "epoch:  2414 train_loss:  150.926 train_acc:  95.027 test_loss:  185.768 test_acc:  94.12\n",
      "epoch:  2415 train_loss:  156.787 train_acc:  95.109 test_loss:  198.731 test_acc:  94.35\n",
      "epoch:  2416 train_loss:  192.135 train_acc:  94.639 test_loss:  287.042 test_acc:  93.576\n",
      "epoch:  2417 train_loss:  179.018 train_acc:  94.509 test_loss:  240.089 test_acc:  93.446\n",
      "epoch:  2418 train_loss:  182.266 train_acc:  94.407 test_loss:  257.443 test_acc:  93.738\n",
      "epoch:  2419 train_loss:  179.919 train_acc:  94.556 test_loss:  256.179 test_acc:  93.728\n",
      "epoch:  2420 train_loss:  155.076 train_acc:  94.738 test_loss:  189.812 test_acc:  94.024\n",
      "epoch:  2421 train_loss:  156.268 train_acc:  94.971 test_loss:  192.212 test_acc:  94.04\n",
      "epoch:  2422 train_loss:  185.526 train_acc:  94.745 test_loss:  223.678 test_acc:  94.057\n",
      "epoch:  2423 train_loss:  147.81 train_acc:  95.075 test_loss:  184.699 test_acc:  94.472\n",
      "epoch:  2424 train_loss:  152.197 train_acc:  95.004 test_loss:  188.168 test_acc:  94.429\n",
      "epoch:  2425 train_loss:  174.586 train_acc:  94.463 test_loss:  203.168 test_acc:  94.029\n",
      "epoch:  2426 train_loss:  173.949 train_acc:  94.664 test_loss:  247.961 test_acc:  93.54\n",
      "epoch:  2427 train_loss:  160.322 train_acc:  94.96 test_loss:  221.55 test_acc:  94.369\n",
      "epoch:  2428 train_loss:  151.947 train_acc:  94.978 test_loss:  179.445 test_acc:  94.105\n",
      "epoch:  2429 train_loss:  248.408 train_acc:  92.857 test_loss:  267.71 test_acc:  92.789\n",
      "epoch:  2430 train_loss:  154.139 train_acc:  94.918 test_loss:  197.268 test_acc:  94.193\n",
      "epoch:  2431 train_loss:  175.947 train_acc:  94.895 test_loss:  237.198 test_acc:  93.757\n",
      "epoch:  2432 train_loss:  151.891 train_acc:  95.128 test_loss:  210.732 test_acc:  94.161\n",
      "epoch:  2433 train_loss:  165.899 train_acc:  94.867 test_loss:  229.066 test_acc:  93.972\n",
      "epoch:  2434 train_loss:  194.405 train_acc:  94.114 test_loss:  266.128 test_acc:  93.093\n",
      "epoch:  2435 train_loss:  167.37 train_acc:  94.494 test_loss:  204.027 test_acc:  93.913\n",
      "epoch:  2436 train_loss:  157.439 train_acc:  94.617 test_loss:  181.626 test_acc:  93.878\n",
      "epoch:  2437 train_loss:  168.222 train_acc:  94.91 test_loss:  214.002 test_acc:  94.224\n",
      "epoch:  2438 train_loss:  168.675 train_acc:  94.374 test_loss:  207.073 test_acc:  93.633\n",
      "epoch:  2439 train_loss:  148.154 train_acc:  95.133 test_loss:  180.077 test_acc:  94.513\n",
      "epoch:  2440 train_loss:  177.467 train_acc:  94.728 test_loss:  251.87 test_acc:  93.582\n",
      "epoch:  2441 train_loss:  181.563 train_acc:  93.827 test_loss:  214.994 test_acc:  93.618\n",
      "epoch:  2442 train_loss:  154.832 train_acc:  95.065 test_loss:  180.455 test_acc:  94.243\n",
      "epoch:  2443 train_loss:  174.008 train_acc:  94.431 test_loss:  179.776 test_acc:  94.344\n",
      "epoch:  2444 train_loss:  235.217 train_acc:  94.158 test_loss:  247.59 test_acc:  93.426\n",
      "epoch:  2445 train_loss:  169.309 train_acc:  94.606 test_loss:  206.612 test_acc:  94.029\n",
      "epoch:  2446 train_loss:  167.408 train_acc:  94.752 test_loss:  200.081 test_acc:  94.035\n",
      "epoch:  2447 train_loss:  230.77 train_acc:  93.048 test_loss:  276.476 test_acc:  92.518\n",
      "epoch:  2448 train_loss:  150.269 train_acc:  95.05 test_loss:  195.352 test_acc:  94.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2449 train_loss:  165.83 train_acc:  94.951 test_loss:  230.5 test_acc:  93.975\n",
      "epoch:  2450 train_loss:  176.283 train_acc:  94.619 test_loss:  219.335 test_acc:  94.116\n",
      "epoch:  2451 train_loss:  147.646 train_acc:  95.081 test_loss:  179.188 test_acc:  94.45\n",
      "epoch:  2452 train_loss:  156.412 train_acc:  94.83 test_loss:  206.545 test_acc:  93.904\n",
      "epoch:  2453 train_loss:  204.13 train_acc:  93.773 test_loss:  260.086 test_acc:  93.1\n",
      "epoch:  2454 train_loss:  157.683 train_acc:  94.721 test_loss:  199.625 test_acc:  94.225\n",
      "epoch:  2455 train_loss:  170.197 train_acc:  94.575 test_loss:  176.909 test_acc:  94.194\n",
      "epoch:  2456 train_loss:  174.173 train_acc:  94.455 test_loss:  220.54 test_acc:  93.63\n",
      "epoch:  2457 train_loss:  194.075 train_acc:  93.847 test_loss:  250.911 test_acc:  93.076\n",
      "epoch:  2458 train_loss:  185.859 train_acc:  94.246 test_loss:  218.561 test_acc:  93.875\n",
      "epoch:  2459 train_loss:  147.528 train_acc:  95.24 test_loss:  185.894 test_acc:  94.414\n",
      "epoch:  2460 train_loss:  173.003 train_acc:  94.241 test_loss:  192.114 test_acc:  93.716\n",
      "epoch:  2461 train_loss:  150.207 train_acc:  95.021 test_loss:  178.076 test_acc:  94.611\n",
      "epoch:  2462 train_loss:  169.414 train_acc:  94.416 test_loss:  221.923 test_acc:  93.499\n",
      "epoch:  2463 train_loss:  170.539 train_acc:  94.95 test_loss:  215.584 test_acc:  93.999\n",
      "epoch:  2464 train_loss:  169.965 train_acc:  94.141 test_loss:  206.287 test_acc:  93.756\n",
      "epoch:  2465 train_loss:  151.063 train_acc:  95.18 test_loss:  197.514 test_acc:  94.243\n",
      "epoch:  2466 train_loss:  154.727 train_acc:  95.02 test_loss:  203.403 test_acc:  94.246\n",
      "epoch:  2467 train_loss:  166.045 train_acc:  95.044 test_loss:  217.414 test_acc:  94.04\n",
      "epoch:  2468 train_loss:  161.333 train_acc:  94.955 test_loss:  194.905 test_acc:  94.393\n",
      "epoch:  2469 train_loss:  159.968 train_acc:  94.51 test_loss:  174.476 test_acc:  94.134\n",
      "epoch:  2470 train_loss:  155.411 train_acc:  95.055 test_loss:  211.064 test_acc:  94.118\n",
      "epoch:  2471 train_loss:  150.413 train_acc:  95.072 test_loss:  204.369 test_acc:  94.112\n",
      "epoch:  2472 train_loss:  165.792 train_acc:  94.743 test_loss:  223.069 test_acc:  93.787\n",
      "epoch:  2473 train_loss:  163.699 train_acc:  95.017 test_loss:  227.519 test_acc:  94.149\n",
      "epoch:  2474 train_loss:  172.744 train_acc:  94.769 test_loss:  207.037 test_acc:  94.214\n",
      "epoch:  2475 train_loss:  267.821 train_acc:  93.723 test_loss:  312.735 test_acc:  93.142\n",
      "epoch:  2476 train_loss:  150.281 train_acc:  94.973 test_loss:  184.884 test_acc:  94.22\n",
      "epoch:  2477 train_loss:  149.928 train_acc:  94.971 test_loss:  180.654 test_acc:  94.393\n",
      "epoch:  2478 train_loss:  166.797 train_acc:  94.399 test_loss:  183.756 test_acc:  94.212\n",
      "epoch:  2479 train_loss:  154.703 train_acc:  94.982 test_loss:  194.148 test_acc:  94.577\n",
      "epoch:  2480 train_loss:  151.668 train_acc:  94.928 test_loss:  183.229 test_acc:  94.138\n",
      "epoch:  2481 train_loss:  202.33 train_acc:  93.323 test_loss:  228.424 test_acc:  93.194\n",
      "epoch:  2482 train_loss:  186.362 train_acc:  94.787 test_loss:  238.483 test_acc:  93.877\n",
      "epoch:  2483 train_loss:  162.797 train_acc:  94.596 test_loss:  208.35 test_acc:  93.923\n",
      "epoch:  2484 train_loss:  150.701 train_acc:  94.937 test_loss:  169.79 test_acc:  94.403\n",
      "epoch:  2485 train_loss:  170.067 train_acc:  94.448 test_loss:  209.087 test_acc:  93.887\n",
      "epoch:  2486 train_loss:  216.678 train_acc:  93.169 test_loss:  245.895 test_acc:  93.155\n",
      "epoch:  2487 train_loss:  163.714 train_acc:  94.889 test_loss:  230.637 test_acc:  93.47\n",
      "epoch:  2488 train_loss:  175.652 train_acc:  94.431 test_loss:  253.048 test_acc:  93.336\n",
      "epoch:  2489 train_loss:  154.482 train_acc:  94.829 test_loss:  203.32 test_acc:  94.08\n",
      "epoch:  2490 train_loss:  171.767 train_acc:  94.302 test_loss:  199.356 test_acc:  94.085\n",
      "epoch:  2491 train_loss:  154.752 train_acc:  94.795 test_loss:  168.864 test_acc:  94.249\n",
      "epoch:  2492 train_loss:  169.726 train_acc:  94.486 test_loss:  219.429 test_acc:  93.584\n",
      "epoch:  2493 train_loss:  218.171 train_acc:  94.117 test_loss:  238.793 test_acc:  93.832\n",
      "epoch:  2494 train_loss:  176.597 train_acc:  94.24 test_loss:  245.016 test_acc:  93.42\n",
      "epoch:  2495 train_loss:  167.469 train_acc:  94.829 test_loss:  207.595 test_acc:  94.048\n",
      "epoch:  2496 train_loss:  147.659 train_acc:  95.211 test_loss:  184.894 test_acc:  94.461\n",
      "epoch:  2497 train_loss:  152.873 train_acc:  95.061 test_loss:  211.104 test_acc:  94.047\n",
      "epoch:  2498 train_loss:  164.09 train_acc:  94.874 test_loss:  241.342 test_acc:  93.857\n",
      "epoch:  2499 train_loss:  190.542 train_acc:  94.425 test_loss:  260.742 test_acc:  93.573\n",
      "epoch:  2500 train_loss:  177.099 train_acc:  94.096 test_loss:  190.312 test_acc:  93.753\n",
      "epoch:  2501 train_loss:  182.355 train_acc:  94.738 test_loss:  232.851 test_acc:  93.925\n",
      "epoch:  2502 train_loss:  172.535 train_acc:  93.964 test_loss:  217.206 test_acc:  93.366\n",
      "epoch:  2503 train_loss:  159.384 train_acc:  94.642 test_loss:  200.639 test_acc:  94.004\n",
      "epoch:  2504 train_loss:  148.39 train_acc:  95.106 test_loss:  173.291 test_acc:  94.608\n",
      "epoch:  2505 train_loss:  163.906 train_acc:  94.961 test_loss:  210.442 test_acc:  93.748\n",
      "epoch:  2506 train_loss:  156.776 train_acc:  95.037 test_loss:  176.994 test_acc:  94.464\n",
      "epoch:  2507 train_loss:  170.962 train_acc:  94.249 test_loss:  201.393 test_acc:  93.804\n",
      "epoch:  2508 train_loss:  161.072 train_acc:  95.026 test_loss:  191.074 test_acc:  94.193\n",
      "epoch:  2509 train_loss:  156.509 train_acc:  94.786 test_loss:  195.449 test_acc:  94.009\n",
      "epoch:  2510 train_loss:  158.334 train_acc:  94.7 test_loss:  179.86 test_acc:  94.357\n",
      "epoch:  2511 train_loss:  199.968 train_acc:  93.742 test_loss:  233.087 test_acc:  93.456\n",
      "epoch:  2512 train_loss:  227.583 train_acc:  93.973 test_loss:  292.258 test_acc:  93.469\n",
      "epoch:  2513 train_loss:  177.127 train_acc:  94.331 test_loss:  219.203 test_acc:  93.783\n",
      "epoch:  2514 train_loss:  160.925 train_acc:  94.557 test_loss:  180.439 test_acc:  94.0\n",
      "epoch:  2515 train_loss:  170.243 train_acc:  94.538 test_loss:  192.981 test_acc:  94.192\n",
      "epoch:  2516 train_loss:  148.571 train_acc:  95.157 test_loss:  188.109 test_acc:  94.348\n",
      "epoch:  2517 train_loss:  146.752 train_acc:  95.13 test_loss:  186.679 test_acc:  94.482\n",
      "epoch:  2518 train_loss:  155.427 train_acc:  94.952 test_loss:  186.464 test_acc:  94.355\n",
      "epoch:  2519 train_loss:  151.912 train_acc:  94.912 test_loss:  170.869 test_acc:  94.556\n",
      "epoch:  2520 train_loss:  189.29 train_acc:  94.496 test_loss:  262.078 test_acc:  93.753\n",
      "epoch:  2521 train_loss:  169.456 train_acc:  94.914 test_loss:  228.084 test_acc:  94.064\n",
      "epoch:  2522 train_loss:  163.7 train_acc:  94.611 test_loss:  206.035 test_acc:  93.865\n",
      "epoch:  2523 train_loss:  154.438 train_acc:  94.921 test_loss:  175.875 test_acc:  94.337\n",
      "epoch:  2524 train_loss:  161.818 train_acc:  94.856 test_loss:  211.993 test_acc:  94.158\n",
      "epoch:  2525 train_loss:  151.092 train_acc:  94.79 test_loss:  182.107 test_acc:  94.395\n",
      "epoch:  2526 train_loss:  150.496 train_acc:  95.052 test_loss:  180.703 test_acc:  94.574\n",
      "epoch:  2527 train_loss:  155.427 train_acc:  94.853 test_loss:  179.502 test_acc:  94.249\n",
      "epoch:  2528 train_loss:  160.385 train_acc:  94.893 test_loss:  170.442 test_acc:  94.31\n",
      "epoch:  2529 train_loss:  168.997 train_acc:  94.375 test_loss:  238.909 test_acc:  93.597\n",
      "epoch:  2530 train_loss:  176.489 train_acc:  94.799 test_loss:  236.371 test_acc:  93.825\n",
      "epoch:  2531 train_loss:  179.338 train_acc:  94.479 test_loss:  224.623 test_acc:  93.781\n",
      "epoch:  2532 train_loss:  192.146 train_acc:  94.249 test_loss:  213.254 test_acc:  93.698\n",
      "epoch:  2533 train_loss:  155.676 train_acc:  95.046 test_loss:  202.654 test_acc:  94.244\n",
      "epoch:  2534 train_loss:  147.138 train_acc:  95.214 test_loss:  188.977 test_acc:  94.395\n",
      "epoch:  2535 train_loss:  152.582 train_acc:  95.071 test_loss:  171.538 test_acc:  94.373\n",
      "epoch:  2536 train_loss:  156.307 train_acc:  94.728 test_loss:  168.52 test_acc:  94.333\n",
      "epoch:  2537 train_loss:  179.17 train_acc:  94.482 test_loss:  238.01 test_acc:  93.622\n",
      "epoch:  2538 train_loss:  148.47 train_acc:  94.87 test_loss:  174.307 test_acc:  94.412\n",
      "epoch:  2539 train_loss:  171.768 train_acc:  94.924 test_loss:  185.944 test_acc:  94.201\n",
      "epoch:  2540 train_loss:  158.451 train_acc:  94.973 test_loss:  179.537 test_acc:  94.592\n",
      "epoch:  2541 train_loss:  174.244 train_acc:  94.245 test_loss:  192.256 test_acc:  93.917\n",
      "epoch:  2542 train_loss:  158.086 train_acc:  95.013 test_loss:  181.299 test_acc:  94.477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2543 train_loss:  176.824 train_acc:  94.737 test_loss:  265.596 test_acc:  93.604\n",
      "epoch:  2544 train_loss:  152.235 train_acc:  94.937 test_loss:  196.248 test_acc:  94.072\n",
      "epoch:  2545 train_loss:  145.632 train_acc:  95.202 test_loss:  194.261 test_acc:  94.382\n",
      "epoch:  2546 train_loss:  195.422 train_acc:  94.383 test_loss:  241.83 test_acc:  93.642\n",
      "epoch:  2547 train_loss:  172.941 train_acc:  94.86 test_loss:  204.679 test_acc:  94.152\n",
      "epoch:  2548 train_loss:  172.034 train_acc:  94.872 test_loss:  223.569 test_acc:  94.074\n",
      "epoch:  2549 train_loss:  155.554 train_acc:  94.81 test_loss:  208.56 test_acc:  93.877\n",
      "epoch:  2550 train_loss:  150.965 train_acc:  95.042 test_loss:  180.224 test_acc:  94.573\n",
      "epoch:  2551 train_loss:  193.341 train_acc:  94.461 test_loss:  292.359 test_acc:  93.018\n",
      "epoch:  2552 train_loss:  148.598 train_acc:  94.862 test_loss:  171.36 test_acc:  94.464\n",
      "epoch:  2553 train_loss:  184.096 train_acc:  94.704 test_loss:  221.785 test_acc:  93.74\n",
      "epoch:  2554 train_loss:  189.372 train_acc:  94.091 test_loss:  214.392 test_acc:  93.668\n",
      "epoch:  2555 train_loss:  181.421 train_acc:  94.708 test_loss:  206.0 test_acc:  93.746\n",
      "epoch:  2556 train_loss:  181.506 train_acc:  94.833 test_loss:  256.592 test_acc:  93.751\n",
      "epoch:  2557 train_loss:  168.784 train_acc:  94.233 test_loss:  188.242 test_acc:  93.971\n",
      "epoch:  2558 train_loss:  169.684 train_acc:  94.656 test_loss:  237.279 test_acc:  94.131\n",
      "epoch:  2559 train_loss:  155.279 train_acc:  95.177 test_loss:  225.413 test_acc:  94.22\n",
      "epoch:  2560 train_loss:  186.217 train_acc:  94.501 test_loss:  267.725 test_acc:  93.475\n",
      "epoch:  2561 train_loss:  176.668 train_acc:  94.758 test_loss:  246.782 test_acc:  93.804\n",
      "epoch:  2562 train_loss:  217.231 train_acc:  93.379 test_loss:  281.741 test_acc:  92.788\n",
      "epoch:  2563 train_loss:  174.488 train_acc:  94.266 test_loss:  189.323 test_acc:  94.204\n",
      "epoch:  2564 train_loss:  153.283 train_acc:  94.985 test_loss:  190.59 test_acc:  94.078\n",
      "epoch:  2565 train_loss:  170.841 train_acc:  94.79 test_loss:  220.342 test_acc:  93.764\n",
      "epoch:  2566 train_loss:  163.059 train_acc:  94.653 test_loss:  185.634 test_acc:  94.178\n",
      "epoch:  2567 train_loss:  157.232 train_acc:  94.885 test_loss:  161.146 test_acc:  94.445\n",
      "min loss:2567\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(230.8347)]\n",
      "1    [tensor(88.8594)]   [tensor(80.9138)]\n",
      "2   [tensor(162.9521)]  [tensor(158.4175)]\n",
      "3   [tensor(185.9385)]  [tensor(206.8000)]\n",
      "4   [tensor(212.8320)]  [tensor(204.4965)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(246.8539)]\n",
      "61  [tensor(127.2646)]  [tensor(128.9318)]\n",
      "62   [tensor(84.7217)]   [tensor(81.9341)]\n",
      "63  [tensor(104.8945)]  [tensor(126.0444)]\n",
      "64  [tensor(156.6016)]  [tensor(163.9644)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  2568 train_loss:  149.39 train_acc:  95.205 test_loss:  203.451 test_acc:  94.487\n",
      "epoch:  2569 train_loss:  149.832 train_acc:  94.855 test_loss:  187.709 test_acc:  94.039\n",
      "epoch:  2570 train_loss:  161.06 train_acc:  94.655 test_loss:  190.225 test_acc:  94.005\n",
      "epoch:  2571 train_loss:  152.352 train_acc:  94.785 test_loss:  170.298 test_acc:  94.513\n",
      "epoch:  2572 train_loss:  189.742 train_acc:  93.788 test_loss:  244.598 test_acc:  93.501\n",
      "epoch:  2573 train_loss:  160.288 train_acc:  94.744 test_loss:  206.773 test_acc:  94.135\n",
      "epoch:  2574 train_loss:  165.624 train_acc:  94.524 test_loss:  204.802 test_acc:  93.929\n",
      "epoch:  2575 train_loss:  171.822 train_acc:  94.546 test_loss:  234.384 test_acc:  93.806\n",
      "epoch:  2576 train_loss:  145.617 train_acc:  94.985 test_loss:  177.745 test_acc:  94.266\n",
      "epoch:  2577 train_loss:  174.285 train_acc:  94.793 test_loss:  238.298 test_acc:  93.956\n",
      "epoch:  2578 train_loss:  152.634 train_acc:  94.909 test_loss:  167.862 test_acc:  94.554\n",
      "epoch:  2579 train_loss:  164.518 train_acc:  94.907 test_loss:  190.423 test_acc:  94.214\n",
      "epoch:  2580 train_loss:  165.96 train_acc:  94.606 test_loss:  203.05 test_acc:  94.148\n",
      "epoch:  2581 train_loss:  195.57 train_acc:  93.747 test_loss:  206.882 test_acc:  93.281\n",
      "epoch:  2582 train_loss:  181.262 train_acc:  94.792 test_loss:  242.58 test_acc:  93.751\n",
      "epoch:  2583 train_loss:  150.228 train_acc:  94.912 test_loss:  192.565 test_acc:  94.216\n",
      "epoch:  2584 train_loss:  180.213 train_acc:  94.652 test_loss:  214.714 test_acc:  93.885\n",
      "epoch:  2585 train_loss:  167.635 train_acc:  94.109 test_loss:  193.997 test_acc:  93.721\n",
      "epoch:  2586 train_loss:  209.754 train_acc:  93.976 test_loss:  271.155 test_acc:  93.072\n",
      "epoch:  2587 train_loss:  168.641 train_acc:  94.813 test_loss:  220.516 test_acc:  93.965\n",
      "epoch:  2588 train_loss:  166.156 train_acc:  94.358 test_loss:  194.01 test_acc:  93.818\n",
      "epoch:  2589 train_loss:  167.004 train_acc:  94.653 test_loss:  183.933 test_acc:  94.02\n",
      "epoch:  2590 train_loss:  160.818 train_acc:  94.751 test_loss:  188.62 test_acc:  94.18\n",
      "epoch:  2591 train_loss:  174.211 train_acc:  94.578 test_loss:  222.319 test_acc:  93.855\n",
      "epoch:  2592 train_loss:  153.172 train_acc:  94.915 test_loss:  198.978 test_acc:  94.194\n",
      "epoch:  2593 train_loss:  169.978 train_acc:  95.0 test_loss:  235.281 test_acc:  93.987\n",
      "epoch:  2594 train_loss:  154.801 train_acc:  94.755 test_loss:  187.004 test_acc:  94.017\n",
      "epoch:  2595 train_loss:  175.781 train_acc:  94.286 test_loss:  244.331 test_acc:  93.365\n",
      "epoch:  2596 train_loss:  191.115 train_acc:  93.999 test_loss:  241.092 test_acc:  93.62\n",
      "epoch:  2597 train_loss:  181.507 train_acc:  94.038 test_loss:  244.768 test_acc:  93.283\n",
      "epoch:  2598 train_loss:  149.729 train_acc:  94.93 test_loss:  176.511 test_acc:  94.065\n",
      "epoch:  2599 train_loss:  181.416 train_acc:  94.68 test_loss:  224.765 test_acc:  94.099\n",
      "epoch:  2600 train_loss:  186.979 train_acc:  93.825 test_loss:  219.513 test_acc:  93.477\n",
      "epoch:  2601 train_loss:  149.791 train_acc:  94.896 test_loss:  195.406 test_acc:  94.047\n",
      "epoch:  2602 train_loss:  205.78 train_acc:  94.259 test_loss:  257.903 test_acc:  93.524\n",
      "epoch:  2603 train_loss:  182.575 train_acc:  94.219 test_loss:  215.368 test_acc:  93.669\n",
      "epoch:  2604 train_loss:  208.932 train_acc:  94.583 test_loss:  241.315 test_acc:  93.612\n",
      "epoch:  2605 train_loss:  177.293 train_acc:  94.163 test_loss:  229.954 test_acc:  93.429\n",
      "epoch:  2606 train_loss:  196.497 train_acc:  93.93 test_loss:  214.039 test_acc:  93.535\n",
      "epoch:  2607 train_loss:  153.457 train_acc:  94.808 test_loss:  195.409 test_acc:  94.361\n",
      "epoch:  2608 train_loss:  162.815 train_acc:  94.656 test_loss:  193.373 test_acc:  94.41\n",
      "epoch:  2609 train_loss:  164.172 train_acc:  94.793 test_loss:  200.631 test_acc:  94.086\n",
      "epoch:  2610 train_loss:  166.425 train_acc:  94.551 test_loss:  222.095 test_acc:  93.932\n",
      "epoch:  2611 train_loss:  167.462 train_acc:  94.853 test_loss:  221.879 test_acc:  94.042\n",
      "epoch:  2612 train_loss:  176.003 train_acc:  94.711 test_loss:  221.696 test_acc:  94.009\n",
      "epoch:  2613 train_loss:  235.003 train_acc:  93.669 test_loss:  338.335 test_acc:  93.241\n",
      "epoch:  2614 train_loss:  159.072 train_acc:  95.062 test_loss:  196.308 test_acc:  94.252\n",
      "epoch:  2615 train_loss:  151.141 train_acc:  95.031 test_loss:  190.538 test_acc:  94.289\n",
      "epoch:  2616 train_loss:  149.75 train_acc:  95.062 test_loss:  182.654 test_acc:  94.415\n",
      "epoch:  2617 train_loss:  165.816 train_acc:  95.044 test_loss:  233.339 test_acc:  94.13\n",
      "epoch:  2618 train_loss:  154.959 train_acc:  95.032 test_loss:  207.791 test_acc:  94.383\n",
      "epoch:  2619 train_loss:  148.625 train_acc:  94.878 test_loss:  183.215 test_acc:  94.2\n",
      "epoch:  2620 train_loss:  159.278 train_acc:  94.662 test_loss:  191.899 test_acc:  94.154\n",
      "epoch:  2621 train_loss:  177.11 train_acc:  94.58 test_loss:  227.325 test_acc:  93.959\n",
      "epoch:  2622 train_loss:  163.54 train_acc:  94.312 test_loss:  193.62 test_acc:  93.87\n",
      "epoch:  2623 train_loss:  148.189 train_acc:  95.04 test_loss:  193.352 test_acc:  94.292\n",
      "epoch:  2624 train_loss:  145.461 train_acc:  95.168 test_loss:  176.067 test_acc:  94.692\n",
      "max acc epoch：2624        max acc：94.692\n",
      "epoch:  2625 train_loss:  173.184 train_acc:  94.238 test_loss:  223.592 test_acc:  93.78\n",
      "epoch:  2626 train_loss:  148.746 train_acc:  95.03 test_loss:  170.333 test_acc:  94.607\n",
      "epoch:  2627 train_loss:  149.43 train_acc:  95.054 test_loss:  194.673 test_acc:  94.285\n",
      "epoch:  2628 train_loss:  148.7 train_acc:  95.012 test_loss:  193.115 test_acc:  94.347\n",
      "epoch:  2629 train_loss:  144.905 train_acc:  95.167 test_loss:  165.735 test_acc:  94.563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2630 train_loss:  167.365 train_acc:  94.795 test_loss:  210.054 test_acc:  93.724\n",
      "epoch:  2631 train_loss:  162.598 train_acc:  95.021 test_loss:  201.197 test_acc:  94.085\n",
      "epoch:  2632 train_loss:  156.971 train_acc:  95.114 test_loss:  191.926 test_acc:  94.282\n",
      "epoch:  2633 train_loss:  173.222 train_acc:  94.337 test_loss:  186.887 test_acc:  93.953\n",
      "epoch:  2634 train_loss:  207.408 train_acc:  93.225 test_loss:  231.127 test_acc:  93.199\n",
      "epoch:  2635 train_loss:  253.977 train_acc:  93.731 test_loss:  303.155 test_acc:  92.937\n",
      "epoch:  2636 train_loss:  162.614 train_acc:  94.695 test_loss:  183.161 test_acc:  94.503\n",
      "epoch:  2637 train_loss:  152.274 train_acc:  95.011 test_loss:  206.997 test_acc:  94.105\n",
      "epoch:  2638 train_loss:  152.048 train_acc:  95.134 test_loss:  202.866 test_acc:  94.5\n",
      "epoch:  2639 train_loss:  148.883 train_acc:  95.229 test_loss:  185.433 test_acc:  94.432\n",
      "epoch:  2640 train_loss:  195.733 train_acc:  94.226 test_loss:  225.67 test_acc:  93.764\n",
      "epoch:  2641 train_loss:  197.709 train_acc:  94.549 test_loss:  228.042 test_acc:  93.941\n",
      "epoch:  2642 train_loss:  163.137 train_acc:  95.014 test_loss:  247.304 test_acc:  93.812\n",
      "epoch:  2643 train_loss:  168.891 train_acc:  94.78 test_loss:  208.06 test_acc:  94.178\n",
      "epoch:  2644 train_loss:  196.712 train_acc:  94.115 test_loss:  231.043 test_acc:  93.632\n",
      "epoch:  2645 train_loss:  178.906 train_acc:  94.852 test_loss:  219.936 test_acc:  93.983\n",
      "epoch:  2646 train_loss:  187.988 train_acc:  94.329 test_loss:  245.192 test_acc:  93.553\n",
      "epoch:  2647 train_loss:  156.233 train_acc:  94.748 test_loss:  213.556 test_acc:  94.13\n",
      "epoch:  2648 train_loss:  152.556 train_acc:  94.937 test_loss:  209.623 test_acc:  94.131\n",
      "epoch:  2649 train_loss:  162.789 train_acc:  94.439 test_loss:  198.599 test_acc:  94.07\n",
      "epoch:  2650 train_loss:  152.424 train_acc:  95.185 test_loss:  184.928 test_acc:  94.393\n",
      "epoch:  2651 train_loss:  177.239 train_acc:  94.654 test_loss:  206.987 test_acc:  94.099\n",
      "epoch:  2652 train_loss:  159.217 train_acc:  94.695 test_loss:  190.524 test_acc:  94.177\n",
      "epoch:  2653 train_loss:  173.244 train_acc:  94.453 test_loss:  183.053 test_acc:  94.423\n",
      "epoch:  2654 train_loss:  152.699 train_acc:  94.69 test_loss:  168.458 test_acc:  94.317\n",
      "epoch:  2655 train_loss:  159.577 train_acc:  94.435 test_loss:  175.226 test_acc:  94.045\n",
      "epoch:  2656 train_loss:  156.171 train_acc:  94.629 test_loss:  173.958 test_acc:  94.237\n",
      "epoch:  2657 train_loss:  173.933 train_acc:  94.433 test_loss:  217.552 test_acc:  93.744\n",
      "epoch:  2658 train_loss:  171.061 train_acc:  94.159 test_loss:  195.059 test_acc:  93.709\n",
      "epoch:  2659 train_loss:  154.141 train_acc:  95.026 test_loss:  188.96 test_acc:  94.107\n",
      "epoch:  2660 train_loss:  153.929 train_acc:  95.074 test_loss:  211.825 test_acc:  94.213\n",
      "epoch:  2661 train_loss:  160.338 train_acc:  94.665 test_loss:  209.925 test_acc:  94.079\n",
      "epoch:  2662 train_loss:  169.052 train_acc:  94.865 test_loss:  228.523 test_acc:  93.694\n",
      "epoch:  2663 train_loss:  155.906 train_acc:  94.861 test_loss:  195.756 test_acc:  93.947\n",
      "epoch:  2664 train_loss:  160.422 train_acc:  94.444 test_loss:  200.004 test_acc:  93.704\n",
      "epoch:  2665 train_loss:  151.785 train_acc:  94.952 test_loss:  183.641 test_acc:  94.433\n",
      "epoch:  2666 train_loss:  190.166 train_acc:  94.522 test_loss:  246.851 test_acc:  93.733\n",
      "epoch:  2667 train_loss:  153.877 train_acc:  94.99 test_loss:  192.422 test_acc:  94.248\n",
      "epoch:  2668 train_loss:  149.448 train_acc:  94.728 test_loss:  161.122 test_acc:  94.34\n",
      "min loss:2668\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(235.6525)]\n",
      "1    [tensor(88.8594)]   [tensor(83.8347)]\n",
      "2   [tensor(162.9521)]  [tensor(165.3762)]\n",
      "3   [tensor(185.9385)]  [tensor(205.9278)]\n",
      "4   [tensor(212.8320)]  [tensor(214.7163)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(252.5657)]\n",
      "61  [tensor(127.2646)]  [tensor(129.5275)]\n",
      "62   [tensor(84.7217)]   [tensor(85.9687)]\n",
      "63  [tensor(104.8945)]  [tensor(129.6935)]\n",
      "64  [tensor(156.6016)]  [tensor(169.3217)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  2669 train_loss:  211.997 train_acc:  93.869 test_loss:  292.766 test_acc:  93.637\n",
      "epoch:  2670 train_loss:  164.863 train_acc:  94.975 test_loss:  241.991 test_acc:  94.213\n",
      "epoch:  2671 train_loss:  163.987 train_acc:  94.762 test_loss:  219.533 test_acc:  94.232\n",
      "epoch:  2672 train_loss:  148.903 train_acc:  95.074 test_loss:  174.581 test_acc:  94.648\n",
      "epoch:  2673 train_loss:  142.412 train_acc:  95.324 test_loss:  185.295 test_acc:  94.674\n",
      "epoch:  2674 train_loss:  169.194 train_acc:  94.413 test_loss:  203.633 test_acc:  93.996\n",
      "epoch:  2675 train_loss:  155.493 train_acc:  95.1 test_loss:  210.526 test_acc:  94.228\n",
      "epoch:  2676 train_loss:  226.231 train_acc:  93.216 test_loss:  250.304 test_acc:  93.218\n",
      "epoch:  2677 train_loss:  167.995 train_acc:  95.039 test_loss:  215.891 test_acc:  94.086\n",
      "epoch:  2678 train_loss:  154.47 train_acc:  94.87 test_loss:  185.074 test_acc:  94.392\n",
      "epoch:  2679 train_loss:  163.854 train_acc:  94.573 test_loss:  193.484 test_acc:  94.26\n",
      "epoch:  2680 train_loss:  221.779 train_acc:  94.193 test_loss:  264.024 test_acc:  93.384\n",
      "epoch:  2681 train_loss:  195.17 train_acc:  94.165 test_loss:  223.968 test_acc:  93.598\n",
      "epoch:  2682 train_loss:  160.34 train_acc:  94.853 test_loss:  187.505 test_acc:  94.2\n",
      "epoch:  2683 train_loss:  170.677 train_acc:  94.93 test_loss:  214.819 test_acc:  94.02\n",
      "epoch:  2684 train_loss:  141.576 train_acc:  95.253 test_loss:  169.431 test_acc:  94.613\n",
      "epoch:  2685 train_loss:  146.061 train_acc:  95.133 test_loss:  173.212 test_acc:  94.548\n",
      "epoch:  2686 train_loss:  145.191 train_acc:  95.309 test_loss:  191.957 test_acc:  94.535\n",
      "epoch:  2687 train_loss:  155.111 train_acc:  95.174 test_loss:  193.365 test_acc:  94.326\n",
      "epoch:  2688 train_loss:  150.734 train_acc:  95.172 test_loss:  200.936 test_acc:  94.067\n",
      "epoch:  2689 train_loss:  164.814 train_acc:  94.464 test_loss:  185.876 test_acc:  93.989\n",
      "epoch:  2690 train_loss:  151.486 train_acc:  94.959 test_loss:  189.297 test_acc:  94.346\n",
      "epoch:  2691 train_loss:  164.132 train_acc:  94.945 test_loss:  207.143 test_acc:  93.955\n",
      "epoch:  2692 train_loss:  176.878 train_acc:  94.565 test_loss:  196.63 test_acc:  93.918\n",
      "epoch:  2693 train_loss:  209.272 train_acc:  93.341 test_loss:  232.386 test_acc:  93.061\n",
      "epoch:  2694 train_loss:  171.943 train_acc:  94.423 test_loss:  193.28 test_acc:  93.892\n",
      "epoch:  2695 train_loss:  146.445 train_acc:  95.062 test_loss:  172.925 test_acc:  94.49\n",
      "epoch:  2696 train_loss:  160.824 train_acc:  94.992 test_loss:  216.992 test_acc:  94.113\n",
      "epoch:  2697 train_loss:  173.29 train_acc:  94.397 test_loss:  206.445 test_acc:  93.898\n",
      "epoch:  2698 train_loss:  173.239 train_acc:  94.303 test_loss:  210.808 test_acc:  93.559\n",
      "epoch:  2699 train_loss:  145.956 train_acc:  95.092 test_loss:  179.634 test_acc:  94.57\n",
      "epoch:  2700 train_loss:  148.867 train_acc:  95.029 test_loss:  191.204 test_acc:  94.623\n",
      "epoch:  2701 train_loss:  218.109 train_acc:  93.278 test_loss:  232.354 test_acc:  93.031\n",
      "epoch:  2702 train_loss:  156.474 train_acc:  94.774 test_loss:  192.269 test_acc:  94.087\n",
      "epoch:  2703 train_loss:  159.985 train_acc:  95.014 test_loss:  207.044 test_acc:  94.17\n",
      "epoch:  2704 train_loss:  151.823 train_acc:  95.094 test_loss:  168.794 test_acc:  94.539\n",
      "epoch:  2705 train_loss:  153.384 train_acc:  95.059 test_loss:  185.339 test_acc:  94.353\n",
      "epoch:  2706 train_loss:  156.17 train_acc:  94.573 test_loss:  196.187 test_acc:  94.039\n",
      "epoch:  2707 train_loss:  184.635 train_acc:  94.442 test_loss:  246.576 test_acc:  93.813\n",
      "epoch:  2708 train_loss:  152.954 train_acc:  94.904 test_loss:  194.343 test_acc:  94.463\n",
      "epoch:  2709 train_loss:  160.234 train_acc:  94.977 test_loss:  190.292 test_acc:  94.622\n",
      "epoch:  2710 train_loss:  210.208 train_acc:  94.595 test_loss:  241.316 test_acc:  93.901\n",
      "epoch:  2711 train_loss:  156.578 train_acc:  94.739 test_loss:  201.358 test_acc:  94.148\n",
      "epoch:  2712 train_loss:  161.177 train_acc:  94.372 test_loss:  196.37 test_acc:  93.673\n",
      "epoch:  2713 train_loss:  156.631 train_acc:  94.951 test_loss:  170.989 test_acc:  94.667\n",
      "epoch:  2714 train_loss:  155.77 train_acc:  94.799 test_loss:  187.124 test_acc:  94.156\n",
      "epoch:  2715 train_loss:  212.635 train_acc:  94.398 test_loss:  277.975 test_acc:  93.324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2716 train_loss:  165.496 train_acc:  94.813 test_loss:  210.441 test_acc:  94.022\n",
      "epoch:  2717 train_loss:  151.944 train_acc:  94.846 test_loss:  180.193 test_acc:  94.487\n",
      "epoch:  2718 train_loss:  149.263 train_acc:  95.135 test_loss:  172.24 test_acc:  94.45\n",
      "epoch:  2719 train_loss:  155.665 train_acc:  94.826 test_loss:  169.874 test_acc:  94.531\n",
      "epoch:  2720 train_loss:  158.09 train_acc:  94.927 test_loss:  220.554 test_acc:  93.969\n",
      "epoch:  2721 train_loss:  147.564 train_acc:  95.115 test_loss:  187.016 test_acc:  94.509\n",
      "epoch:  2722 train_loss:  171.388 train_acc:  94.235 test_loss:  208.717 test_acc:  93.863\n",
      "epoch:  2723 train_loss:  142.053 train_acc:  95.207 test_loss:  169.266 test_acc:  94.674\n",
      "epoch:  2724 train_loss:  161.378 train_acc:  94.959 test_loss:  221.596 test_acc:  94.18\n",
      "epoch:  2725 train_loss:  179.277 train_acc:  94.644 test_loss:  231.872 test_acc:  93.926\n",
      "epoch:  2726 train_loss:  162.982 train_acc:  94.532 test_loss:  174.255 test_acc:  94.278\n",
      "epoch:  2727 train_loss:  163.28 train_acc:  95.038 test_loss:  203.291 test_acc:  94.111\n",
      "epoch:  2728 train_loss:  149.748 train_acc:  95.104 test_loss:  184.593 test_acc:  94.485\n",
      "epoch:  2729 train_loss:  147.78 train_acc:  95.265 test_loss:  186.502 test_acc:  94.371\n",
      "epoch:  2730 train_loss:  151.181 train_acc:  94.79 test_loss:  182.272 test_acc:  94.226\n",
      "epoch:  2731 train_loss:  148.321 train_acc:  95.176 test_loss:  178.567 test_acc:  94.614\n",
      "epoch:  2732 train_loss:  144.878 train_acc:  95.117 test_loss:  178.199 test_acc:  94.422\n",
      "epoch:  2733 train_loss:  152.763 train_acc:  95.019 test_loss:  173.049 test_acc:  94.48\n",
      "epoch:  2734 train_loss:  148.905 train_acc:  95.061 test_loss:  175.525 test_acc:  94.488\n",
      "epoch:  2735 train_loss:  156.231 train_acc:  94.562 test_loss:  204.935 test_acc:  93.916\n",
      "epoch:  2736 train_loss:  145.475 train_acc:  95.074 test_loss:  177.822 test_acc:  94.495\n",
      "epoch:  2737 train_loss:  189.082 train_acc:  93.885 test_loss:  196.929 test_acc:  93.855\n",
      "epoch:  2738 train_loss:  179.272 train_acc:  94.805 test_loss:  227.44 test_acc:  93.916\n",
      "epoch:  2739 train_loss:  159.306 train_acc:  95.033 test_loss:  190.872 test_acc:  94.476\n",
      "epoch:  2740 train_loss:  156.327 train_acc:  94.774 test_loss:  198.401 test_acc:  94.044\n",
      "epoch:  2741 train_loss:  148.148 train_acc:  95.209 test_loss:  185.774 test_acc:  94.443\n",
      "epoch:  2742 train_loss:  176.688 train_acc:  94.41 test_loss:  217.168 test_acc:  93.677\n",
      "epoch:  2743 train_loss:  151.675 train_acc:  94.781 test_loss:  167.143 test_acc:  94.444\n",
      "epoch:  2744 train_loss:  141.535 train_acc:  95.151 test_loss:  173.973 test_acc:  94.739\n",
      "max acc epoch：2744        max acc：94.739\n",
      "epoch:  2745 train_loss:  186.852 train_acc:  94.408 test_loss:  260.077 test_acc:  93.939\n",
      "epoch:  2746 train_loss:  154.225 train_acc:  94.468 test_loss:  183.751 test_acc:  94.011\n",
      "epoch:  2747 train_loss:  159.084 train_acc:  94.22 test_loss:  193.596 test_acc:  94.015\n",
      "epoch:  2748 train_loss:  219.629 train_acc:  93.532 test_loss:  254.203 test_acc:  93.256\n",
      "epoch:  2749 train_loss:  173.545 train_acc:  94.435 test_loss:  222.034 test_acc:  94.045\n",
      "epoch:  2750 train_loss:  147.146 train_acc:  95.148 test_loss:  180.12 test_acc:  94.67\n",
      "epoch:  2751 train_loss:  155.802 train_acc:  94.891 test_loss:  181.492 test_acc:  94.177\n",
      "epoch:  2752 train_loss:  146.317 train_acc:  95.124 test_loss:  190.945 test_acc:  94.49\n",
      "epoch:  2753 train_loss:  168.657 train_acc:  95.011 test_loss:  200.02 test_acc:  94.174\n",
      "epoch:  2754 train_loss:  178.564 train_acc:  94.691 test_loss:  283.586 test_acc:  93.634\n",
      "epoch:  2755 train_loss:  148.502 train_acc:  95.207 test_loss:  206.852 test_acc:  94.178\n",
      "epoch:  2756 train_loss:  166.161 train_acc:  94.831 test_loss:  230.222 test_acc:  94.291\n",
      "epoch:  2757 train_loss:  145.703 train_acc:  95.224 test_loss:  173.639 test_acc:  94.514\n",
      "epoch:  2758 train_loss:  156.35 train_acc:  95.166 test_loss:  201.548 test_acc:  94.229\n",
      "epoch:  2759 train_loss:  176.241 train_acc:  94.461 test_loss:  229.73 test_acc:  94.114\n",
      "epoch:  2760 train_loss:  164.103 train_acc:  94.811 test_loss:  163.024 test_acc:  94.684\n",
      "epoch:  2761 train_loss:  161.32 train_acc:  94.663 test_loss:  201.284 test_acc:  94.222\n",
      "epoch:  2762 train_loss:  164.1 train_acc:  94.955 test_loss:  214.545 test_acc:  93.762\n",
      "epoch:  2763 train_loss:  170.321 train_acc:  94.911 test_loss:  244.264 test_acc:  93.786\n",
      "epoch:  2764 train_loss:  144.231 train_acc:  95.126 test_loss:  176.071 test_acc:  94.593\n",
      "epoch:  2765 train_loss:  149.15 train_acc:  94.843 test_loss:  179.977 test_acc:  94.383\n",
      "epoch:  2766 train_loss:  164.535 train_acc:  95.019 test_loss:  206.558 test_acc:  94.211\n",
      "epoch:  2767 train_loss:  156.07 train_acc:  94.811 test_loss:  204.831 test_acc:  93.983\n",
      "epoch:  2768 train_loss:  154.708 train_acc:  94.908 test_loss:  183.65 test_acc:  94.333\n",
      "epoch:  2769 train_loss:  161.087 train_acc:  94.629 test_loss:  194.621 test_acc:  94.437\n",
      "epoch:  2770 train_loss:  167.947 train_acc:  94.837 test_loss:  230.177 test_acc:  94.091\n",
      "epoch:  2771 train_loss:  154.869 train_acc:  94.994 test_loss:  187.265 test_acc:  94.308\n",
      "epoch:  2772 train_loss:  147.639 train_acc:  94.945 test_loss:  166.167 test_acc:  94.743\n",
      "max acc epoch：2772        max acc：94.743\n",
      "epoch:  2773 train_loss:  153.02 train_acc:  95.123 test_loss:  181.99 test_acc:  94.351\n",
      "epoch:  2774 train_loss:  166.385 train_acc:  94.98 test_loss:  196.528 test_acc:  94.383\n",
      "epoch:  2775 train_loss:  164.98 train_acc:  94.823 test_loss:  217.974 test_acc:  93.952\n",
      "epoch:  2776 train_loss:  158.001 train_acc:  95.023 test_loss:  173.226 test_acc:  94.415\n",
      "epoch:  2777 train_loss:  170.845 train_acc:  94.434 test_loss:  216.116 test_acc:  93.669\n",
      "epoch:  2778 train_loss:  177.922 train_acc:  94.376 test_loss:  236.562 test_acc:  93.853\n",
      "epoch:  2779 train_loss:  175.308 train_acc:  94.807 test_loss:  204.65 test_acc:  94.154\n",
      "epoch:  2780 train_loss:  205.173 train_acc:  93.862 test_loss:  290.678 test_acc:  92.761\n",
      "epoch:  2781 train_loss:  150.002 train_acc:  95.048 test_loss:  182.383 test_acc:  94.322\n",
      "epoch:  2782 train_loss:  149.624 train_acc:  95.197 test_loss:  183.23 test_acc:  94.687\n",
      "epoch:  2783 train_loss:  170.197 train_acc:  94.222 test_loss:  199.419 test_acc:  93.824\n",
      "epoch:  2784 train_loss:  199.351 train_acc:  93.396 test_loss:  202.59 test_acc:  93.393\n",
      "epoch:  2785 train_loss:  150.581 train_acc:  94.894 test_loss:  166.761 test_acc:  94.609\n",
      "epoch:  2786 train_loss:  176.599 train_acc:  94.54 test_loss:  183.088 test_acc:  94.26\n",
      "epoch:  2787 train_loss:  157.761 train_acc:  95.023 test_loss:  213.223 test_acc:  94.289\n",
      "epoch:  2788 train_loss:  157.622 train_acc:  94.934 test_loss:  214.996 test_acc:  94.132\n",
      "epoch:  2789 train_loss:  200.53 train_acc:  94.009 test_loss:  251.868 test_acc:  93.376\n",
      "epoch:  2790 train_loss:  147.955 train_acc:  95.097 test_loss:  196.983 test_acc:  94.162\n",
      "epoch:  2791 train_loss:  164.605 train_acc:  94.812 test_loss:  216.959 test_acc:  93.999\n",
      "epoch:  2792 train_loss:  212.914 train_acc:  94.225 test_loss:  350.961 test_acc:  93.059\n",
      "epoch:  2793 train_loss:  167.861 train_acc:  94.991 test_loss:  249.479 test_acc:  94.118\n",
      "epoch:  2794 train_loss:  158.481 train_acc:  95.007 test_loss:  229.886 test_acc:  93.758\n",
      "epoch:  2795 train_loss:  156.529 train_acc:  95.015 test_loss:  206.682 test_acc:  94.236\n",
      "epoch:  2796 train_loss:  197.606 train_acc:  94.451 test_loss:  270.428 test_acc:  93.164\n",
      "epoch:  2797 train_loss:  142.986 train_acc:  95.176 test_loss:  176.316 test_acc:  94.53\n",
      "epoch:  2798 train_loss:  145.338 train_acc:  95.079 test_loss:  175.271 test_acc:  94.478\n",
      "epoch:  2799 train_loss:  169.49 train_acc:  94.415 test_loss:  205.761 test_acc:  94.141\n",
      "epoch:  2800 train_loss:  172.243 train_acc:  94.638 test_loss:  182.403 test_acc:  94.247\n",
      "epoch:  2801 train_loss:  149.541 train_acc:  95.028 test_loss:  171.183 test_acc:  94.489\n",
      "epoch:  2802 train_loss:  143.925 train_acc:  95.086 test_loss:  175.62 test_acc:  94.477\n",
      "epoch:  2803 train_loss:  169.254 train_acc:  94.985 test_loss:  206.177 test_acc:  93.988\n",
      "epoch:  2804 train_loss:  146.173 train_acc:  95.278 test_loss:  203.285 test_acc:  94.279\n",
      "epoch:  2805 train_loss:  179.648 train_acc:  94.024 test_loss:  207.727 test_acc:  93.707\n",
      "epoch:  2806 train_loss:  149.822 train_acc:  94.961 test_loss:  197.97 test_acc:  94.192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2807 train_loss:  161.712 train_acc:  94.622 test_loss:  233.028 test_acc:  94.068\n",
      "epoch:  2808 train_loss:  200.827 train_acc:  93.429 test_loss:  229.532 test_acc:  93.192\n",
      "epoch:  2809 train_loss:  168.382 train_acc:  95.011 test_loss:  204.113 test_acc:  94.032\n",
      "epoch:  2810 train_loss:  166.544 train_acc:  94.604 test_loss:  229.54 test_acc:  94.072\n",
      "epoch:  2811 train_loss:  164.07 train_acc:  94.879 test_loss:  185.807 test_acc:  94.282\n",
      "epoch:  2812 train_loss:  179.077 train_acc:  94.74 test_loss:  233.696 test_acc:  93.749\n",
      "epoch:  2813 train_loss:  140.874 train_acc:  95.178 test_loss:  167.132 test_acc:  94.64\n",
      "epoch:  2814 train_loss:  155.437 train_acc:  94.709 test_loss:  208.871 test_acc:  94.137\n",
      "epoch:  2815 train_loss:  145.133 train_acc:  95.235 test_loss:  185.708 test_acc:  94.5\n",
      "epoch:  2816 train_loss:  155.545 train_acc:  95.085 test_loss:  222.981 test_acc:  94.034\n",
      "epoch:  2817 train_loss:  165.437 train_acc:  94.414 test_loss:  224.424 test_acc:  93.66\n",
      "epoch:  2818 train_loss:  153.281 train_acc:  95.042 test_loss:  191.438 test_acc:  94.605\n",
      "epoch:  2819 train_loss:  151.557 train_acc:  95.011 test_loss:  213.553 test_acc:  94.237\n",
      "epoch:  2820 train_loss:  184.26 train_acc:  94.148 test_loss:  195.914 test_acc:  93.897\n",
      "epoch:  2821 train_loss:  163.678 train_acc:  94.379 test_loss:  170.324 test_acc:  94.155\n",
      "epoch:  2822 train_loss:  169.167 train_acc:  94.229 test_loss:  190.898 test_acc:  93.956\n",
      "epoch:  2823 train_loss:  152.341 train_acc:  95.092 test_loss:  194.734 test_acc:  94.376\n",
      "epoch:  2824 train_loss:  146.878 train_acc:  95.179 test_loss:  182.74 test_acc:  94.397\n",
      "epoch:  2825 train_loss:  161.375 train_acc:  94.513 test_loss:  207.502 test_acc:  93.869\n",
      "epoch:  2826 train_loss:  175.402 train_acc:  94.473 test_loss:  185.01 test_acc:  94.094\n",
      "epoch:  2827 train_loss:  161.203 train_acc:  94.703 test_loss:  188.397 test_acc:  93.894\n",
      "epoch:  2828 train_loss:  181.578 train_acc:  94.266 test_loss:  193.724 test_acc:  93.943\n",
      "epoch:  2829 train_loss:  211.568 train_acc:  94.046 test_loss:  277.243 test_acc:  93.398\n",
      "epoch:  2830 train_loss:  145.998 train_acc:  95.104 test_loss:  188.496 test_acc:  94.387\n",
      "epoch:  2831 train_loss:  164.261 train_acc:  94.336 test_loss:  198.03 test_acc:  93.711\n",
      "epoch:  2832 train_loss:  156.419 train_acc:  94.847 test_loss:  194.517 test_acc:  94.357\n",
      "epoch:  2833 train_loss:  159.376 train_acc:  94.726 test_loss:  190.256 test_acc:  94.207\n",
      "epoch:  2834 train_loss:  153.137 train_acc:  94.667 test_loss:  172.229 test_acc:  94.408\n",
      "epoch:  2835 train_loss:  151.602 train_acc:  95.182 test_loss:  176.978 test_acc:  94.396\n",
      "epoch:  2836 train_loss:  179.555 train_acc:  94.802 test_loss:  287.461 test_acc:  93.373\n",
      "epoch:  2837 train_loss:  189.287 train_acc:  93.72 test_loss:  241.176 test_acc:  93.454\n",
      "epoch:  2838 train_loss:  152.144 train_acc:  95.25 test_loss:  203.591 test_acc:  94.408\n",
      "epoch:  2839 train_loss:  143.666 train_acc:  95.104 test_loss:  163.997 test_acc:  94.566\n",
      "epoch:  2840 train_loss:  165.614 train_acc:  95.052 test_loss:  215.068 test_acc:  94.073\n",
      "epoch:  2841 train_loss:  144.463 train_acc:  95.139 test_loss:  182.358 test_acc:  94.448\n",
      "epoch:  2842 train_loss:  153.155 train_acc:  94.914 test_loss:  197.76 test_acc:  94.287\n",
      "epoch:  2843 train_loss:  155.222 train_acc:  95.017 test_loss:  198.611 test_acc:  94.194\n",
      "epoch:  2844 train_loss:  154.387 train_acc:  94.947 test_loss:  186.994 test_acc:  94.297\n",
      "epoch:  2845 train_loss:  165.521 train_acc:  94.91 test_loss:  247.2 test_acc:  93.492\n",
      "epoch:  2846 train_loss:  173.174 train_acc:  94.765 test_loss:  210.768 test_acc:  93.824\n",
      "epoch:  2847 train_loss:  169.333 train_acc:  94.804 test_loss:  194.807 test_acc:  94.143\n",
      "epoch:  2848 train_loss:  171.67 train_acc:  94.62 test_loss:  180.967 test_acc:  94.405\n",
      "epoch:  2849 train_loss:  146.008 train_acc:  95.224 test_loss:  177.06 test_acc:  94.719\n",
      "epoch:  2850 train_loss:  150.319 train_acc:  95.259 test_loss:  191.775 test_acc:  94.397\n",
      "epoch:  2851 train_loss:  145.085 train_acc:  95.091 test_loss:  185.531 test_acc:  94.269\n",
      "epoch:  2852 train_loss:  164.614 train_acc:  94.501 test_loss:  200.541 test_acc:  94.055\n",
      "epoch:  2853 train_loss:  151.779 train_acc:  95.204 test_loss:  196.846 test_acc:  94.261\n",
      "epoch:  2854 train_loss:  146.894 train_acc:  95.145 test_loss:  204.451 test_acc:  94.461\n",
      "epoch:  2855 train_loss:  150.67 train_acc:  95.207 test_loss:  213.079 test_acc:  94.428\n",
      "epoch:  2856 train_loss:  155.807 train_acc:  94.748 test_loss:  191.243 test_acc:  94.132\n",
      "epoch:  2857 train_loss:  151.122 train_acc:  95.213 test_loss:  193.318 test_acc:  94.474\n",
      "epoch:  2858 train_loss:  140.332 train_acc:  95.196 test_loss:  166.738 test_acc:  94.659\n",
      "epoch:  2859 train_loss:  155.265 train_acc:  94.78 test_loss:  182.281 test_acc:  94.137\n",
      "epoch:  2860 train_loss:  148.165 train_acc:  95.121 test_loss:  190.515 test_acc:  94.268\n",
      "epoch:  2861 train_loss:  154.515 train_acc:  94.928 test_loss:  186.348 test_acc:  94.329\n",
      "epoch:  2862 train_loss:  142.867 train_acc:  95.104 test_loss:  170.21 test_acc:  94.48\n",
      "epoch:  2863 train_loss:  151.044 train_acc:  95.162 test_loss:  200.132 test_acc:  94.541\n",
      "epoch:  2864 train_loss:  179.095 train_acc:  94.594 test_loss:  262.272 test_acc:  93.559\n",
      "epoch:  2865 train_loss:  153.471 train_acc:  95.282 test_loss:  223.278 test_acc:  94.37\n",
      "epoch:  2866 train_loss:  151.506 train_acc:  95.01 test_loss:  171.005 test_acc:  94.576\n",
      "epoch:  2867 train_loss:  165.271 train_acc:  94.298 test_loss:  193.702 test_acc:  93.981\n",
      "epoch:  2868 train_loss:  205.151 train_acc:  94.192 test_loss:  259.386 test_acc:  93.46\n",
      "epoch:  2869 train_loss:  158.075 train_acc:  94.854 test_loss:  215.848 test_acc:  93.859\n",
      "epoch:  2870 train_loss:  141.349 train_acc:  95.287 test_loss:  174.16 test_acc:  94.787\n",
      "max acc epoch：2870        max acc：94.787\n",
      "epoch:  2871 train_loss:  158.425 train_acc:  94.499 test_loss:  175.767 test_acc:  94.299\n",
      "epoch:  2872 train_loss:  142.428 train_acc:  95.168 test_loss:  160.38 test_acc:  94.701\n",
      "min loss:2872\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(232.4860)]\n",
      "1    [tensor(88.8594)]   [tensor(81.3773)]\n",
      "2   [tensor(162.9521)]  [tensor(163.2371)]\n",
      "3   [tensor(185.9385)]  [tensor(203.4857)]\n",
      "4   [tensor(212.8320)]  [tensor(210.7724)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(255.7899)]\n",
      "61  [tensor(127.2646)]  [tensor(130.3883)]\n",
      "62   [tensor(84.7217)]   [tensor(83.4171)]\n",
      "63  [tensor(104.8945)]  [tensor(129.0090)]\n",
      "64  [tensor(156.6016)]  [tensor(166.7482)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  2873 train_loss:  150.438 train_acc:  94.877 test_loss:  190.437 test_acc:  94.315\n",
      "epoch:  2874 train_loss:  165.065 train_acc:  94.933 test_loss:  185.634 test_acc:  94.415\n",
      "epoch:  2875 train_loss:  163.931 train_acc:  95.019 test_loss:  218.764 test_acc:  94.114\n",
      "epoch:  2876 train_loss:  185.041 train_acc:  94.508 test_loss:  209.482 test_acc:  94.056\n",
      "epoch:  2877 train_loss:  186.722 train_acc:  94.697 test_loss:  284.746 test_acc:  93.463\n",
      "epoch:  2878 train_loss:  143.317 train_acc:  95.275 test_loss:  193.589 test_acc:  94.594\n",
      "epoch:  2879 train_loss:  147.592 train_acc:  95.178 test_loss:  178.14 test_acc:  94.532\n",
      "epoch:  2880 train_loss:  187.171 train_acc:  93.993 test_loss:  225.409 test_acc:  93.77\n",
      "epoch:  2881 train_loss:  160.004 train_acc:  95.099 test_loss:  209.668 test_acc:  94.254\n",
      "epoch:  2882 train_loss:  139.152 train_acc:  95.303 test_loss:  167.125 test_acc:  94.652\n",
      "epoch:  2883 train_loss:  143.936 train_acc:  95.232 test_loss:  173.526 test_acc:  94.586\n",
      "epoch:  2884 train_loss:  219.498 train_acc:  93.43 test_loss:  245.178 test_acc:  93.345\n",
      "epoch:  2885 train_loss:  185.469 train_acc:  93.893 test_loss:  205.79 test_acc:  93.546\n",
      "epoch:  2886 train_loss:  142.063 train_acc:  95.171 test_loss:  169.044 test_acc:  94.523\n",
      "epoch:  2887 train_loss:  150.703 train_acc:  94.77 test_loss:  170.076 test_acc:  94.586\n",
      "epoch:  2888 train_loss:  233.986 train_acc:  94.301 test_loss:  297.933 test_acc:  93.354\n",
      "epoch:  2889 train_loss:  193.998 train_acc:  94.491 test_loss:  270.224 test_acc:  93.768\n",
      "epoch:  2890 train_loss:  162.038 train_acc:  94.785 test_loss:  225.892 test_acc:  93.861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2891 train_loss:  141.946 train_acc:  95.254 test_loss:  178.728 test_acc:  94.415\n",
      "epoch:  2892 train_loss:  167.786 train_acc:  94.927 test_loss:  214.019 test_acc:  94.012\n",
      "epoch:  2893 train_loss:  153.699 train_acc:  95.022 test_loss:  214.536 test_acc:  93.963\n",
      "epoch:  2894 train_loss:  144.496 train_acc:  94.933 test_loss:  162.468 test_acc:  94.492\n",
      "epoch:  2895 train_loss:  149.744 train_acc:  95.289 test_loss:  189.105 test_acc:  94.464\n",
      "epoch:  2896 train_loss:  150.722 train_acc:  94.856 test_loss:  166.433 test_acc:  94.519\n",
      "epoch:  2897 train_loss:  152.916 train_acc:  94.779 test_loss:  193.966 test_acc:  94.169\n",
      "epoch:  2898 train_loss:  159.881 train_acc:  94.826 test_loss:  221.395 test_acc:  94.16\n",
      "epoch:  2899 train_loss:  151.717 train_acc:  95.13 test_loss:  194.984 test_acc:  94.395\n",
      "epoch:  2900 train_loss:  156.091 train_acc:  94.726 test_loss:  166.558 test_acc:  94.673\n",
      "epoch:  2901 train_loss:  183.001 train_acc:  94.735 test_loss:  218.746 test_acc:  93.896\n",
      "epoch:  2902 train_loss:  149.756 train_acc:  95.181 test_loss:  213.396 test_acc:  94.276\n",
      "epoch:  2903 train_loss:  142.279 train_acc:  95.096 test_loss:  179.947 test_acc:  94.499\n",
      "epoch:  2904 train_loss:  154.319 train_acc:  95.055 test_loss:  185.225 test_acc:  94.486\n",
      "epoch:  2905 train_loss:  160.761 train_acc:  94.89 test_loss:  187.384 test_acc:  94.147\n",
      "epoch:  2906 train_loss:  159.284 train_acc:  94.964 test_loss:  195.615 test_acc:  94.242\n",
      "epoch:  2907 train_loss:  145.144 train_acc:  95.136 test_loss:  187.377 test_acc:  94.298\n",
      "epoch:  2908 train_loss:  153.066 train_acc:  94.766 test_loss:  189.695 test_acc:  94.144\n",
      "epoch:  2909 train_loss:  196.77 train_acc:  94.587 test_loss:  261.681 test_acc:  93.713\n",
      "epoch:  2910 train_loss:  177.984 train_acc:  94.881 test_loss:  227.077 test_acc:  93.945\n",
      "epoch:  2911 train_loss:  141.035 train_acc:  95.152 test_loss:  166.03 test_acc:  94.71\n",
      "epoch:  2912 train_loss:  150.698 train_acc:  95.207 test_loss:  204.658 test_acc:  94.387\n",
      "epoch:  2913 train_loss:  153.91 train_acc:  95.224 test_loss:  202.883 test_acc:  94.634\n",
      "epoch:  2914 train_loss:  140.286 train_acc:  95.19 test_loss:  169.421 test_acc:  94.737\n",
      "epoch:  2915 train_loss:  141.733 train_acc:  95.271 test_loss:  180.138 test_acc:  94.541\n",
      "epoch:  2916 train_loss:  188.462 train_acc:  93.919 test_loss:  213.599 test_acc:  93.634\n",
      "epoch:  2917 train_loss:  141.53 train_acc:  95.277 test_loss:  174.976 test_acc:  94.702\n",
      "epoch:  2918 train_loss:  140.951 train_acc:  95.269 test_loss:  176.69 test_acc:  94.708\n",
      "epoch:  2919 train_loss:  150.545 train_acc:  95.159 test_loss:  199.052 test_acc:  94.477\n",
      "epoch:  2920 train_loss:  153.716 train_acc:  95.186 test_loss:  187.523 test_acc:  94.53\n",
      "epoch:  2921 train_loss:  155.853 train_acc:  94.912 test_loss:  192.938 test_acc:  94.261\n",
      "epoch:  2922 train_loss:  168.177 train_acc:  94.719 test_loss:  219.656 test_acc:  94.1\n",
      "epoch:  2923 train_loss:  154.299 train_acc:  94.606 test_loss:  212.627 test_acc:  93.97\n",
      "epoch:  2924 train_loss:  180.734 train_acc:  93.871 test_loss:  199.36 test_acc:  93.516\n",
      "epoch:  2925 train_loss:  166.923 train_acc:  94.915 test_loss:  211.493 test_acc:  93.993\n",
      "epoch:  2926 train_loss:  341.786 train_acc:  92.214 test_loss:  405.785 test_acc:  91.9\n",
      "epoch:  2927 train_loss:  149.266 train_acc:  94.78 test_loss:  170.929 test_acc:  94.503\n",
      "epoch:  2928 train_loss:  149.72 train_acc:  94.693 test_loss:  186.464 test_acc:  93.978\n",
      "epoch:  2929 train_loss:  143.458 train_acc:  95.11 test_loss:  189.388 test_acc:  94.394\n",
      "epoch:  2930 train_loss:  144.528 train_acc:  95.111 test_loss:  163.837 test_acc:  94.741\n",
      "epoch:  2931 train_loss:  156.478 train_acc:  94.973 test_loss:  206.073 test_acc:  94.162\n",
      "epoch:  2932 train_loss:  140.883 train_acc:  95.295 test_loss:  189.435 test_acc:  94.527\n",
      "epoch:  2933 train_loss:  162.774 train_acc:  94.745 test_loss:  211.676 test_acc:  94.026\n",
      "epoch:  2934 train_loss:  167.56 train_acc:  94.38 test_loss:  207.116 test_acc:  93.896\n",
      "epoch:  2935 train_loss:  159.909 train_acc:  95.022 test_loss:  213.069 test_acc:  94.183\n",
      "epoch:  2936 train_loss:  145.06 train_acc:  94.987 test_loss:  166.6 test_acc:  94.875\n",
      "max acc epoch：2936        max acc：94.875\n",
      "epoch:  2937 train_loss:  142.375 train_acc:  95.283 test_loss:  187.547 test_acc:  94.429\n",
      "epoch:  2938 train_loss:  145.611 train_acc:  95.264 test_loss:  182.764 test_acc:  94.427\n",
      "epoch:  2939 train_loss:  151.886 train_acc:  94.93 test_loss:  178.015 test_acc:  94.455\n",
      "epoch:  2940 train_loss:  152.595 train_acc:  94.978 test_loss:  184.96 test_acc:  94.375\n",
      "epoch:  2941 train_loss:  147.713 train_acc:  95.125 test_loss:  172.05 test_acc:  94.596\n",
      "epoch:  2942 train_loss:  138.728 train_acc:  95.335 test_loss:  168.137 test_acc:  94.685\n",
      "epoch:  2943 train_loss:  139.457 train_acc:  95.145 test_loss:  164.592 test_acc:  94.642\n",
      "epoch:  2944 train_loss:  142.926 train_acc:  95.061 test_loss:  166.641 test_acc:  94.552\n",
      "epoch:  2945 train_loss:  159.105 train_acc:  95.146 test_loss:  194.393 test_acc:  94.169\n",
      "epoch:  2946 train_loss:  150.833 train_acc:  94.994 test_loss:  193.61 test_acc:  94.51\n",
      "epoch:  2947 train_loss:  160.232 train_acc:  95.177 test_loss:  217.407 test_acc:  93.987\n",
      "epoch:  2948 train_loss:  148.14 train_acc:  95.125 test_loss:  175.963 test_acc:  94.6\n",
      "epoch:  2949 train_loss:  141.516 train_acc:  95.253 test_loss:  179.667 test_acc:  94.7\n",
      "epoch:  2950 train_loss:  208.24 train_acc:  94.629 test_loss:  217.895 test_acc:  94.186\n",
      "epoch:  2951 train_loss:  157.569 train_acc:  94.751 test_loss:  188.005 test_acc:  94.436\n",
      "epoch:  2952 train_loss:  146.162 train_acc:  95.16 test_loss:  174.296 test_acc:  94.399\n",
      "epoch:  2953 train_loss:  186.344 train_acc:  94.671 test_loss:  261.952 test_acc:  93.526\n",
      "epoch:  2954 train_loss:  193.444 train_acc:  94.514 test_loss:  308.25 test_acc:  93.645\n",
      "epoch:  2955 train_loss:  144.464 train_acc:  95.137 test_loss:  164.749 test_acc:  94.345\n",
      "epoch:  2956 train_loss:  148.306 train_acc:  95.053 test_loss:  192.74 test_acc:  94.365\n",
      "epoch:  2957 train_loss:  154.761 train_acc:  94.963 test_loss:  217.607 test_acc:  94.183\n",
      "epoch:  2958 train_loss:  143.57 train_acc:  95.165 test_loss:  185.257 test_acc:  94.543\n",
      "epoch:  2959 train_loss:  178.001 train_acc:  94.223 test_loss:  215.481 test_acc:  93.791\n",
      "epoch:  2960 train_loss:  147.873 train_acc:  94.843 test_loss:  170.163 test_acc:  94.607\n",
      "epoch:  2961 train_loss:  138.723 train_acc:  95.321 test_loss:  170.257 test_acc:  94.609\n",
      "epoch:  2962 train_loss:  183.235 train_acc:  94.427 test_loss:  194.646 test_acc:  94.149\n",
      "epoch:  2963 train_loss:  208.882 train_acc:  93.589 test_loss:  246.516 test_acc:  93.038\n",
      "epoch:  2964 train_loss:  166.392 train_acc:  94.958 test_loss:  215.807 test_acc:  93.851\n",
      "epoch:  2965 train_loss:  156.918 train_acc:  94.742 test_loss:  206.029 test_acc:  94.026\n",
      "epoch:  2966 train_loss:  157.554 train_acc:  94.606 test_loss:  195.076 test_acc:  94.241\n",
      "epoch:  2967 train_loss:  144.561 train_acc:  94.992 test_loss:  187.892 test_acc:  94.351\n",
      "epoch:  2968 train_loss:  252.137 train_acc:  92.729 test_loss:  306.162 test_acc:  92.429\n",
      "epoch:  2969 train_loss:  185.037 train_acc:  94.801 test_loss:  220.488 test_acc:  93.899\n",
      "epoch:  2970 train_loss:  171.476 train_acc:  94.465 test_loss:  187.671 test_acc:  94.282\n",
      "epoch:  2971 train_loss:  167.39 train_acc:  94.91 test_loss:  206.821 test_acc:  93.762\n",
      "epoch:  2972 train_loss:  236.115 train_acc:  93.735 test_loss:  344.42 test_acc:  92.652\n",
      "epoch:  2973 train_loss:  179.559 train_acc:  94.641 test_loss:  263.429 test_acc:  93.861\n",
      "epoch:  2974 train_loss:  145.274 train_acc:  95.198 test_loss:  172.314 test_acc:  94.487\n",
      "epoch:  2975 train_loss:  212.058 train_acc:  93.425 test_loss:  229.007 test_acc:  93.252\n",
      "epoch:  2976 train_loss:  153.363 train_acc:  95.006 test_loss:  206.187 test_acc:  94.292\n",
      "epoch:  2977 train_loss:  176.24 train_acc:  94.405 test_loss:  243.948 test_acc:  93.58\n",
      "epoch:  2978 train_loss:  164.711 train_acc:  94.43 test_loss:  178.808 test_acc:  94.239\n",
      "epoch:  2979 train_loss:  207.14 train_acc:  94.551 test_loss:  265.513 test_acc:  93.645\n",
      "epoch:  2980 train_loss:  140.422 train_acc:  95.15 test_loss:  161.65 test_acc:  94.69\n",
      "epoch:  2981 train_loss:  143.428 train_acc:  95.213 test_loss:  176.426 test_acc:  94.519\n",
      "epoch:  2982 train_loss:  169.275 train_acc:  94.657 test_loss:  225.167 test_acc:  93.998\n",
      "epoch:  2983 train_loss:  143.606 train_acc:  95.188 test_loss:  186.061 test_acc:  94.635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2984 train_loss:  145.174 train_acc:  95.26 test_loss:  181.16 test_acc:  94.555\n",
      "epoch:  2985 train_loss:  148.704 train_acc:  94.889 test_loss:  190.479 test_acc:  94.448\n",
      "epoch:  2986 train_loss:  179.553 train_acc:  94.781 test_loss:  227.707 test_acc:  93.829\n",
      "epoch:  2987 train_loss:  143.837 train_acc:  95.022 test_loss:  179.903 test_acc:  94.516\n",
      "epoch:  2988 train_loss:  182.727 train_acc:  94.768 test_loss:  197.128 test_acc:  94.306\n",
      "epoch:  2989 train_loss:  171.196 train_acc:  94.6 test_loss:  210.651 test_acc:  94.024\n",
      "epoch:  2990 train_loss:  147.298 train_acc:  95.051 test_loss:  186.359 test_acc:  94.326\n",
      "epoch:  2991 train_loss:  148.702 train_acc:  95.039 test_loss:  156.745 test_acc:  94.606\n",
      "min loss:2991\n",
      "                Actual           Predicted\n",
      "0   [tensor(216.3389)]  [tensor(232.6099)]\n",
      "1    [tensor(88.8594)]   [tensor(81.3389)]\n",
      "2   [tensor(162.9521)]  [tensor(161.6214)]\n",
      "3   [tensor(185.9385)]  [tensor(204.9102)]\n",
      "4   [tensor(212.8320)]  [tensor(204.8959)]\n",
      "..                 ...                 ...\n",
      "60  [tensor(226.4121)]  [tensor(250.2151)]\n",
      "61  [tensor(127.2646)]  [tensor(128.8636)]\n",
      "62   [tensor(84.7217)]   [tensor(83.2393)]\n",
      "63  [tensor(104.8945)]  [tensor(123.2333)]\n",
      "64  [tensor(156.6016)]  [tensor(163.2453)]\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "epoch:  2992 train_loss:  164.568 train_acc:  94.869 test_loss:  238.466 test_acc:  94.032\n",
      "epoch:  2993 train_loss:  151.64 train_acc:  95.177 test_loss:  212.555 test_acc:  94.119\n",
      "epoch:  2994 train_loss:  141.773 train_acc:  95.244 test_loss:  168.553 test_acc:  94.632\n",
      "epoch:  2995 train_loss:  154.464 train_acc:  94.848 test_loss:  168.262 test_acc:  94.467\n",
      "epoch:  2996 train_loss:  146.138 train_acc:  95.268 test_loss:  184.112 test_acc:  94.383\n",
      "epoch:  2997 train_loss:  169.452 train_acc:  94.716 test_loss:  212.15 test_acc:  94.191\n",
      "epoch:  2998 train_loss:  162.536 train_acc:  94.563 test_loss:  200.984 test_acc:  93.852\n",
      "epoch:  2999 train_loss:  144.573 train_acc:  94.796 test_loss:  161.489 test_acc:  94.492\n",
      "End max acc epoch：2999        max acc：94.875\n",
      "                Actual                                     Predicted\n",
      "0   [tensor(216.3389)]  [tensor(236.3148, grad_fn=<UnbindBackward>)]\n",
      "1    [tensor(88.8594)]   [tensor(83.7932, grad_fn=<UnbindBackward>)]\n",
      "2   [tensor(162.9521)]  [tensor(165.1516, grad_fn=<UnbindBackward>)]\n",
      "3   [tensor(185.9385)]  [tensor(201.8875, grad_fn=<UnbindBackward>)]\n",
      "4   [tensor(212.8320)]  [tensor(214.7123, grad_fn=<UnbindBackward>)]\n",
      "..                 ...                                           ...\n",
      "60  [tensor(226.4121)]  [tensor(254.5182, grad_fn=<UnbindBackward>)]\n",
      "61  [tensor(127.2646)]  [tensor(127.0062, grad_fn=<UnbindBackward>)]\n",
      "62   [tensor(84.7217)]   [tensor(85.9566, grad_fn=<UnbindBackward>)]\n",
      "63  [tensor(104.8945)]  [tensor(125.3112, grad_fn=<UnbindBackward>)]\n",
      "64  [tensor(156.6016)]  [tensor(167.0948, grad_fn=<UnbindBackward>)]\n",
      "\n",
      "[65 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Model()\n",
    "#损失函数\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = torch.nn.MSELoss()#损失函数采用常用的均方损失函数\n",
    "\n",
    "#便于随着训练的进行观察数值的变化\n",
    "train_loss=[]\n",
    "test_loss=[]\n",
    "min_loss=300\n",
    "max_acc=90\n",
    "\n",
    "def accury(pred,y):\n",
    "    return 100-(abs(pred - y)/y*100)\n",
    "\n",
    "def get_model():\n",
    "    #获得这个模型\n",
    "    model = Model()\n",
    "    #优化函数 优化的是模型所有变量即model.parameters()\n",
    "    opt = torch.optim.Adam(model.parameters(),lr=0.0005)  #lr=0.001  #选择使用哪种优化器 761个数据集用这个优化器\n",
    "    #opt = torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.3)   # lr=0.01,momentum=0.3\n",
    "    return model,opt\n",
    "\n",
    "model,optim = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for x,y in train_dl:\n",
    "        y_pred = model(x)\n",
    "        #print(y_pred,y)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        # 梯度置为0\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        # 反向传播求解梯度\n",
    "        loss.backward()\n",
    "        # 优化\n",
    "        optim.step()\n",
    "    # 不需要进行梯度计算\n",
    "    with torch.no_grad():\n",
    "        epoch_loss = loss_fn(model(train_x), train_y).data\n",
    "        epoch_acc = accury(model(train_x), train_y).numpy()\n",
    "        epoch_test_loss = loss_fn(model(test_x), test_y).data\n",
    "        epoch_test_acc = accury(model(test_x), test_y).numpy()\n",
    "        #print(epoch_test_acc,sum(epoch_acc),len(epoch_acc),sum(epoch_acc)/len(epoch_acc))\n",
    "        print('epoch: ',epoch,'train_loss: ',round(epoch_loss.item(),3),'train_acc: ', round(float(sum(epoch_acc)/len(epoch_acc)),3),\n",
    "              'test_loss: ',round(epoch_test_loss.item(),3),'test_acc: ',round(float(sum(epoch_test_acc)/len(epoch_test_acc)),3))\n",
    "        train_loss.append(epoch_loss)\n",
    "        test_loss.append(epoch_test_loss)\n",
    "        if epoch_test_loss < min_loss:\n",
    "            min_loss = epoch_test_loss\n",
    "            print(\"min loss:\"+str(epoch))\n",
    "            torch.save(model, 'H:/vamf_model/model_fps4_8.pth')############\n",
    "            df = pd.DataFrame({'Actual':list(test_y), 'Predicted':list(model(test_x))})  \n",
    "            print(df)\n",
    "        if round(float(sum(epoch_test_acc)/len(epoch_test_acc)),3) > max_acc:\n",
    "            max_acc = round(float(sum(epoch_test_acc)/len(epoch_test_acc)),3)\n",
    "            print(\"max acc epoch：\"+str(epoch)+\"        max acc：\"+str(max_acc))\n",
    "            \n",
    "print(\"End max acc epoch：\"+str(epoch)+\"        max acc：\"+str(max_acc))         \n",
    "#print(model(test_x), test_y)\n",
    "df = pd.DataFrame({'Actual':list(test_y), 'Predicted':list(model(test_x))})  \n",
    "print(df)\n",
    "#df = pd.DataFrame({'Actual':list(train_y), 'Predicted':list(model(train_x))})  \n",
    "#print(df)\n",
    "\n",
    "plt.plot(range(1,epochs+1),train_loss,label='train_loss')\n",
    "plt.plot(range(1,epochs+1),test_loss,label='test_loss')\n",
    "plt.show()\n",
    "#torch.save(model, 'H:/vamf_model/model_vmaf.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e36ae475",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0019941329956054688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [143.8826],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [147.5556],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [149.9311],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [117.4952],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 78.4659],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 65.0285],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.0194],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 64.3608],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.9512],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609],\n",
       "        [ 89.2609]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#预测\n",
    "import time\n",
    "\n",
    "model = Model()\n",
    "model=torch.load( 'H:/vamf_model/model_fps4_8.pth')#######3\n",
    "model.eval()\n",
    "df = pd.read_excel('E:/研学/实验数据/Jockey.xlsx')\n",
    "N4 = [4,\"first_frame\",\"TI\",\"SI\",'keypoint',\n",
    "       'brightness','edgeLength','Hue1', 'Hue2',\n",
    "       'Hue3','Hue4',\t'Hue5',\t'Hue6',\t'Hue7',\t'Saturation',\n",
    "       'Value']\n",
    "\n",
    "N6 = [6,\"first_frame\", \"TI\",'keypoint','brightness','edgeLength',\n",
    "      'Hue1', 'Hue2','Hue3','Hue4',\t'Hue5',\t'Hue6',\t'Hue7',\t 'Saturation','Value']\n",
    "\n",
    "N8 =  [8,\"first_frame\",\"TI\",\"SI\",'keypoint',\n",
    "      'brightness','contrast','edgeLength','Hue1',\n",
    "      'Hue2','Hue3','Hue4',\t'Hue5',\t'Hue6',\t'Hue7',\n",
    "      'Saturation']\n",
    "df = df[N6].values#########\n",
    "x = torch.from_numpy(df).type(torch.FloatTensor)\n",
    "t=time.time()\n",
    "pre = model(x)\n",
    "t0=time.time()-t\n",
    "print(t0)\n",
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85c68393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list转dataframe\n",
    "df = pd.DataFrame(list(pre.detach().numpy().reshape(-1,90)[0]), columns=['fps'])\n",
    "# 保存到本地excel\n",
    "df.to_excel(\"E:/研学/实验数据/LOL_fps6.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "662939fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-1-1b8b3ab6d2e6>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-1b8b3ab6d2e6>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    N=[ \"frame_SI\",\"frame_TI\",'Value','Saturation','Hue7',\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#批量处理FPS预测的特征重要性\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt   \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "\n",
    "#scale放里面误差极大，需对3个x训练不同的模型################\n",
    "ACC=[]\n",
    "for i in range(16):\n",
    "    N=[ \"frame_SI\",\"frame_TI\",'Value','Saturation','Hue7',\n",
    "       'Hue6','Hue5','Hue4','Hue3','Hue2','Hue1','edgeLength',\n",
    "       'contrast','brightness','keypoint',\"first_frame\"]\n",
    "    del N[i]\n",
    "    print(N)\n",
    "    #读数据集\n",
    "    data = pd.read_csv(\"F:/video_cut/Bear_size.csv\") \n",
    "    X = data[N].values\n",
    "    Y = data.video8.values.reshape(-1, 1)################\n",
    "    train_x,test_x,train_y,test_y=train_test_split(X,Y,train_size=0.8,random_state=1)  #shuffle=False 就是按照顺序划分的测试集和验证集,默认为true才行\n",
    "    #print(train_x.size)\n",
    "\n",
    "    #将数据转换成Tensor LongTensor等价于int64\n",
    "    train_x = torch.from_numpy(train_x).type(torch.FloatTensor)\n",
    "    train_y = torch.from_numpy(train_y).type(torch.FloatTensor)\n",
    "    test_x = torch.from_numpy(test_x).type(torch.FloatTensor)\n",
    "    test_y = torch.from_numpy(test_y).type(torch.FloatTensor)\n",
    "    print(test_x.shape)\n",
    "    points = int(test_x.shape[1])\n",
    "\n",
    "    #数据只有3805行故batchsize设置8较优\n",
    "\n",
    "    batch = 8#32 \n",
    "    no_of_batches = len(data)//batch\n",
    "    epochs = 3000#3000\n",
    "\n",
    "    #TensorDataset()可以对tensor进行打包即合并\n",
    "    train_ds = TensorDataset(train_x,train_y)\n",
    "    #希望模型不关注训练集数据顺序故用乱序\n",
    "    train_dl = DataLoader(train_ds,batch_size=batch,shuffle=True)\n",
    "    test_ds = TensorDataset(test_x,test_y)\n",
    "    #对测试集不需要用乱序避免工作量增加\n",
    "    test_dl = DataLoader(test_ds,batch_size=batch)\n",
    "    #print(test_x,test_y)\n",
    "\n",
    "    #创建模型\n",
    "    #继承nn.Module这个类并自定义模型\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Model, self).__init__()\n",
    "            self.linear1 = nn.Linear(points,points*2)  # 输入是一个点对（x1，x2）,所以我们输入的神经节点是两个\n",
    "            self.linear2 = nn.Linear(points*2,points*2, bias=True)\n",
    "            self.linear3 = nn.Linear(points*2,points*2, bias=True)\n",
    "            self.linear4 = nn.Linear(points*2,points*2, bias=True)\n",
    "            self.linear5 = nn.Linear(points*2,points, bias=True)\n",
    "            self.linear6 = nn.Linear(points,1)  # 输出层由于是二分类，所以输出节点是2\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.linear1(x))\n",
    "            x = F.relu(self.linear2(x))\n",
    "            x = F.relu(self.linear3(x))\n",
    "            x = F.relu(self.linear4(x))\n",
    "            x = F.relu(self.linear5(x))\n",
    "            c =self.linear6(x)\n",
    "            #c = F.softmax(self.linear6(x),dim=1)\n",
    "            return c\n",
    "    \n",
    "    model = Model()\n",
    "    #损失函数\n",
    "    #loss_fn = nn.CrossEntropyLoss()\n",
    "    loss_fn = torch.nn.MSELoss()#损失函数采用常用的均方损失函数\n",
    "\n",
    "    #便于随着训练的进行观察数值的变化\n",
    "    train_loss=[]\n",
    "    test_loss=[]\n",
    "    min_loss=1\n",
    "    max_acc=1\n",
    "\n",
    "    def accury(pred,y):\n",
    "        return 100-(abs(pred - y)/y*100)\n",
    "\n",
    "    def get_model():\n",
    "        #获得这个模型\n",
    "        model = Model()\n",
    "        #优化函数 优化的是模型所有变量即model.parameters()\n",
    "        opt = torch.optim.Adam(model.parameters(),lr=0.0005)  #lr=0.001  #选择使用哪种优化器 761个数据集用这个优化器\n",
    "        #opt = torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.3)   # lr=0.01,momentum=0.3\n",
    "        return model,opt\n",
    "\n",
    "    model,optim = get_model()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for x,y in train_dl:\n",
    "            y_pred = model(x)\n",
    "            #print(y_pred,y)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            # 梯度置为0\n",
    "        \n",
    "            optim.zero_grad()\n",
    "            # 反向传播求解梯度\n",
    "            loss.backward()\n",
    "            # 优化\n",
    "            optim.step()\n",
    "        # 不需要进行梯度计算\n",
    "        with torch.no_grad():\n",
    "            epoch_loss = loss_fn(model(train_x), train_y).data\n",
    "            epoch_acc = accury(model(train_x), train_y).numpy()\n",
    "            epoch_test_loss = loss_fn(model(test_x), test_y).data\n",
    "            epoch_test_acc = accury(model(test_x), test_y).numpy()\n",
    "            #print(epoch_test_acc,sum(epoch_acc),len(epoch_acc),sum(epoch_acc)/len(epoch_acc))\n",
    "            print('epoch: ',epoch,'train_loss: ',round(epoch_loss.item(),3),'train_acc: ', round(float(sum(epoch_acc)/len(epoch_acc)),3),\n",
    "                  'test_loss: ',round(epoch_test_loss.item(),3),'test_acc: ',round(float(sum(epoch_test_acc)/len(epoch_test_acc)),3))\n",
    "            train_loss.append(epoch_loss)\n",
    "            test_loss.append(epoch_test_loss)\n",
    "            if epoch_test_loss < min_loss:\n",
    "                min_loss = epoch_test_loss\n",
    "                print(\"min loss:\"+str(epoch))\n",
    "                df = pd.DataFrame({'Actual':list(test_y), 'Predicted':list(model(test_x))})  \n",
    "            if round(float(sum(epoch_test_acc)/len(epoch_test_acc)),3) > max_acc:\n",
    "                max_acc = round(float(sum(epoch_test_acc)/len(epoch_test_acc)),3)         \n",
    "    print(\"End max acc epoch：\"+str(epoch)+\"        max acc：\"+str(max_acc)) \n",
    "    ACC.append(max_acc)\n",
    "print(ACC)\n",
    "# list转dataframe\n",
    "df = pd.DataFrame(ACC, columns=['acc'])\n",
    "# 保存到本地excel\n",
    "df.to_excel(\"F:/video_cut/psnr_acc.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d153a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_envs]",
   "language": "python",
   "name": "conda-env-pytorch_envs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
